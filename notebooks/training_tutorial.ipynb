{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HerdNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PytorchWildlife.models import detection as pw_detection\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# herdnet_path = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\base_models_weights\\20220329_HerdNet_Ennedi_dataset_2023.pth\"\n",
    "# herdnet_detector = pw_detection.HerdNet(weights=herdnet_path,device=\"cuda\")\n",
    "# herdnet_detector.single_image_detection(img=r\"D:\\PhD\\Data per camp\\DetectionDataset\\Rep 1\\train\\images\\DJI_20231001142304_0082_0_51_2160_0_3440_1280.jpg\",det_conf_thres=0.25,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_csv = r\"D:\\PhD\\Data per camp\\DetectionDataset\\Rep 1\\train\\gt.csv\"\n",
    "\n",
    "def save_non_empty_gt(gt_csv:str):\n",
    "    \n",
    "    # images,x,y,labels\n",
    "\n",
    "    df_gt = pd.read_csv(gt_csv)\n",
    "    df_gt = df_gt.dropna(axis=0,how=\"any\")\n",
    "    df_gt.loc[:,'label_id'] = df_gt['label_id'].apply(int)\n",
    "    df_gt = df_gt.astype({'label_id':'int64'})\n",
    "\n",
    "    df_gt.rename(columns={'labels':'label_names','label_id':'labels'},\n",
    "                 inplace=True)\n",
    "    \n",
    "    # df_gt.drop(columns=['x0','x1','y0','y1'],inplace=True)\n",
    "\n",
    "    save_path = Path(gt_csv).with_stem(\"gt_nonempty\")\n",
    "    df_gt.to_csv(save_path,\n",
    "                  index=False,\n",
    "                )\n",
    "\n",
    "    print(f\"Saving non-empty gt.csv to {save_path}.\")\n",
    "    return df_gt, str(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_label_format(loaded_df:pd.DataFrame)->str:\n",
    "    \"\"\"checks label format\n",
    "\n",
    "    Args:\n",
    "        loaded_df (pd.DataFrame): target values\n",
    "\n",
    "    Raises:\n",
    "        NotImplementedError: when the format is not yolo or yolo-obb\n",
    "\n",
    "    Returns:\n",
    "        str: yolo or yolo-obb\n",
    "    \"\"\"\n",
    "\n",
    "    num_features = len(loaded_df.columns)\n",
    "\n",
    "    if num_features == 5:\n",
    "        return \"yolo\"\n",
    "    elif num_features == 9:\n",
    "        return \"yolo-obb\"\n",
    "    else:\n",
    "        raise NotImplementedError(f\"The number of features ({num_features}) in the label file is wrong. Check yolo or yolo-obb format from ultralytics.\")\n",
    "\n",
    "\n",
    "def get_groundtruth(yolo_labels_dir, save_path:str=None,load_gt_csv:str=None):\n",
    "\n",
    "    if load_gt_csv is not None:\n",
    "        return pd.read_csv(load_gt_csv)\n",
    "\n",
    "    cols1 = ['id','x1','y1','x2','y2','x3','y3','x4','y4']\n",
    "    cols2 = ['id','x','y','w','h']\n",
    "\n",
    "    if not Path(yolo_labels_dir).exists():\n",
    "        raise FileNotFoundError('Directory does not exist.')\n",
    "\n",
    "    # Iterate through labels\n",
    "    dfs = list()\n",
    "    for label_path in tqdm(Path(yolo_labels_dir).glob(\"*.txt\"),desc=\"Getting groundtruths\"):\n",
    "        df = pd.read_csv(label_path,sep=' ',header=None)\n",
    "\n",
    "        image_path = Path(str(label_path).replace('labels','images')).with_suffix(\".jpg\")\n",
    "        img = Image.open(image_path)\n",
    "        img_width, img_height = img.size\n",
    "\n",
    "        label_format = check_label_format(df)\n",
    "\n",
    "        if label_format == 'yolo':\n",
    "            df.columns = cols2\n",
    "            df.loc[:,'x'] = df['x']*img_width\n",
    "            df.loc[:,'y'] = df['y']*img_height\n",
    "            df.loc[:,'w'] = df['w']*img_width\n",
    "            df.loc[:,'h'] = df['h']*img_height\n",
    "        else: # yolo-obb\n",
    "            df.columns = cols1\n",
    "            df['x'] = (df['x1'] + df['x2'])*img_width*0.5\n",
    "            df['y'] = (df['y1'] + df['y4'])*img_height*0.5\n",
    "            df['w'] = (df['x2'] - df['x1'])*img_width\n",
    "            df['h'] = (df['y4'] - df['y1'])*img_height\n",
    "        df['images'] = str(image_path)\n",
    "        df.rename(columns={'id':'labels'}, inplace=True)\n",
    "        dfs.append(df)\n",
    "        img.close()\n",
    "    \n",
    "    dfs = pd.concat(dfs)\n",
    "    if save_path is not None:\n",
    "        dfs.to_csv(save_path, index=False)\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Machine Learning\\anaconda3\\envs\\label-backend\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Machine Learning\\anaconda3\\envs\\label-backend\\Lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from torch.utils.data import ConcatDataset\n",
    "from animaloc.datasets import CSVDataset\n",
    "from animaloc.data.transforms import MultiTransformsWrapper, DownSample, PointsToMask, FIDT\n",
    "\n",
    "def load_dataset(data_config_yaml:str,\n",
    "                 split:str='train',\n",
    "                 transforms:dict=None,\n",
    "                 down_ratio:int=2,\n",
    "                 num_classes = 6,\n",
    "                 patch_size:int=800):\n",
    "\n",
    "    \n",
    "    if transforms is None:\n",
    "        transforms = {}\n",
    "        transforms['train'] = ([    #A.Resize(patch_size,patch_size,p=1.0),\n",
    "                                    A.VerticalFlip(p=0.5), \n",
    "                                    A.HorizontalFlip(p=0.5),\n",
    "                                    A.RandomRotate90(p=0.5),\n",
    "                                    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.2),\n",
    "                                    A.Blur(blur_limit=15, p=0.2),\n",
    "                                    A.Normalize(p=1.0)\n",
    "                                ], \n",
    "                                [\n",
    "                                    MultiTransformsWrapper([\n",
    "                                    FIDT(num_classes=num_classes, down_ratio=down_ratio),\n",
    "                                    PointsToMask(radius=2, \n",
    "                                                 num_classes=num_classes, \n",
    "                                                 squeeze=True, \n",
    "                                                 down_ratio=int(patch_size//(16*patch_size/512))\n",
    "                                                )\n",
    "                                ])\n",
    "                                ])\n",
    "        transforms['val'] = (\n",
    "                                [#A.Resize(patch_size,patch_size,p=1.0),\n",
    "                                 A.Normalize(p=1.0)],\n",
    "                                [DownSample(down_ratio=down_ratio, anno_type='point')]\n",
    "                            )\n",
    "        transforms['test'] = transforms['val']\n",
    "    \n",
    "\n",
    "    with open(data_config_yaml,'r') as file:\n",
    "        data_config = yaml.load(file,Loader=yaml.FullLoader)\n",
    "    datasets = list()\n",
    "    df_gts = list()\n",
    "    root = data_config['path']\n",
    "    for data in tqdm(data_config[split],desc=\"concatenating datasets\"):\n",
    "        img_dir = os.path.join(root,data)\n",
    "        # path_to_csv = Path(img_dir).parent/\"gt.csv\"\n",
    "        df = get_groundtruth(yolo_labels_dir=img_dir.replace(\"images\",\"labels\"), \n",
    "                             save_path=None,\n",
    "                             load_gt_csv=None # path_to_csv\n",
    "                            )        \n",
    "        dataset = CSVDataset(\n",
    "                            csv_file = df,\n",
    "                            root_dir = img_dir,\n",
    "                            albu_transforms = transforms[split][0],\n",
    "                            end_transforms = transforms[split][1]\n",
    "                            )\n",
    "        datasets.append(dataset)\n",
    "        df_gts.append(df)\n",
    "    \n",
    "    return ConcatDataset(datasets=datasets), pd.concat(df_gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting groundtruths: 4984it [00:07, 654.10it/s]:00<?, ?it/s]\n",
      "concatenating datasets: 100%|██████████| 1/1 [00:08<00:00,  8.23s/it]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, df_train_labels = load_dataset(data_config_yaml=r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification.yaml\",\n",
    "                            split='train',\n",
    "                            down_ratio=2,\n",
    "                            transforms=None,\n",
    "                            num_classes=6,\n",
    "                            patch_size=800\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "0     0.001721\n",
       "1     0.056920\n",
       "2     0.000536\n",
       "3     0.001568\n",
       "4     0.004629\n",
       "5     0.136026\n",
       "6     0.011323\n",
       "7     0.040777\n",
       "8     0.005929\n",
       "9     0.022875\n",
       "10    0.005623\n",
       "11    0.011093\n",
       "12    0.058680\n",
       "13    0.008798\n",
       "14    0.352689\n",
       "15    0.192487\n",
       "16    0.024061\n",
       "17    0.023066\n",
       "18    0.041198\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_labels_freq = df_train_labels['labels'].value_counts().sort_index()/len(df_train_labels)\n",
    "df_train_labels_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting groundtruths: 899it [00:01, 600.41it/s]0:00<?, ?it/s]\n",
      "concatenating datasets: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "val_dataset, df_val_labels = load_dataset(data_config_yaml=r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification.yaml\",\n",
    "                            split='val',\n",
    "                            down_ratio=2,\n",
    "                            transforms=None,\n",
    "                            num_classes=19,\n",
    "                            patch_size=1280\n",
    "                        )\n",
    "\n",
    "# test_dataset = CSVDataset(\n",
    "#     csv_file = '/content/data/test.csv',\n",
    "#     root_dir = '/content/data/test',\n",
    "#     albu_transforms = [A.Normalize(p=1.0)],\n",
    "#     end_transforms = [DownSample(down_ratio=down_ratio, anno_type='point')]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a in train_dataset:\n",
    "#     print(a)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_dataset, batch_size = 16, shuffle = True)\n",
    "\n",
    "val_dataloader = DataLoader(dataset = val_dataset, batch_size = 16, shuffle = False)\n",
    "\n",
    "# test_dataloader = DataLoader(dataset = test_dataset, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      580.6,      17.568,      1863.8,       637.2,         216,      7.3515,       88.31,      24.523,      168.63,      43.714,      177.81,      90.137,      17.041,      113.65,      2.8353,      5.1951,      41.559,      43.351,      24.272])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute loss weigths\n",
    "weights = 1/(df_train_labels_freq + 1e-6)\n",
    "weights = weights.to_numpy()\n",
    "# weights = weights.clip(min=1.,max=200)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of classes\n",
    "weights.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from animaloc.models import HerdNet\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from animaloc.models import LossWrapper\n",
    "from animaloc.train.losses import FocalLoss\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "num_classes = weights.shape[0]\n",
    "down_ratio = 2\n",
    "\n",
    "herdnet = HerdNet(num_classes=num_classes, down_ratio=down_ratio, pretrained=False)\n",
    "herdnet_path = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\base_models_weights\\20220329_HerdNet_Ennedi_dataset_2023.pth\"\n",
    "checkpoint = torch.load(herdnet_path)\n",
    "herdnet.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "\n",
    "# herdnet = pw_detection.HerdNet(weights=herdnet_path,device=\"cuda\").model  \n",
    "\n",
    "# move to device\n",
    "herdnet = herdnet.to('cuda')\n",
    "weights = Tensor(weights).to('cuda')\n",
    "\n",
    "losses = [\n",
    "    {'loss': FocalLoss(reduction='mean'), 'idx': 0, 'idy': 0, 'lambda': 1.0, 'name': 'focal_loss'},\n",
    "    {'loss': CrossEntropyLoss(reduction='mean', weight=weights), 'idx': 1, 'idy': 1, 'lambda': 1.0, 'name': 'ce_loss'}\n",
    "    ]\n",
    "\n",
    "herdnet = LossWrapper(herdnet, losses=losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.\n"
     ]
    }
   ],
   "source": [
    "!wandb login\n",
    "!wandb offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from animaloc.train import Trainer\n",
    "from animaloc.eval import PointsMetrics, HerdNetStitcher, HerdNetEvaluator\n",
    "\n",
    "work_dir = '../.tmp'\n",
    "\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-3\n",
    "epochs = 30\n",
    "patch_size = 800\n",
    "\n",
    "optimizer = Adam(params=herdnet.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "metrics = PointsMetrics(radius=20, num_classes=num_classes)\n",
    "\n",
    "stitcher = HerdNetStitcher(\n",
    "    model=herdnet, \n",
    "    size=(patch_size,patch_size), \n",
    "    batch_size=1,\n",
    "    overlap=160, \n",
    "    down_ratio=down_ratio, \n",
    "    reduction='mean'\n",
    "    )\n",
    "\n",
    "evaluator = HerdNetEvaluator(\n",
    "    model=herdnet, \n",
    "    dataloader=val_dataloader, \n",
    "    metrics=metrics, \n",
    "    stitcher=stitcher, \n",
    "    work_dir=work_dir, \n",
    "    header='validation'\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=herdnet,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    lr_milestones=[20,],\n",
    "    optimizer=optimizer,\n",
    "    auto_lr=True,\n",
    "    num_epochs=epochs,\n",
    "    evaluator=evaluator,\n",
    "    work_dir=work_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# herdnet = trainer.resume(pth_path=herdnet_path,select='max',validate_on='recall')\n",
    "# trainer.model = herdnet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAINING] - Epoch: [1] [  1/312] eta: 14:51:28 lr: 0.000002 loss: 80600.1016 (80600.1016) focal_loss: 80597.1797 (80597.1797) ce_loss: 2.9197 (2.9197) time: 171.4360 data: 2.9852 max mem: 51244\n",
      "[TRAINING] - Epoch: [1] [  1/312] eta: 14:51:28 lr: 0.000002 loss: 80600.1016 (80600.1016) focal_loss: 80597.1797 (80597.1797) ce_loss: 2.9197 (2.9197) time: 171.4360 data: 2.9852 max mem: 51244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINING] - Epoch: [1] [  1/312] eta: 14:51:28 lr: 0.000002 loss: 80600.1016 (80600.1016) focal_loss: 80597.1797 (80597.1797) ce_loss: 2.9197 (2.9197) time: 171.4360 data: 2.9852 max mem: 51244\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m herdenet \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwarmup_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Machine Learning\\anaconda3\\envs\\label-backend\\Lib\\site-packages\\animaloc\\train\\trainers.py:273\u001b[0m, in \u001b[0;36mTrainer.start\u001b[1;34m(self, warmup_iters, checkpoints, select, validate_on, wandb_flag)\u001b[0m\n\u001b[0;32m    268\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    271\u001b[0m \n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     train_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb_flag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wandb_flag:\n\u001b[0;32m    275\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: train_output, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch})\n",
      "File \u001b[1;32mc:\\Users\\Machine Learning\\anaconda3\\envs\\label-backend\\Lib\\site-packages\\animaloc\\train\\trainers.py:514\u001b[0m, in \u001b[0;36mTrainer._train\u001b[1;34m(self, epoch, warmup_iters, wandb_flag)\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_lr_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_lr_scheduler(\n\u001b[0;32m    508\u001b[0m         \u001b[38;5;28mmin\u001b[39m(warmup_iters, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \n\u001b[0;32m    509\u001b[0m         \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m warmup_iters\n\u001b[0;32m    510\u001b[0m         )\n\u001b[0;32m    512\u001b[0m batches_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 514\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_logger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_every\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Machine Learning\\anaconda3\\envs\\label-backend\\Lib\\site-packages\\animaloc\\utils\\logger.py:73\u001b[0m, in \u001b[0;36mCustomLogger.log_every\u001b[1;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[0;32m     71\u001b[0m MB \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024.0\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024.0\u001b[39m\n\u001b[0;32m     72\u001b[0m flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 73\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_time\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Machine Learning\\anaconda3\\envs\\label-backend\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Machine Learning\\anaconda3\\envs\\label-backend\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Machine Learning\\anaconda3\\envs\\label-backend\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Machine Learning\\anaconda3\\envs\\label-backend\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:335\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Machine Learning\\anaconda3\\envs\\label-backend\\Lib\\site-packages\\animaloc\\datasets\\csv.py:229\u001b[0m, in \u001b[0;36mCSVDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    226\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_image(index)\n\u001b[0;32m    227\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_target(index)\n\u001b[1;32m--> 229\u001b[0m tr_img, tr_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tr_img, tr_target\n",
      "File \u001b[1;32mc:\\Users\\Machine Learning\\anaconda3\\envs\\label-backend\\Lib\\site-packages\\animaloc\\datasets\\csv.py:200\u001b[0m, in \u001b[0;36mCSVDataset._transforms\u001b[1;34m(self, image, target)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manno_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPoint\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    191\u001b[0m     transform_pipeline \u001b[38;5;241m=\u001b[39m albumentations\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malbu_transforms, \n\u001b[0;32m    193\u001b[0m         keypoint_params\u001b[38;5;241m=\u001b[39malbumentations\u001b[38;5;241m.\u001b[39mKeypointParams(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m         )\n\u001b[0;32m    197\u001b[0m     )\n\u001b[0;32m    199\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m transform_pipeline(\n\u001b[1;32m--> 200\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    201\u001b[0m         keypoints \u001b[38;5;241m=\u001b[39m target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannos\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlabel_fields\n\u001b[0;32m    203\u001b[0m     )\n\u001b[0;32m    205\u001b[0m     tr_image \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(transformed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    206\u001b[0m     transformed\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Machine Learning\\anaconda3\\envs\\label-backend\\Lib\\site-packages\\PIL\\Image.py:696\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    694\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 696\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (\u001b[38;5;167;01mMemoryError\u001b[39;00m, \u001b[38;5;167;01mRecursionError\u001b[39;00m)):\n",
      "File \u001b[1;32mc:\\Users\\Machine Learning\\anaconda3\\envs\\label-backend\\Lib\\site-packages\\PIL\\Image.py:776\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[1;34m(self, encoder_name, *args)\u001b[0m\n\u001b[0;32m    773\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merrcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in tobytes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m--> 776\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "herdenet = trainer.start(warmup_iters=10, checkpoints='best', select='max', validate_on='f1_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaddlePaddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMROTATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "label-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
