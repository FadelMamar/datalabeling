{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get GPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\datalabeling\\.venv\\Lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datalabeling.common.annotation_utils import GPSUtils\n",
    "from pathlib import Path\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gps_coords(image_dir:str,\n",
    "                   image_paths:list[str]=None,\n",
    "                   exts:list[str] = ['*.jpg','*.jpeg','*.png',]):\n",
    "\n",
    "    exts = [e.lower() for e in exts] + [e.capitalize() for e in exts]\n",
    "\n",
    "    if image_paths is None:\n",
    "        image_paths = chain.from_iterable([Path(image_dir).glob(ext) for ext in exts])\n",
    "\n",
    "    gps_coords = [GPSUtils.get_gps_coord(file_name=path,return_as_decimal=True)[0] for path in image_paths]\n",
    "    \n",
    "    gps_coords = pd.DataFrame(data=gps_coords,\n",
    "                 columns=[\"Latitude\", \"Longitude\", \"Elevation\"])\n",
    "    \n",
    "    return gps_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GPSVersionID': [2, 3, 0, 0],\n",
       " 'GPSLatitudeRef': 'S',\n",
       " 'GPSLatitude': (23.0, 12.0, 38.622960045019695),\n",
       " 'GPSLongitudeRef': 'E',\n",
       " 'GPSLongitude': (18.0, 21.0, 29.31192005710207),\n",
       " 'GPSAltitude': '1496.6827309236949',\n",
       " 'GPSTrack': 79.89536266349585}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_exif = GPSUtils.get_exif(r\"D:\\savmap_dataset_v2\\raw\\images\\0aff71b5a50d4357bfcca7cf5f614b7d.JPG\")\n",
    "\n",
    "gps_info = GPSUtils.get_gps_info(extracted_exif)\n",
    "\n",
    "gps_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gps = get_gps_coords(image_dir=r\"D:\\savmap_dataset_v2\\raw\\images\",\n",
    "                        image_paths=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Elevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-23.185786</td>\n",
       "      <td>18.380313</td>\n",
       "      <td>1489.539400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-23.184062</td>\n",
       "      <td>18.376709</td>\n",
       "      <td>1486.842700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-23.215928</td>\n",
       "      <td>18.360828</td>\n",
       "      <td>1496.319588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-23.215876</td>\n",
       "      <td>18.355488</td>\n",
       "      <td>1495.295409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-23.221646</td>\n",
       "      <td>18.395676</td>\n",
       "      <td>1480.894410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>-23.215110</td>\n",
       "      <td>18.430430</td>\n",
       "      <td>1478.588015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>-23.193977</td>\n",
       "      <td>18.384050</td>\n",
       "      <td>1461.046392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>-23.181181</td>\n",
       "      <td>18.380090</td>\n",
       "      <td>1488.729800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>-23.209417</td>\n",
       "      <td>18.430249</td>\n",
       "      <td>1479.213622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>-23.210628</td>\n",
       "      <td>18.429554</td>\n",
       "      <td>1484.437008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1308 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Latitude  Longitude    Elevation\n",
       "0    -23.185786  18.380313  1489.539400\n",
       "1    -23.184062  18.376709  1486.842700\n",
       "2    -23.215928  18.360828  1496.319588\n",
       "3    -23.215876  18.355488  1495.295409\n",
       "4    -23.221646  18.395676  1480.894410\n",
       "...         ...        ...          ...\n",
       "1303 -23.215110  18.430430  1478.588015\n",
       "1304 -23.193977  18.384050  1461.046392\n",
       "1305 -23.181181  18.380090  1488.729800\n",
       "1306 -23.209417  18.430249  1479.213622\n",
       "1307 -23.210628  18.429554  1484.437008\n",
       "\n",
       "[1308 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get parent image from sliced image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os\n",
    "from itertools import product\n",
    "import torchvision, PIL\n",
    "\n",
    "def get_coordinates(img_path:str,tile_w:int,tile_h:int,overlaping_factor:float):\n",
    "    \n",
    "    pil_img = PIL.Image.open(img_path)\n",
    "    img_tensor = torchvision.transforms.ToTensor()(pil_img)\n",
    "\n",
    "    image_width=img_tensor.shape[2]\n",
    "    image_height=img_tensor.shape[1]\n",
    "\n",
    "    # x limits\n",
    "    lim = math.ceil((image_width-tile_w)/((1-overlaping_factor)*tile_w))\n",
    "    x_right = [math.floor(tile_w + i*(1-overlaping_factor)*tile_w) for i in range(lim)]\n",
    "    x_coords = [(x-tile_w,x) for x in x_right]\n",
    "    if len(x_coords)>0:\n",
    "        left,right = x_coords[-1]\n",
    "        x_coords[-1] = (left,image_width) # extending to remaining pixels\n",
    "\n",
    "    # y limits\n",
    "    lim = math.ceil((image_height-tile_h)/((1-overlaping_factor)*tile_h))\n",
    "    y_bottom = [math.floor(tile_h + i*(1-overlaping_factor)*tile_h) for i in range(lim)]\n",
    "    y_coords = [(y-tile_h,y) for y in y_bottom]\n",
    "    if len(y_coords)>0:\n",
    "        top,bottom = y_coords[-1]\n",
    "        y_coords[-1] = (top,image_height) # extending to remaining pixels\n",
    "\n",
    "    # tiles coordinates\n",
    "    if len(y_coords)>0 and len(x_coords)>0:\n",
    "        pass\n",
    "    elif len(y_coords) == 0:\n",
    "        y_coords = [(0,image_height),]\n",
    "    elif len(x_coords) == 0:\n",
    "        x_coords = [(0,image_width),]\n",
    "\n",
    "    coordinates = product(x_coords,y_coords)\n",
    "\n",
    "    return list(coordinates)\n",
    "\n",
    "def get_save_paths(\n",
    "    batch:list,\n",
    "    basename: str,\n",
    "    dest_folder: str\n",
    "    ) -> None:\n",
    "    ''' Save mini-batch tensors into image files\n",
    "\n",
    "    Use torchvision save_image function,\n",
    "    see https://pytorch.org/vision/stable/utils.html#torchvision.utils.save_image\n",
    "\n",
    "    Args:\n",
    "        batch (list): mini-batch tensor\n",
    "        basename (str) : parent image name, with extension\n",
    "        dest_folder (str): destination folder path\n",
    "    '''\n",
    "    paths = list()\n",
    "    base_wo_extension, extension = basename.split('.')[0], basename.split('.')[1]\n",
    "    for i, b in enumerate(range(len(batch))):\n",
    "        full_path = '_'.join([base_wo_extension, str(i) + '.']) + extension\n",
    "        save_path = os.path.join(dest_folder, full_path)\n",
    "\n",
    "        get_save_paths.append(save_path)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords =  get_coordinates(img_path:str,tile_w=2000,tile_h=2000,overlaping_factor=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image tiling for annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meanings of arguments\n",
    "- ```-ratioheight``` : proportion of tile  w.r.t height of image. Example 0.5 means dividing the image in two bands w.r.t height.\n",
    "- ```-ratiowidth``` : proportion of tile w.r.t to width of image. Example 1.0 means the width of the tile is the same as the image.\n",
    "- ```-overlapfactor``` : percentage of overlap. It should be less than 1.\n",
    "- ```-rmheight``` : percentage of height to remove or crop at bottom and top\n",
    "- ```-rmwidth``` : percentage of width to remove or crop on each side of the image\n",
    "- ```-pattern``` : \"**/*.JPG\" will get all .JPG images in directory and subdirectories. On windows it will get both .JPG and .jpg. On unix it will only get .JPG images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New script for tiling data\n",
    "# images_to_tile = r\"D:\\PhD\\Data per camp\\Extra training data\\savmap_dataset_v2\\raw_data\\images\"\n",
    "# destination_directory = r\"D:\\PhD\\Data per camp\\Extra training data\\savmap_dataset_v2\\raw_data\\images-tiled\"\n",
    "!python ../../HerdNet/tools/patcher.py \"D:\\PhD\\Harvard data\\Satara_east\\Selection\" 0 0 0 -overlapfactor 0.1  -ratiowidth 0.5 -ratioheight 0.5 -rmheight 0.1 -rmwidth 0.1 -dest \"D:\\PhD\\Harvard data\\Satara_east\\Selection - tiled\" -pattern \"**/*.JPG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-annotating data for Labelstudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "from datalabeling.ml import Annotator\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a JSON file to be uuploaded to Label studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "# provide correct alias, \"pt\", \"onnx\"\n",
    "# alias = \"last\" # the aliases are found in mlflow tracker UI, use \"last-1\" to use the previous model\n",
    "# name = \"obb-detector\" # detector, \"obb-detector\"\n",
    "# handler = Annotator(mlflow_model_alias=alias,\n",
    "#                     mlflow_model_name=name,\n",
    "#                     is_yolo_obb= name.strip() == \"obb-detector\",\n",
    "#                     # dotenv_path=\"../.env\"\n",
    "#                     )\n",
    "# path_img_dir=r\"D:\\PhD\\Africa Parks\\Liuwa aerial survey_ALL\\CENSUS 2019\\DAY 2 CENSUS 2019_CONVERTED\\AP 2019 day 2 - tiled\"\n",
    "# root=\"D:\\\\\"\n",
    "# save_json_path = os.path.join(Path(path_img_dir).parent, f\"{Path(path_img_dir).name}_preannotation_label-studio.json\")\n",
    "\n",
    "# # build and saves json\n",
    "# directory_preds = handler.build_upload_json(path_img_dir=path_img_dir,\n",
    "#                                             root=root,\n",
    "#                                             save_json_path=save_json_path,\n",
    "#                                             pattern=\"**/*.JPG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-annotating an existing project using Label studio API\n",
    "It seems that it will not work well (i.e. filtering) with older projects created prior to Label studio software update.\n",
    "It is the **recommended way of pre-annotating data in Labelstudio**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide correct alias, \"pt\", \"onnx\"\n",
    "aliases = [\"yolov12s\",]\n",
    "project_id = 96 # insert correct project_id by loooking at the url\n",
    "for alias in aliases:\n",
    "    name = \"detector\" # detector, \"obb-detector\"\n",
    "    handler = Annotator(mlflow_model_alias=alias,\n",
    "                        mlflow_model_name=name,\n",
    "                        confidence_threshold=0.15,\n",
    "                        is_yolo_obb=name.strip() == \"obb-detector\",\n",
    "                        dotenv_path=\"../.env\")\n",
    "    handler.upload_predictions(project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before running the script below, make sure you have exported the annotations so you CAN revert back!!!**\n",
    "- CLEANING ANNOTATIONS that have been mistakenly saved with label=\"wildlife\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Cleaning annotations - NO WAY BACK\n",
    "name = \"obb-detector\"\n",
    "handler = Annotator(mlflow_model_alias=\"version6\",\n",
    "                        mlflow_model_name=name,\n",
    "                        confidence_threshold=0.25,\n",
    "                        is_yolo_obb=name.strip() == \"obb-detector\",\n",
    "                        dotenv_path=\"../.env\")\n",
    "\n",
    "# Select project\n",
    "project_id = 88\n",
    "project = handler.labelstudio_client.get_project(id=project_id)\n",
    "\n",
    "# Delete annotations saved with label \"wildlife\" assigned by the predictor\n",
    "tasks = project.get_tasks()\n",
    "for task in tqdm(tasks,desc=\"correcting annotations\"):\n",
    "        task_id = task['id']\n",
    "        img_url = task['data']['image']\n",
    "\n",
    "        if len(task[\"annotations\"][0]['result'])>1:\n",
    "            results_to_keep = []\n",
    "            annot_id = task[\"annotations\"][0][\"id\"]\n",
    "            for annot in task['annotations'][0]['result']:\n",
    "                if annot['value']['rectanglelabels'][0] != 'wildlife':\n",
    "                    results_to_keep.append(annot)\n",
    "                    # print(annot['value'],annot['id'],end=\"\\n\")\n",
    "            # print(f\"Updating annotations {annot_id} from task {task_id}.\")\n",
    "            # print(results_to_keep)\n",
    "            project.update_annotation(annot_id,result=results_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(task['annotations']), len(task['annotations'][0]['result']), task['id'], task[\"annotations\"][0][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task['annotations'][0]['result'][0] #['value']['rectanglelabels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_keep = []\n",
    "for annot in task['annotations'][0]['result']:\n",
    "    if annot['value']['rectanglelabels'][0] != 'wildlife':\n",
    "        results_to_keep.append(annot)\n",
    "        print(annot['value'],annot['id'],end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.update_annotation(annotation_id=...,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up inference on intel, make changes inn ultralytics/nn/autobackend.py:\n",
    "```\n",
    "- device_name = \"AUTO:NPU,GPU,CPU\" # CPU, GPU, NPU, AUTO,\"AUTO:GPU,NPU\"\n",
    "- inference_mode = \"LATENCY\" # OpenVINO inference modes are 'LATENCY', 'THROUGHPUT' (not recommended), or 'CUMULATIVE_THROUGHPUT'\n",
    "- LOGGER.info(f\"Using OpenVINO {inference_mode} mode for inference...\")\n",
    "- ov_compiled_model = core.compile_model(\n",
    "                ov_model,\n",
    "                device_name=device_name,  # AUTO selects best available device, do not modify\n",
    "                config={\"PERFORMANCE_HINT\": inference_mode,\n",
    "                        \"CACHE_DIR\": os.environ[\"OPENVINO_CACHE_MODEL\"]}, # make sure to set environment variable\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using path_to_weights\n",
    "# go to ultralytics.nn.autobackend to modify ov_compiled device to \"AUTO:NPU,GPU,CPU\"\n",
    "\n",
    "use_sliding_window=True\n",
    "\n",
    "handler = Annotator(path_to_weights=r\"D:\\datalabeling\\base_models_weights\\best.pt\",\n",
    "                    is_yolo_obb=True,\n",
    "                    tilesize=640,\n",
    "                    overlapratio=0.1,\n",
    "                    use_sliding_window=use_sliding_window,\n",
    "                    confidence_threshold=0.5,\n",
    "                    # device=\"NPU\", # \"cpu\", \"cuda\"\n",
    "                    tag_to_append=f\"-sahi:{use_sliding_window}\",\n",
    "                    dotenv_path=\"../.env\")\n",
    "\n",
    "project_id = 3 # insert correct project_id by loooking at the url\n",
    "top_n=10\n",
    "handler.upload_predictions(project_id=project_id,top_n=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_studio_ml.utils import get_local_path\n",
    "from urllib.parse import unquote\n",
    "path = unquote(\"/data/local-files/?d=savmap_dataset_v2%5Cimages_splits%5C003a34ee6b7841e6851b8fe511ebe102_0.JPG\")\n",
    "get_local_path(path,download_resources=False)#,os.path.exists(get_local_path(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "from datalabeling.ml import Detector, Annotator\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster, FastMarkerCluster\n",
    "import pandas as pd\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load env variable, loads model cache location!!\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/09/2025 12:01:44 - INFO - datalabeling.ml.models -   Computing device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# provide correct alias, \"pt\", \"onnx\"\n",
    "alias = \"yolov11x-obb\"\n",
    "project_id = 96 # insert correct project_id by loooking at the url\n",
    "\n",
    "dotenv_path=\"../.env\"\n",
    "\n",
    "name = \"obb-detector\" # detector, \"obb-detector\"\n",
    "\n",
    "handler = Annotator(mlflow_model_alias=alias,\n",
    "                    mlflow_model_name=name,\n",
    "                    confidence_threshold=0.15,\n",
    "                    is_yolo_obb=name.strip() == \"obb-detector\",\n",
    "                    dotenv_path=dotenv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/09/2025 14:04:35 - INFO - datalabeling.ml.models -   Computing device: cuda\n"
     ]
    }
   ],
   "source": [
    "handler = Detector(path_to_weights=r\"C:/Users/Machine Learning/Desktop/workspace-wildAI/datalabeling/runs/mlflow/140168774036374062/f5b7124be14c4c89b8edd26bcf7a9a76/artifacts/weights/best.pt\",\n",
    "                   imgsz=960,\n",
    "                   tilesize=960,\n",
    "                   confidence_threshold=0.25,\n",
    "                   use_sliding_window=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir = r\"D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Camp 6-8\\Rep 1\"\n",
    "save_path = Path(image_dir).parent / \"detection_gps.csv\"\n",
    "\n",
    "exts = [\n",
    "        \"*.jpg\",\n",
    "        \"*.jpeg\",\n",
    "        \"*.png\",\n",
    "    ]\n",
    "exts = [e.lower() for e in exts] + [e.capitalize() for e in exts]\n",
    "\n",
    "image_paths = chain.from_iterable([Path(image_dir).glob(ext) for ext in exts])\n",
    "\n",
    "image_paths = list(Path(image_dir).glob('*.jpg'))[:9]\n",
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing predictions...: 100%|██████████| 9/9 [00:31<00:00,  3.51s/it]\n",
      "pred results as df: 100%|██████████| 94/94 [00:00<00:00, 2764.54it/s]\n"
     ]
    }
   ],
   "source": [
    "results = handler.predict_directory(\n",
    "        path_to_dir = None,\n",
    "        images_paths = image_paths,\n",
    "        return_gps = True,\n",
    "        return_coco = False,\n",
    "        as_dataframe = True,\n",
    "        save_path = None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>area</th>\n",
       "      <th>file_name</th>\n",
       "      <th>x_min</th>\n",
       "      <th>y_min</th>\n",
       "      <th>bbox_w</th>\n",
       "      <th>bbox_h</th>\n",
       "      <th>x_max</th>\n",
       "      <th>y_max</th>\n",
       "      <th>gps</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>px_Latitude</th>\n",
       "      <th>px_Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.883107</td>\n",
       "      <td>0</td>\n",
       "      <td>wildlife</td>\n",
       "      <td>4389</td>\n",
       "      <td>D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Camp 6-...</td>\n",
       "      <td>2378.009277</td>\n",
       "      <td>1579.267395</td>\n",
       "      <td>79.137451</td>\n",
       "      <td>62.371948</td>\n",
       "      <td>2457.146729</td>\n",
       "      <td>1641.639343</td>\n",
       "      <td>23.0 56.0m 56.524s S 30.0 33.0m 8.8846s E 830....</td>\n",
       "      <td>-23.949034</td>\n",
       "      <td>30.552468</td>\n",
       "      <td>830.694</td>\n",
       "      <td>-23.948800</td>\n",
       "      <td>30.552100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.857370</td>\n",
       "      <td>0</td>\n",
       "      <td>wildlife</td>\n",
       "      <td>4049</td>\n",
       "      <td>D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Camp 6-...</td>\n",
       "      <td>2396.050964</td>\n",
       "      <td>870.193237</td>\n",
       "      <td>61.375610</td>\n",
       "      <td>67.708618</td>\n",
       "      <td>2457.426575</td>\n",
       "      <td>937.901855</td>\n",
       "      <td>23.0 56.0m 56.524s S 30.0 33.0m 8.8846s E 830....</td>\n",
       "      <td>-23.949034</td>\n",
       "      <td>30.552468</td>\n",
       "      <td>830.694</td>\n",
       "      <td>-23.948656</td>\n",
       "      <td>30.552104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.855882</td>\n",
       "      <td>0</td>\n",
       "      <td>wildlife</td>\n",
       "      <td>4732</td>\n",
       "      <td>D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Camp 6-...</td>\n",
       "      <td>2133.558807</td>\n",
       "      <td>2115.529785</td>\n",
       "      <td>93.452026</td>\n",
       "      <td>60.395996</td>\n",
       "      <td>2227.010834</td>\n",
       "      <td>2175.925781</td>\n",
       "      <td>23.0 56.0m 56.524s S 30.0 33.0m 8.8846s E 830....</td>\n",
       "      <td>-23.949034</td>\n",
       "      <td>30.552468</td>\n",
       "      <td>830.694</td>\n",
       "      <td>-23.948908</td>\n",
       "      <td>30.552045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.855088</td>\n",
       "      <td>0</td>\n",
       "      <td>wildlife</td>\n",
       "      <td>4128</td>\n",
       "      <td>D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Camp 6-...</td>\n",
       "      <td>2614.296637</td>\n",
       "      <td>1226.002075</td>\n",
       "      <td>63.287691</td>\n",
       "      <td>66.551514</td>\n",
       "      <td>2677.584328</td>\n",
       "      <td>1292.553589</td>\n",
       "      <td>23.0 56.0m 56.524s S 30.0 33.0m 8.8846s E 830....</td>\n",
       "      <td>-23.949034</td>\n",
       "      <td>30.552468</td>\n",
       "      <td>830.694</td>\n",
       "      <td>-23.948729</td>\n",
       "      <td>30.552152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.852219</td>\n",
       "      <td>0</td>\n",
       "      <td>wildlife</td>\n",
       "      <td>3220</td>\n",
       "      <td>D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Camp 6-...</td>\n",
       "      <td>2387.762024</td>\n",
       "      <td>1427.064575</td>\n",
       "      <td>80.992432</td>\n",
       "      <td>43.456421</td>\n",
       "      <td>2468.754456</td>\n",
       "      <td>1470.520996</td>\n",
       "      <td>23.0 56.0m 56.524s S 30.0 33.0m 8.8846s E 830....</td>\n",
       "      <td>-23.949034</td>\n",
       "      <td>30.552468</td>\n",
       "      <td>830.694</td>\n",
       "      <td>-23.948767</td>\n",
       "      <td>30.552103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  category_id category_name  area  \\\n",
       "0  0.883107            0      wildlife  4389   \n",
       "1  0.857370            0      wildlife  4049   \n",
       "2  0.855882            0      wildlife  4732   \n",
       "3  0.855088            0      wildlife  4128   \n",
       "4  0.852219            0      wildlife  3220   \n",
       "\n",
       "                                           file_name        x_min  \\\n",
       "0  D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Camp 6-...  2378.009277   \n",
       "1  D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Camp 6-...  2396.050964   \n",
       "2  D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Camp 6-...  2133.558807   \n",
       "3  D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Camp 6-...  2614.296637   \n",
       "4  D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Camp 6-...  2387.762024   \n",
       "\n",
       "         y_min     bbox_w     bbox_h        x_max        y_max  \\\n",
       "0  1579.267395  79.137451  62.371948  2457.146729  1641.639343   \n",
       "1   870.193237  61.375610  67.708618  2457.426575   937.901855   \n",
       "2  2115.529785  93.452026  60.395996  2227.010834  2175.925781   \n",
       "3  1226.002075  63.287691  66.551514  2677.584328  1292.553589   \n",
       "4  1427.064575  80.992432  43.456421  2468.754456  1470.520996   \n",
       "\n",
       "                                                 gps   Latitude  Longitude  \\\n",
       "0  23.0 56.0m 56.524s S 30.0 33.0m 8.8846s E 830.... -23.949034  30.552468   \n",
       "1  23.0 56.0m 56.524s S 30.0 33.0m 8.8846s E 830.... -23.949034  30.552468   \n",
       "2  23.0 56.0m 56.524s S 30.0 33.0m 8.8846s E 830.... -23.949034  30.552468   \n",
       "3  23.0 56.0m 56.524s S 30.0 33.0m 8.8846s E 830.... -23.949034  30.552468   \n",
       "4  23.0 56.0m 56.524s S 30.0 33.0m 8.8846s E 830.... -23.949034  30.552468   \n",
       "\n",
       "   Elevation  px_Latitude  px_Longitude  \n",
       "0    830.694   -23.948800     30.552100  \n",
       "1    830.694   -23.948656     30.552104  \n",
       "2    830.694   -23.948908     30.552045  \n",
       "3    830.694   -23.948729     30.552152  \n",
       "4    830.694   -23.948767     30.552103  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['file_name'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.read_csv(r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\.tmp\\detections.csv\")\n",
    "\n",
    "# results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_results = pd.read_csv(r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\.tmp\\detections.csv\")\n",
    "\n",
    "# print(loaded_results.head())\n",
    "\n",
    "# # folium map\n",
    "# m = folium.Map(location=[loaded_results.Latitude.mean(), loaded_results.Longitude.mean()], zoom_start=13)\n",
    "\n",
    "# folium.TileLayer('Esri.WorldImagery', name='Esri Satellite', control=True).add_to(m)\n",
    "\n",
    "# marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "# for idx, row in loaded_results.iterrows():\n",
    "#     folium.Marker(\n",
    "#         location=[row.Latitude, row.Longitude],\n",
    "#         popup=row.name\n",
    "#     ).add_to(marker_cluster)\n",
    "\n",
    "# m.save('.tmp/cluster_map_2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Latitude', ylabel='Longitude'>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAloAAAGwCAYAAABxbMuTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAASy9JREFUeJzt3Ql8TXf+//FPkFBb7FvtSwm1K9J2qsRaNYT+BmWstdVWqqZ0ZmoZFdVSXdCOtYpUFF1Qa+kgTKX2oqV2Ca1KrLHl/B+f7+937//ebJLIcZOb1/PxOOTec3KWbPd9v9/P93t8LMuyBAAAAOkuW/rvEgAAAIqgBQAAYBOCFgAAgE0IWgAAADYhaAEAANiEoAUAAGATghYAAIBNcti1Y9xfXFycnD9/XvLlyyc+Pj6ePh0AAJACOgXp1atXpVSpUpItW/JtVgQtD9KQVaZMGU+fBgAASIMzZ85I6dKlk92GoOVB2pLl+Eblz5/f06cDAABS4MqVK6ahxPE6nhyClgc5ugs1ZBG0AADIXFJS9kMxPAAAgE0IWgAAADYhaAEAANiEoAUAAGATghYAAIBNCFoAAAA2IWgBAAB4Y9CaNWuW1KpVyzmPVGBgoKxdu9a5PjY2VgYPHiyFCxeWvHnzSqdOneTChQvJ7rNXr15mXgvXpXXr1m7blC9fPsE2ISEhzvVbtmyR9u3bS8mSJSVPnjxSp04dWbx4sds+FixYkGAfuXLlSrevDQAAyPw8OmGpTluvAadKlSrmvkELFy40AWfPnj1So0YNGTFihKxevVrCwsLE399fhgwZIh07dpTt27cnu18NVvPnz3c+zpkzZ4JtJkyYIP369XM+dp3ddceOHSYA/u1vf5PixYvLN998Iz169DDn8Pzzzzu303B49OhR52PuVwgAADJM0GrXrp3b40mTJplWrp07d5oQNnfuXFmyZIk0a9bMrNfwFBAQYNY3btw4yf1qsCpRokSyx9ZgldQ2Y8eOdXs8fPhwWb9+vaxYscItaGmwut9xAABA1pVharTu3bsnoaGhcv36ddOFGBERIXfu3JHmzZs7t6lWrZqULVtWwsPDk92Xdv0VK1ZMqlatKoMGDZJLly4l2EZb0rRLsm7dujJ16lS5e/dusvuMiYmRQoUKuT137do1KVeunLnfkbbEHTp0KNl93Lp1y9wfyXUBAAD2KP/6aufiKR6/1+GBAwdMsNJ6LK3DWrlypVSvXl327t0rfn5+UqBAAbfttSsvKioq2W5D7V6sUKGCHD9+3LROtWnTxoSz7Nmzm22GDRsm9erVM8FJuwnHjBkjkZGRMm3atET3uWzZMvnhhx/k448/dj6nIW7evHmmi1FD2DvvvCNPPvmkCVtJ3cl78uTJMn78+DR+pQAAQEo8MfFb+e36PbfnNGyVzOcr4W+0lIfJx9LiKA+6ffu2nD592oSV5cuXy5w5c2Tr1q0maPXu3du0Arlq2LChNG3aVKZMmZKi/f/6669SqVIl2bhxowQFBSW6jQamAQMGmBaq+PVc3333neku1C5NrdNKira+abdm165dZeLEiYluo9fiej2Ou3/rtXNTaQAA0kdyLVgnQ9o+8P719VvrtlPy+u3xrkNttapcubLUr1/ftPjUrl1bZsyYYWqfNIRFR0e7ba+jDlNTF1WxYkUpUqSIHDt2LMltGjVqZLoOT5486fa8Bj6tI5s+fXqyIUv5+vqabsjkjqMhzjHC0rEAAID0c79uwofdjejxoBVfXFycafXR4KXhZdOmTc51OsJPW7+0qzGlzp49a2q0dKqGpGjrWbZs2Uxdl2udV9u2bU3LWf/+/VNUY6bdoMkdBwAAZC0erdHS2iitn9IC96tXr5oRhhpw1q1bZ5rk+vbtKyNHjjS1VNr6M3ToUBOyXEccaoG8toQFBwebrj+tgdL5trTVS2u0Ro8ebVrMWrVqZbbXWq1du3aZ7kcdeaiPdRqJ7t27S8GCBd26C3W0oe7LUROmrW+OgnidHkLPQ/etrW5aUH/q1Cl56aWXPPK1BAAAGY9Hg9bFixdNl5wWomuw0sJyDVktWrQw67XLTluaNOxoK5eGpZkzZ7rtQ1u5tI9UabH7/v37zXxcGn5KlSolLVu2NDVTjtor/V9HN44bN87sU4vmNWhpoHPQz79x44YJcLo4NGnSxARBdfnyZTMPl4YwDWjaAqeF9VrIDwAAPENrsOyu0cpUxfBZWWqK6QAAQMoETlovkVfvJHg+vUYdpub12+PTOwAAAKQnR5hybdl62C1ZDgQtAADglU56KFxl6FGHAAAA3oKgBQAAYBOCFgAAgE0IWgAAADYhaAEAANiEoAUAAGATghYAAIBNCFoAAAA2IWgBAADYhKAFAABgE4IWAACATQhaAAAANiFoAQAA2CSHXTsGAAAo//pq58cnQ9pKVkPQAgAA6a7mm6vl6q2EoavgIz6y583nJKug6xAAAKS7+CHL4fJNS7ISghYAALCtuzAt670JQQsAAMAmBC0AAACbELQAAEC6ut/owpNZaPQhQQsAAKQ7HV2Ymue9FdM7AACAdOeYwqE882gBAADY42QWDFeu6DoEAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAAPDGoDVr1iypVauW5M+f3yyBgYGydu1a5/rY2FgZPHiwFC5cWPLmzSudOnWSCxcuJLvPXr16iY+Pj9vSunVrt23Kly+fYJuQkBDn+i1btkj79u2lZMmSkidPHqlTp44sXrw4wbHCwsKkWrVqkitXLqlZs6asWbMmXb4uAADAO3g0aJUuXdoEnIiICNm9e7c0a9bMBJxDhw6Z9SNGjJCvv/7aBJqtW7fK+fPnpWPHjvfdrwaryMhI57J06dIE20yYMMFtm6FDhzrX7dixwwTAL774Qvbv3y+9e/eWHj16yDfffOO2TdeuXaVv376yZ88e6dChg1kOHjyYbl8fAACQuflYlmVJBlKoUCGZOnWqvPDCC1K0aFFZsmSJ+VgdOXJEAgICJDw8XBo3bpxki1Z0dLSsWrUqyWNoi9Yrr7xilpRq27atFC9eXObNm2ced+7cWa5fv+4WvvSctPVr9uzZie7j1q1bZnG4cuWKlClTRmJiYkyLHgAAyPj09dvf3z9Fr98Zpkbr3r17EhoaasKLdiFqK9edO3ekefPmzm20m65s2bImaCVHu/6KFSsmVatWlUGDBsmlS5cSbKMtadolWbduXRPs7t69m+w+9YupIdBBz8H13FSrVq2SPbfJkyebb4xj0ZAFAAC8Vw5Pn8CBAwdMsNJ6LK3DWrlypVSvXl327t0rfn5+UqBAAbfttVUpKioq2W5D7V6sUKGCHD9+XMaOHStt2rQxASh79uxmm2HDhkm9evVMcNIuwDFjxpjuw2nTpiW6z2XLlskPP/wgH3/8sfM5PQc9l9Scmx5n5MiRCVq0AACAd/J40NJWJw1V2mK0fPly6dmzp6nHSqsuXbo4P9YCda21qlSpkmnlCgoKMs+7hh1dr4FuwIABpsUpZ86cbvv77rvvTI3Wv//9b6lRo4Y8CN13/P0DAADv5fGuQw05lStXlvr165ugU7t2bZkxY4aUKFFCbt++beqtXOmoQ12XUhUrVpQiRYrIsWPHktymUaNGpuvw5MmTbs9r4GvXrp1Mnz7dFMO70nOIPwIytecGAAC8m8eDVnxxcXGmYFyDl6+vr2zatMm57ujRo3L69GnT1ZhSZ8+eNTVaOlVDUrRFLVu2bKauy0FbwLQAfsqUKdK/f/8En6Pn4HpuasOGDak6NwAA4N082nWoNUtaP6UF7levXjUjDDXgrFu3zhSL69QJ2s2ntVRa1a9TMGiQcR1xqAXy2hIWHBws165dk/Hjx5v5trRlSWu0Ro8ebVrMtFBdaa3Wrl27pGnTppIvXz7zWKeR6N69uxQsWNDZXfj888/L8OHDzb4cdVfa+uYoiNd1TZo0kXfffdcEMi3k1ykqPvnkE498LQEAQAZkeVCfPn2scuXKWX5+flbRokWtoKAga/369c71N2/etF5++WWrYMGCVu7cua3g4GArMjLSbR96CfPnzzcf37hxw2rZsqXZl6+vr9l3v379rKioKOf2ERERVqNGjSx/f38rV65cVkBAgPXWW29ZsbGxzm169uxp9ht/adKkiduxly1bZj322GPm/GvUqGGtXr06VdcfExNj9qv/AwCAzCE1r98Zbh6trCQ183AAAICMIVPOowUAAOBtCFoAAAA2IWgBAADYhKAFAABgE4IWAACATQhaAAAANiFoAQAA2ISgBQAAYBOCFgAAgE0IWgAAADYhaAEAANiEoAUAAGATghYAAIBNCFoAAAA2IWgBAADYhKAFAABgE4IWAACATQhaAAAANiFoAQAA2ISgBQAAYBOCFgAAgE0IWgAAADYhaAEAANgkh107BgAgKyj/+mrnxydD2nr0XJDxELQAAEiDwEnrJfLqnQShq4x/TvnPmOYeOy9kLHQdAgCQBvFDlsOZmFsP/VyQcRG0AAB4gO7CtKxH1kHQAgAAsAlBCwAAwCYELQAAUul+owsZfQgHghYAAGmgowtT8zyyJqZ3AAAgDRxTODCPFpJD0AIA4AEQrpBhuw5nzZoltWrVkvz585slMDBQ1q5d61wfGxsrgwcPlsKFC0vevHmlU6dOcuHChWT32atXL/Hx8XFbWrdu7bZN+fLlE2wTEhLidlzdT82aNSVHjhzSoUOHBMfZsmVLgn3oEhUVlS5fGwAAkPl5tEWrdOnSJuBUqVJFLMuShQsXSvv27WXPnj1So0YNGTFihKxevVrCwsLE399fhgwZIh07dpTt27cnu18NVvPnz3c+zpkzYX/5hAkTpF+/fs7H+fLlc3587949eeSRR2TYsGHyxRdfJHuso0ePmpDoUKxYsRRfPwAA8G4eDVrt2rVzezxp0iTTyrVz504TwubOnStLliyRZs2amfUangICAsz6xo0bJ7lfDVYlSpRI9tgarJLaJk+ePOY8lIa66OjoJPejwapAgQLJHgsAAGRNGWbUobYihYaGyvXr100XYkREhNy5c0eaN///94uqVq2alC1bVsLDw5Pdl3braQCqWrWqDBo0SC5dupRgG21J0y7JunXrytSpU+Xu3btpOu86depIyZIlpUWLFvdtabt165ZcuXLFbQEAAN7L48XwBw4cMMFK66K0DmvlypVSvXp12bt3r/j5+SVoLSpevHiydVDabajdixUqVJDjx4/L2LFjpU2bNiacZc+e3WyjXYL16tWTQoUKyY4dO2TMmDESGRkp06ZNS/F5a7iaPXu2NGjQwASoOXPmyLPPPiu7du0y+07M5MmTZfz48Sk+BgAAyNx8LC2O8qDbt2/L6dOnJSYmRpYvX24Cy9atW03Q6t27twkxrho2bChNmzaVKVOmpGj/v/76q1SqVEk2btwoQUFBiW4zb948GTBggFy7di1BPZcWxWvX4apVq+57rCZNmpgWt0WLFiW6Xq/F9Xq0RatMmTLm2l3rvAAAQMalr99aO56S12+Pdx1qq1XlypWlfv36psWndu3aMmPGDFM/pSEsfn2Ujjq8X/2Vq4oVK0qRIkXk2LFjSW7TqFEj03V48uTJB7oWDYHJHUdDnGOEpWMBAADey+NBK764uDjT6qPBy9fXVzZt2uQ2wk9bv7SrMaXOnj1rarS0qy8p2nqWLVu2Bx4xqPtJ7jgAACBr8WiNltZGaf2UdrddvXrVjDDUQvZ169aZJrm+ffvKyJEjTS2Vtv4MHTrUhCzXEYdaIK8tYcHBwabrT2ugdL4tbfXSGq3Ro0ebFrNWrVqZ7bVWS+uotPtRRx7qY51Gonv37lKwYEHnfn/66SfTovbHH3+Yc9MQ5Sh+V++9956pA9NpKLS+TLs8N2/eLOvXr3/oX0cAAJAxeTRoXbx4UXr06GEK0TVY6eSlGrJ0BJ+aPn26aWnS4KStXBqWZs6c6bYPbeXSPlKlxe779+8383Fpl2OpUqWkZcuWMnHiRGftlf6voxvHjRtn9qlhSYOWBjpXzz33nJw6dcr5WEcnKkdJm4awV199Vc6dOye5c+c25651YBrgAAAAMkQxfFaWmmI6AACQMWSqYngAAABvRdACAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAAPDGoDVr1iypVauW5M+f3yyBgYGydu1a5/rY2FgZPHiwFC5cWPLmzSudOnWSCxcuJLvPXr16iY+Pj9vSunVrt23Kly+fYJuQkBC34+p+atasKTly5JAOHTokeqwtW7ZIvXr1JGfOnFK5cmVZsGDBA39NAACA9/Bo0CpdurQJOBEREbJ7925p1qyZtG/fXg4dOmTWjxgxQr7++msJCwuTrVu3yvnz56Vjx4733a8Gq8jISOeydOnSBNtMmDDBbZuhQ4c61927d08eeeQRGTZsmDRv3jzRY5w4cULatm0rTZs2lb1798orr7wiL730kqxbt+6BviYAAMB75PDkwdu1a+f2eNKkSaaVa+fOnSaEzZ07V5YsWWICmJo/f74EBASY9Y0bN05yv9rCVKJEiWSPnS9fviS3yZMnjzkPtX37domOjk6wzezZs6VChQry7rvvmsd6Xtu2bZPp06dLq1atEt3vrVu3zOJw5cqVZM8RAABkbhmmRktbkUJDQ+X69eumC1Fbue7cuePWolStWjUpW7ashIeHJ7sv7dIrVqyYVK1aVQYNGiSXLl1KsI22pGmXZN26dWXq1Kly9+7dVJ2vnkP81i4NWMmd2+TJk8Xf39+5lClTJlXHBAAAmYtHW7TUgQMHTLDSuiitw1q5cqVUr17ddMf5+flJgQIF3LYvXry4REVFJdttqN2L2tp0/PhxGTt2rLRp08YEoOzZs5tttEtQa6sKFSokO3bskDFjxpjuw2nTpqX4vPUc9Fzin5u2Ut28edN0Pcanxxk5cqTzsW5L2AIAwHt5PGhpq5OGqpiYGFm+fLn07NnT1GOlVZcuXZwfazG7FttXqlTJtHIFBQWZ513Djq7XQDdgwADT4qTdjnbRfdu5fwAAkLF4vOtQQ46O2Ktfv74JOrVr15YZM2aY+qnbt28nqI/SUYf3q79yVbFiRSlSpIgcO3YsyW0aNWpkug5PnjyZ4v3qOcQfAamPdfRkYq1ZAAAg6/F40IovLi7OFIxr8PL19ZVNmzY51x09elROnz5tuhpT6uzZs6ZGq2TJkkluoy1q2bJlM3VdKaXn4HpuasOGDak6NwAA4N082nWoNUtaP6UF7levXjUjDLWLT6dI0GLxvn37mm4+raXSliKdgkGDjOuIQy2Q15aw4OBguXbtmowfP97Mt6UtTlqjNXr0aNNi5hgJqLVau3btMtMy6MhDfazTSHTv3l0KFizo3O9PP/1kWtT++OMPc24axlSdOnXM/wMHDpQPP/zQ7L9Pnz6yefNmWbZsmaxevfqhfx0BAEAGZXlQnz59rHLlyll+fn5W0aJFraCgIGv9+vXO9Tdv3rRefvllq2DBglbu3Lmt4OBgKzIy0m0fegnz5883H9+4ccNq2bKl2Zevr6/Zd79+/ayoqCjn9hEREVajRo0sf39/K1euXFZAQID11ltvWbGxsW771c/VfcdfXH333XdWnTp1zPlXrFjReR4pFRMTY/ap/wMAgMwhNa/fPvpPWgKa1jRp65O2Gr344oumdUgnFNWWJx09iPvTUYfacqcDAfTrBgAAvOv1O01dh6dOnTLTKGi9lNZTtWjRwgStKVOmmMc6mScAAEBWl6Zi+OHDh0uDBg3k8uXLbiPstE4qfoE4AABAVpWmFq3//Oc/ZqJPnZoh/s2az507l17nBgAAkPVatHQKBr1lTmJTKWgXIgAAANIYtFq2bCnvvfee87GPj4+ZWuHNN9+U5557Lj3PDwAAINNK06hDbbnSean0U3/55RdTr6X/6wzs33//faom/szKGHUIAIB3v34/0PQOoaGhsn//ftOapTdp7tatG7efSQWCFgAAmY/t0zuYT8yRw8ymDgAAgAcMWl999VVKN5U///nPKd4WAABAsnrQ6tChg9tjLYCP3+uoz6nERiQCAABkNdlSM6WDY1m/fr25ufLatWslOjraLPqx1ml9++239p4xAABAJpGmGq1XXnnF3Gbn6aefdj6noxBz584t/fv3l8OHD6fnOQIAAGSdebT0RtIFChRI8LxW4J88eTI9zgsAACBrBq0nnnhCRo4cKRcuXHA+px+/9tpr0rBhw/Q8PwAAgKwVtObNmyeRkZFStmxZqVy5sln0Y73P4dy5c9P/LAEAALJKjZYGK52odMOGDXLkyBHzXEBAgDRv3tw58hAAACCrS/PM8HhwzAwPAEDmY/vM8BMmTEh2/T//+c+07BYAAMCrpClorVy50u3xnTt35MSJE+a2PJUqVSJoAQAApDVo7dmzJ9FmtF69eklwcHB6nBcAAEDWHHWYGO2jHD9+vPzjH/9Ir10CAABkaukWtJQWhekCAACANHYdvv/++26PdeCizqu1aNEiadOmTXqdGwAAQNYLWtOnT3d7nC1bNilatKj07NlTxowZk17nBgAAkPWClo4wBAAAgA01Wn369JGrV68meP769etmHQAAANIYtBYuXCg3b95M8Lw+9+mnn6bHeQEAAGStrkOdK0sL33XRFq1cuXI51927d0/WrFkjxYoVs+M8AQAAvDtoFShQwNw0WpfHHnsswXp9XufSAgAAQCqD1nfffWdas5o1ayZffPGFFCpUyLnOz89PypUrJ6VKlbLjPAEAALw7aDVp0sQ56rBs2bKmBQsAAAAPGLT2798vjz/+uJkzS2d/P3DgQJLb1qpVK6W7BQAA8FopDlp16tSRqKgoU+yuH2trlnYjxqfPa2E8AABAVpfi6R20u1Bnf3d8/Ouvv5r/4y/6fErNmjXLtH7pDal1CQwMlLVr1zrXx8bGyuDBg6Vw4cKSN29e6dSpk1y4cCHZffbq1ctZsO9YWrdu7bZN+fLlE2wTEhKSoAXvT3/6kxlZWaZMGXn77bfd1i9YsCDBPlxHYQIAAKS4RUsL3RP7+EGULl3aBJwqVaqY1jGdn6t9+/ayZ88eqVGjhowYMUJWr14tYWFh4u/vL0OGDJGOHTvK9u3bk92vBqv58+c7H+fMmTPBNhMmTJB+/fo5H+fLl89tGouWLVtK8+bNZfbs2aabVCdi1VGX/fv3d26n4fDo0aPOx9SsAQCAB74Fz1dffZXo845WncqVK0uFChXuu5927dq5PZ40aZJp5dq5c6cJYXPnzpUlS5aYUY5Kw1NAQIBZ37hx4yT3q8GqRIkSyR5bg1VS2yxevFhu374t8+bNM6MpNfTt3btXpk2b5ha09HrvdxwAAJB1pSlodejQIdEaLcdz+v/TTz8tq1atkoIFC6Zon1rXpS1Xehsf7UKMiIiQO3fumFYlh2rVqpnRjuHh4ckGrS1btphaMj22hrR//etfpvvRlbakTZw40ezvxRdfNK1nOXL875dD9//MM8+YkOXQqlUrmTJlily+fNl5TdeuXTOte3FxcVKvXj156623TChLyq1bt8zi2nIGAAC8V5puwbNhwwZ54oknzP86AlEX/bhRo0byzTffyPfffy+XLl2SUaNG3Xdf2i2n9VfaCjVw4EBZuXKlVK9e3RTea9DR7jpXxYsXN+uS6zbU2wBt2rTJBKOtW7dKmzZt3Ar0hw0bJqGhoWZesAEDBpiANHr0aOd63b8eJ/5xHetU1apVTYvXl19+KZ999pkJW08++aScPXs2yXObPHmy6QJ1LFr7BQAAvJiVBjVq1LC2b9+e4Plt27ZZ1atXNx9v2LDBKlOmzH33devWLeuXX36xdu/ebb3++utWkSJFrEOHDlmLFy+2/Pz8Emz/xBNPWKNHj07xuR4/flyb3ayNGzcmuc3cuXOtHDlyWLGxseZxixYtrP79+7tto+ek+/npp58S3cft27etSpUqWX//+9+TPI7uPyYmxrmcOXPG7FM/BpDxhO46ZQ0P/dFa9sNpT58KgAxEX7dT+vqdpq7D48ePm0Lw+PQ5x6hDLXD//fff77svbbXSmi5Vv359+eGHH2TGjBnSuXNnUycVHR3t1qqlow5TUxdVsWJFKVKkiBw7dkyCgoIS3UZb4u7evSsnT540LVW6//ijGx2Pkzq2r6+v1K1b1xwnKdpql1hhPoCM5cDZaAmeuUPuxv1vecSqPedlzIoD8tXgp6T6o/6ePj0A3t51qIHotddek99++835nH6s3W/apah++eWXNHWNaRec1jHpMTS8aBegg47wO336tKnhSintytNuzJIlSya5jRa660Ssjhti6/61+1NrxBy0a1RDWFI1Z9o1qd2gyR0HQObgGrIc9PGfP0p+xDMApEvQ0tGAOmeWjgzU1ihd9GNtEZozZ46zUPzvf/97svsZM2aMCTT6eRpS9LEWsnfr1s3UMPXt21dGjhxpaqm0OL53794mBLkWwmuBvNZ1OY6pAVBHJeo+NaTpdBF6flrM7ih0f++992Tfvn2m9U1HGGohfPfu3Z0hSovjtaVNj3/o0CH5/PPPTSubnovr9BDr1683+/jxxx/N5586dUpeeumltHxJAWQQn//3dIKQ5aDPh+0+89DPCUDmlaauQ23Z+emnn0zQ+Pnnn53PtWjRwrQMOUYm3s/FixelR48eEhkZaYKVTl66bt06sx81ffp0sz+dqFRbuTQszZw5020f2sqlxfgqe/bsZqJRnY9Luxz1Btc6H5aOLnR02en/Wgg/btw4s0+dhkKDlmuI0nPRa9PJUrVlTbse//nPf7pN7aCjD3UeLi2O14Cm2+3YscMU8gPIvMJPXEp2/fbjv8v/NGAgC4CU8dFCrRRui3Sm0ztoqNOgmFjNGwDPtGj9bUXS93Kd+kItghaQxV1Jxet3mlq0lHbL6aKtUlpX5UqnPQCAzKhzw7LyxqqDiXYf5sjmQ8gCYH+N1vjx402XnAYtHVmo3WiuCwBkZjq6UEOVK32szwOA7V2HOrJOb7L817/+NbWfChd0HQIZmxa+a03WU5WK0JIF4OF1Her8VjoLOgB4Mw1XBCwAD73rUKcw0Js9AwAAIJ1btGJjY+WTTz6RjRs3mikZdGJRV9OmTUvLbgEAALxKmoKWzlVVp04d8/HBgwfd1vn4uBeQAgAAZFVpClo6UzsAAABsqNGKfy9BXQAAAJAOQUsnKNV7/enQxnLlypmlQIEC5lY38ScvBQAAyKrS1HX4xhtvmBtLh4SEyFNP/e8Eftu2bTP3D9RC+UmTJqX3eQIAAGSNCUv1Zs2zZ8+WP//5z27Pf/nll/Lyyy/LuXPn0vMcvRYTlgIA4N2v32nqOvzjjz+kWrVqCZ7X53QdAAAA0hi0ateuLR9++GGC5/U5nVcLAAAAaazR0vsctm3b1kxYGhgYaJ4LDw+XM2fOyJo1a9L7HAEAALJOi1aTJk3k559/luDgYImOjjZLx44d5dChQ7Jo0aL0P0sAAICsUgyflH379km9evXk3r176bVLr0YxPAAAmY/txfAAAAC4P4IWAACATQhaAAAAGWHUoRa8J0eL4gEAAJCGoKWFX/db36NHj9TsEgAAwGulKmjNnz/fvjMBAADwMtRoAQAA2ISgBQAAYBOCFgAAgE0IWgAAADYhaAEAANiEoAUAAGATghYAAIBNCFoAAAA2IWgBAADYhKAFAABgE4IWAACANwatWbNmSa1atSR//vxmCQwMlLVr1zrXx8bGyuDBg6Vw4cKSN29e6dSpk1y4cCHZffbq1Ut8fHzcltatW7ttU758+QTbhISEuG2zf/9++dOf/iS5cuWSMmXKyNtvv53gWGFhYVKtWjWzTc2aNWXNmjUP/DUBAADew6NBq3Tp0ibgREREyO7du6VZs2bSvn17OXTokFk/YsQI+frrr02g2bp1q5w/f146dux43/1qsIqMjHQuS5cuTbDNhAkT3LYZOnSoc92VK1ekZcuWUq5cOXNuU6dOlXHjxsknn3zi3GbHjh3StWtX6du3r+zZs0c6dOhgloMHD6bb1wcAAGRuPpZlWZKBFCpUyASbF154QYoWLSpLliwxH6sjR45IQECAhIeHS+PGjZNs0YqOjpZVq1YleQxt0XrllVfMklRL2xtvvCFRUVHi5+dnnnv99dfNPvUcVOfOneX69evyzTffOD9Pz6lOnToye/bsRPd769Yts7gGOm0ti4mJMS16AAAg49PXb39//xS9fmeYGq179+5JaGioCS/ahagtSXfu3JHmzZs7t9FuurJly5qglZwtW7ZIsWLFpGrVqjJo0CC5dOlSgm20JU27JOvWrWuC3d27d53rdP/PPPOMM2SpVq1aydGjR+Xy5cvObVzPzbFNcuc2efJk841xLBqyAACA98rh6RM4cOCACVZaj6V1WCtXrpTq1avL3r17TdApUKCA2/bFixc3LU3JdRtq92KFChXk+PHjMnbsWGnTpo0JQNmzZzfbDBs2TOrVq2daz7QLcMyYMab7cNq0aWa97l8/P/5xHesKFixo/nc8l9Jz0+OMHDkyQYsWAADwTh4PWtrqpKFKm9+WL18uPXv2NPVYadWlSxfnx1qgrsX2lSpVMq1cQUFB5nnXsKPrNdANGDDAtDjlzJlT7KL7tnP/AAAgY/F416GGnMqVK0v9+vVN0Kldu7bMmDFDSpQoIbdv3zb1Vq501KGuS6mKFStKkSJF5NixY0lu06hRI9N1ePLkSfNY9x9/dKPjsePYSW2TmnMDAADezeNBK764uDhTMK7By9fXVzZt2uRcpzVSp0+fNl2NKXX27FlTo1WyZMkkt9EWtWzZspm6LqX7//77702NmMOGDRtM65t2Gzq2cT03xzapOTcAAODlLA96/fXXra1bt1onTpyw9u/fbx77+PhY69evN+sHDhxolS1b1tq8ebO1e/duKzAw0Cyuqlataq1YscJ8fPXqVWvUqFFWeHi42efGjRutevXqWVWqVLFiY2PNNjt27LCmT59u7d271zp+/Lj12WefWUWLFrV69Ojh3Gd0dLRVvHhx669//at18OBBKzQ01MqdO7f18ccfO7fZvn27lSNHDuudd96xDh8+bL355puWr6+vdeDAgRRff0xMjI74NP8DAIDMITWv3x4NWn369LHKlStn+fn5mbATFBTkDFnq5s2b1ssvv2wVLFjQBJ3g4GArMjLSbR96ofPnzzcf37hxw2rZsqXZl4Ye3Xe/fv2sqKgo5/YRERFWo0aNLH9/fytXrlxWQECA9dZbbzmDmMO+ffusp59+2sqZM6f16KOPWiEhIQnOf9myZdZjjz1mzr9GjRrW6tWrU3X9BC0AADKf1Lx+Z7h5tLKS1MzDAQAAMoZMOY8WAACAt/H49A4APO/X367JqT9uSPnCeaRCkTyePh0A8BoELSALi75xW4Yt3Svf//Kb87lnqhSVD7rWFf/cvh49NwDwBnQdAlmYhqztx353e04fD126x2PnBADehKAFZOHuQm3JuhdvPIw+1udP/H7dY+cGAN6CoAVkUVqTlZyTlwhaAPCgCFpAFlWuUO5k12thPADgwRC0gCyqYtG8pvA9u4+P2/P6WJ9n9CEAPDiCFpCF6ejCpyoXcXtOH+vzAIAHx/QOQBamUzh82rehKXzXmizm0QKA9EXQAmDCFQELANIfXYcAAAA2IWgBAADYhKAFAABgE4IWAACATQhaAAAANiFoAQAA2ISgBQAAYBOCFgAAgE0IWgAAADYhaAEAANiEoAUAAGATghYAAIBNCFoAAAA2IWgBAADYhKAFAABgE4IWAACATQhaAAAANiFoAQAA2ISgBQAAYBOCFgAAgE0IWgAAADYhaAEAAHhj0Jo1a5bUqlVL8ufPb5bAwEBZu3atc31sbKwMHjxYChcuLHnz5pVOnTrJhQsXkt1nr169xMfHx21p3bp1otveunVL6tSpY7bZu3ev27ply5aZdblz55Zy5crJ1KlT3dZv2bIlwXF0iYqKeqCvCQAA8B45PHnw0qVLS0hIiFSpUkUsy5KFCxdK+/btZc+ePVKjRg0ZMWKErF69WsLCwsTf31+GDBkiHTt2lO3btye7Xw1W8+fPdz7OmTNnotuNHj1aSpUqJfv27XN7XsNet27d5IMPPpCWLVvK4cOHpV+/fvLII4+Yc3B19OhRExIdihUrlsavBgAA8DYeDVrt2rVzezxp0iTTyrVz504TwubOnStLliyRZs2amfUangICAsz6xo0bJ7lfDVYlSpRI9tgaptavXy9ffPGFWyuaWrRokXTo0EEGDhxoHlesWFHGjBkjU6ZMMS1s2nLlGqwKFCiQpusHAADeLcPUaN27d09CQ0Pl+vXrpgsxIiJC7ty5I82bN3duU61aNSlbtqyEh4cnuy/t1tMAVLVqVRk0aJBcunTJbb12P2oLlQYq7RpMrEsxV65cbs9pa9bZs2fl1KlTbs9r92LJkiWlRYsW921p0/1euXLFbQEAAN7L40HrwIEDpv5KW6G0BWnlypVSvXp1U+vk5+eXoLWoePHiydZBabfhp59+Kps2bTItUFu3bpU2bdqYIKe0i1LruPRYDRo0SHQfrVq1khUrVph9xMXFyc8//yzvvvuuWRcZGWn+13A1e/Zs0yKmS5kyZeTZZ5+VH3/8Mclzmzx5sukCdSz6OQAAwHt5tOtQaauTFqLHxMTI8uXLpWfPniYcpVWXLl2cH9esWdMU21eqVMm0cgUFBZm6q6tXr5quwKRoa9fx48fl+eefN61qWoM1fPhwGTdunGTLls153ro4PPnkk+Zzpk+fblrKEqPHHDlypPOxtmgRtgAA8F4eb9HSVqvKlStL/fr1TYtP7dq1ZcaMGabG6vbt2xIdHZ2g2+9+9VeutL6qSJEicuzYMfN48+bNputRW9By5Mhhjq20dUtDntIaLG0Nu3btmukq1Ba0hg0bOveXFN3GcZzE6DEdIywdCwAA8F4eD1rxaVed1jJp8PL19TXdd64j/E6fPm1quFJK66q0Rku7+tT7779vRhlqK5oua9asMc9//vnnphjfVfbs2eXRRx81YXDp0qXmuEWLFk3yWLo/x3EAAAA82nWoXWlaP6UF7tqdpyMMtYtv3bp1poapb9++pqutUKFCpvVn6NChJuy4jjjUAnltCQsODjYtUOPHjzfzbWmrl3bl6RQO2mqldVdKj+VK68OUdi/qSEf1+++/m25MrbnSubx0tKNOMeHapfnee+9JhQoVzDQUus2cOXNMa5mOZAQAAPB40Lp48aL06NHDFJhrsNJ6Kg1ZOoJPab2T1kRpcNJWLg1LM2fOdNuHtnJpfZejBWr//v1mPi7tctQ5snQerIkTJyY5l1ZSdB+jRo0yxfMa7jQAOroPlXZrvvrqq3Lu3DkzclHPfePGjdK0adN0+doAAIDMz8fSJAGP0GJ4DZgaFKnXAgDA+16/M1yNFgAAgLcgaAEAANiEoAUAAGATghYAAIBNCFoAAAA2IWgBAADYhKAFAABgE4IWAACATQhaAAAANiFoAQAA2ISgBQAAYBOCFgAAgE0IWgAAADYhaAEAANiEoAUAAGATghYAAIBNCFoAAAA2IWgBAADYhKAFAABgE4IWAACATQhaAAAANiFoAQAA2ISgBQAAYBOCFgAAgE0IWgAAADYhaAEAANiEoAUAAGATghYAAIBNCFoAAAA2IWgBAADYhKAFAABgE4IWAACATQhaAAAA3hi0Zs2aJbVq1ZL8+fObJTAwUNauXetcHxsbK4MHD5bChQtL3rx5pVOnTnLhwoVk99mrVy/x8fFxW1q3bp3otrdu3ZI6deqYbfbu3eu2btmyZWZd7ty5pVy5cjJ16tQEn79lyxapV6+e5MyZUypXriwLFixI89cCAAB4H48GrdKlS0tISIhERETI7t27pVmzZtK+fXs5dOiQWT9ixAj5+uuvJSwsTLZu3Srnz5+Xjh073ne/GqwiIyOdy9KlSxPdbvTo0VKqVKkEz2vY69atmwwcOFAOHjwoM2fOlOnTp8uHH37o3ObEiRPStm1badq0qQlpr7zyirz00kuybt26B/qaAAAAL2JlMAULFrTmzJljRUdHW76+vlZYWJhz3eHDhy095fDw8CQ/v2fPnlb79u3ve5w1a9ZY1apVsw4dOmT2uWfPHue6rl27Wi+88ILb9u+//75VunRpKy4uzjwePXq0VaNGDbdtOnfubLVq1SrF1xoTE2OOrf8DAIDMITWv3xmmRuvevXsSGhoq169fN12I2sp1584dad68uXObatWqSdmyZSU8PDzZfWmXXrFixaRq1aoyaNAguXTpktt67X7s16+fLFq0yHQNJtalmCtXLrfnHnnkETl79qycOnXKPNZzcD031apVq2TPTfd75coVtwUAAHgvjwetAwcOmPorrXPSrrqVK1dK9erVJSoqSvz8/KRAgQJu2xcvXtysS67b8NNPP5VNmzbJlClTTJdjmzZtTJBTlmWZOi49VoMGDRLdhwamFStWmH3ExcXJzz//LO+++65Zp12RSs9BzyX+uWl4unnzZqL7nTx5svj7+zuXMmXKpPKrBQAAMpMcnj4BbXXSGqeYmBhZvny59OzZ04SjtOrSpYvz45o1a5pi+0qVKplWrqCgIPnggw/k6tWrMmbMmCT3oa1dx48fl+eff960qmmh/vDhw2XcuHGSLVvas6kec+TIkc7HGsoIWwAAeC+Pt2hpq5WO2Ktfv75p8aldu7bMmDFDSpQoIbdv35bo6OgE3X66LqUqVqwoRYoUkWPHjpnHmzdvNt172oKWI0cOc2ylrVsa8pSOQtTWsGvXrpmuQm29atiwoXN/Ss8h/ghIfayhTLsZE6PHdIywdCwAAMB7eTxoxadddVrLpMHL19fXdN85HD16VE6fPm1quFJK66q0RqtkyZLm8fvvvy/79u0zrWi6rFmzxjz/+eefy6RJk9w+N3v27PLoo4+aMKgjF/W4RYsWNev0Y9dzUxs2bEjVuQEAAO/m0a5D7UrT+iktcNfuvCVLlpguPp0iQWuY+vbta7raChUqZFp/hg4daoJM48aN3QrktSUsODjYtECNHz/ezLelLU7a/adTOGirldZdKT2WK60PU9q9qNNNqN9//910Yz777LNmLq/58+c7p5hw0Bovne5B99+nTx/TUqZzb61evfohffUAAEBG59GgdfHiRenRo4cpMNdgpfVUGrJatGhh1uvcVVoTpcFJW7k0LOmcVq60lUvruxwtUPv375eFCxeaLkedI6tly5YyceJE022XGrqPUaNGmeJ5DXcaAB3dh6pChQomVOlcX9rVqSFtzpw5zkAHAADgo3M8ePoksiothteAqUGRei0AALzv9TvD1WgBAAB4C4IWAACATQhaAAAANiFoAQAA2ISgBQAAYBOCFgAAgE0IWgAAADYhaAEAANiEoAUAAGATghYAAIBNCFoAAAA2IWgBAADYhKAFAABgE4IWAACATQhaAAAANiFoAQAA2ISgBQAAYJMcdu0YnvXrb9fk1B83pHzhPFKhSB5Pnw4AAFkSQcvLRN+4LcOW7pXvf/nN+dwzVYrKB13rin9uX4+eGwAAWQ1dh15GQ9b2Y7+7PaePhy7d47FzAgAgqyJoeVl3obZk3bMst+f1sT5/4vfrHjs3AACyIoKWF9GarOScvETQAgDgYSJoeZFyhXInu14L4wEAwMND0PIiFYvmNYXv2X183J7Xx/o8ow8BAHi4CFpeRkcXPlW5iNtz+lifBwAADxfTO3gZncLh074NTeG71mQxjxYAAJ5D0PJSGq4IWAAAeBZdhwAAADYhaAEAANiEoAUAAGATghYAAIBNCFoAAAA2IWgBAADYhKAFAADgjUFr1qxZUqtWLcmfP79ZAgMDZe3atc71sbGxMnjwYClcuLDkzZtXOnXqJBcuXEh2n7169RIfHx+3pXXr1olue+vWLalTp47ZZu/evW7r1q1bJ40bN5Z8+fJJ0aJFzbFPnjzpXL9ly5YEx9ElKirqgb8uAADAO3g0aJUuXVpCQkIkIiJCdu/eLc2aNZP27dvLoUOHzPoRI0bI119/LWFhYbJ161Y5f/68dOzY8b771WAVGRnpXJYuXZrodqNHj5ZSpUoleP7EiRPmPPR8NIBp6Pr9998TPfbRo0fdjlWsWLE0fS0AAID38ejM8O3atXN7PGnSJNPKtXPnThPC5s6dK0uWLDGBR82fP18CAgLMem1tSkrOnDmlRIkSyR5bW87Wr18vX3zxhVsrmtLgd+/ePfnXv/4l2bL9bxYdNWqUCV937twRX19f57YarAoUKJCi69UWNF0crly5kqLPAwAAmVOGuQWPBhttubp+/brpQtSwo6GmefPmzm2qVasmZcuWlfDw8GSDlnbraQAqWLCgCWkamLT70UG7H/v16yerVq2S3LlzJ/j8+vXrm4ClwU67Iq9duyaLFi0y5+IaspR2PWp4evzxx2XcuHHy1FNPJXlekydPlvHjxyd4nsAFAEDm4Xjdtizr/htbHrZ//34rT548Vvbs2S1/f39r9erV5vnFixdbfn5+CbZ/4oknrNGjRye5v6VLl1pffvml2e/KlSutgIAA8zl379416+Pi4qzWrVtbEydONI9PnDihXyVrz549bvvZsmWLVaxYMXNeuj4wMNC6fPmyc/2RI0es2bNnW7t377a2b99u9e7d28qRI4cVERGR5LnFxsZaMTExzuWnn34y+2ZhYWFhYWGRTLecOXPmvjnH4y1aVatWNXVQMTExsnz5cunZs6epx0qrLl26OD+uWbOmKbavVKmSaeUKCgqSDz74QK5evSpjxoxJch9a0K4tXnouXbt2Ndv/85//lBdeeEE2bNhgit71vHVxePLJJ+X48eMyffp00/qVVJemLg5a4H/mzBlTcK/7fJBkXaZMGbMvHVSQVWTV687K1851Z63rzsrXznWfydDXrS1Zmg0Sq/OOz+NBy8/PTypXruzssvvhhx9kxowZ0rlzZ7l9+7ZER0e71UBpt9/96q9cVaxYUYoUKSLHjh0zQWvz5s2m69E18KgGDRpIt27dZOHChfLRRx+Jv7+/vP322871n332mfnm79q1K8luy4YNG8q2bdtSfG7aPam1aOnFMXozq8mq152Vr53rznqy6rVz3RmX5oRMOY9WXFycqXnS0KX1UJs2bXIb4Xf69GlTw5VSZ8+elUuXLknJkiXN4/fff1/27dtnWtF0WbNmjXn+888/N8X46saNG84ieIfs2bM7zy8puj/HcQAAADzaoqXdd23atDEF7toEpyMMtYtPp1PQpNi3b18ZOXKkFCpUyCTboUOHmpDl2qKkBfJaZB4cHGyK1rXYXOe80lYv7crTKRy0xaxVq1Zmez2WK+2+U9q96Ghdatu2rekCnDBhgrPrcOzYsVKuXDmpW7eu2ea9996TChUqSI0aNcx8X3PmzDGtZTqSEQAAwONB6+LFi9KjRw8z/5QGK62n0pDVokULs17DjrYsaXDSVi4NSzNnznTbh7ZyaX2Xo9Vp//79pvtPuxy177Rly5YyceLEBF2FydGRihr6tOtQFx2ZqAHv22+/lUceecRso92ar776qpw7d86s13PfuHGjNG3aVB42vbY333wzVdfoDbLqdWfla+e6s9Z1Z+Vr57pzirfw0Yp4T58EAACAN8pwNVoAAADegqAFAABgE4IWAACATQhaAAAANiFoPWR//PGHmRhVp6vQiVh1CgudliI5On3E4MGDzf0adToKHYWpE7e60vnFdFoKHQGp93l87bXX5O7du27b6NQZ9erVM6M5dMqLBQsWJHnMkJAQM1v9K6+8kupzyYzXrjcz15GjjknydJRp/JuNP/vss+Zr4roMHDjQ6687rd/zjHzdOiXME088Ye7KoPvo0KGDGcGcHt9vb7h2b/yef//999KuXTszGl2/l3qv2/j03rbxv+etW7f2+uvWMXF69xOdB1JH1ut9fX/55Zf7XndGv3alE5CXL19ecuXKJY0aNZL//ve/kl6/5yl235v0IF3pfRZr165t7dy50/rPf/5jVa5c2eratWuynzNw4ECrTJky1qZNm8y9FRs3bmw9+eSTzvV6H8fHH3/cat68ubln45o1a6wiRYpYY8aMcW7z66+/Wrlz57ZGjhxp7rH4wQcfmPs4fvvttwmO99///tcqX768VatWLWv48OGpOpfMeu1fffWVuc/mzz//bB09etQaO3as5evrax08eNC5TZMmTax+/fpZkZGRzkXvWent153W73lGvu5WrVpZ8+fPN9e5d+9e67nnnrPKli1rXbt27YG/395w7d74PdfPe+ONN6wVK1aYe9TpvXDj69mzp7kG1+/5H3/84fXXHRISYu41vGrVKmvfvn3Wn//8Z6tChQrWzZs3M/W1h4aGmnsmz5s3zzp06JD5fS5QoIB14cKFdPk9TymC1kPkuIn0Dz/84Hxu7dq1lo+Pj3Xu3LlEPyc6Otq88IWFhTmfO3z4sNlPeHi4eaw/hNmyZbOioqKc28yaNcvKnz+/devWLfNYb8Rdo0YNt3137tzZ/NF1dfXqVatKlSrWhg0bzA+ga9BKyblk5muPr2DBgtacOXOcj+N/PVIis193Wr/nme26L168aI6zdevWB/p+e8O1Z4XveXJBq3379kleozded1xcnFWiRAlr6tSpbueXM2dOa+nSpZn62hs2bGgNHjzY+fjevXtWqVKlrMmTJz/w73lq0HX4EOk9FrVpVe+r6KBNtDopq95DMTERERFy584ds53rbPg6w73uz7FfvYF28eLFndvo5K56c85Dhw45t3Hdh2Mbxz4ctDlXm2vjb5vSc8nM1+5w7949CQ0NlevXrye43dPixYvNvTMff/xxc2cDvV2TN193Wr/nmem6lWPSY70LxYN8v73h2rPK9zwp2h2lXVVVq1aVQYMGmVu4JSezX/eJEyckKirKbT86gbh2s2Xmv+u3b982x3LdRs9LH8e/rrT8nqeGx28qnZXoD7P+ArvKkSOH+QOn65L6HL3xtuuNtZX+ADo+R/93/YF0rHesS24b/cG9efOm6ZfXF9kff/zR3Ng7reeSWa9dHThwwAQMrR/QuoGVK1dK9erVnZ/z4osvmtswaa2D3oHgb3/7m6ltWbFihdded1q/55nhuh30/qVai/jUU0+ZP7QP8v32hmvPCt/zpGg9VseOHc3t1fQWbnrrNb1NnL4wO+53623X7dhXYvvJzH/XL1++bN48JrbNkSNHHvj3PDUIWung9ddflylTpiS7zeHDhyUjO3PmjAwfPlw2bNhgigZTc+0azHTR2xVlxmt30HewemNwfYe/fPly6dmzp2zdutUZOvr37+/2Dkhvv6ShRIsnvfm6k/pZT+p7nlmu27UV9+DBg7Jt2za3512/3/ru+ptvvjFvRpL6fnvTtXv79zwpXbp0cfsdd1yXhofEeMt1J/Y9DwsLMx8vW7bMq6+9f7zfcx0QEBQUZIK23gM5PRC00oHe81BHqySnYsWK5kbXen9HVzqKQkdt6LrE6PPaBKr3bnR9B6AjNByfo//HH0nhGMHhuk38UR36WEeK6LsebWLVc9MRHA76bkBHrHz44YfmXpOJnYte+6effmruWZnU1yCjX7uDvsvSkSuqfv365kVlxowZ8vHHHyc4N73uv/zlL2a7f//73/L000975XXHPxfHz7reDzSp73lmuW41ZMgQE6D059xxU/mk6Gg9DVpJfb+95dq9/XueUo7r1tZebfXr3LmzV163Y1/6eRoyHNe+adMm06X3xhtvJPp5Gf3as2fPbpbEtknq3JR2mapjx46lW9CiGP4hchQO6igLh3Xr1qWocHD58uXO544cOZJo4aDrSIqPP/7YFA7GxsY6Cwd1FIcrHRniKBy8cuWKdeDAAbelQYMGVvfu3c3HKT2XzHjtSWnatKkpjk3Ktm3bzLnoKB1vve60fs8z+nVrAbAWyWphrI64TImUfL+94dq99XuekmL4+M6cOWPO/8svv/Ta63YUw7/zzjvO53TUXWqK4TPqtTds2NAaMmSIWzH8o48+6lYMn9bf89QgaD1kOhS2bt261q5du8w3VEf4uQ6FPXv2rFW1alWz3nUorA6/3rx5s/mBDgwMNEv8obAtW7Y0w7V1eGvRokUTHQr72muvmREeH330UZLTOyQ3GuN+55JZr/311183o65OnDhh7d+/3zzWPxbr1683648dO2ZNmDDBnINuo394K1asaD3zzDNefd0pOZfMeN2DBg0yw9m3bNniNqz7xo0bD/z9zuzXnpJzyYzXrSOqdaoAXfSFdNq0aebjU6dOOdePGjXKvNjr93zjxo1WvXr1zDU4Xty98bod0zvotAf6c65/B3TkZWqmd8io1x4aGmoC44IFC0wo7N+/v7lOx2jGB/09TymC1kN26dIl80OYN29ek8579+5tfhEc9Jutvwzfffed8zn9YX/55ZfNsHv9wQoODjZ/GF2dPHnSatOmjfXII4+Y+UZeffVV686dO27b6D7r1Klj5hXRHyadSyc5iQWtlJxLZrz2Pn36WOXKlTPr9Rc6KCjILWycPn3a/PIVKlTI/OLqXDH6C56S+VYy83Wn9Fwy23XrcRNbHNs9yPc7s197Ss8ls123rk/suh2ttxo09YVdfw+0xUV/L3R+JdcpBrzxuh2tWv/4xz+s4sWLm593/Tug8+qlREa+dqXza2mo0220hUvn+3J40N/zlPLRf9KnExIAAACumEcLAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAACl28uRJ6du3r1SoUMHcV1DvCfjmm2+aexgmZ8CAAWZb/ZyiRYtK+/bt5ciRI27b6D0Wn3zyScmXL5+5J+Hf/vY3c//ExOj9CHU713smqjt37siECRPMsXLlyiW1a9eWb7/9NtXXOWzYMHP/15w5c0qdOnUkrQhaAPCAypcvL++9954t+3722WfNTY2BjELDUVxcnLnx/KFDh2T69Okye/ZsGTt2bLKfV79+fZk/f74cPnxY1q1bp3emkZYtW8q9e/fM+n379slzzz0nrVu3lj179sjnn38uX331lbz++usJ9qVhqmvXrvKnP/0pwbq///3v5tw++OAD+emnn2TgwIESHBxs9plaffr0SfSG4qmSrvPMA0AGp7ce0Xu5pYXe4kPvExjfxYsXrevXr6f6psUpkditsICM5u233zb3R0yNffv2md8Vveeg0nsZNmjQwG2br776ysqVK5d15coVt+f1ptLdu3dP9HeyZMmS1ocffuj2XMeOHa1u3bq53WD6rbfessqXL2/2X6tWLSssLCzR83zzzTet2rVrW2lFixYAPCDtBsmdO7enTwPwmJiYGClUqFCKt79+/bpp3dLuxzJlypjnbt26Zbr6XGk3Y2xsrERERDif27x5s4SFhclHH32U6L6T2s+2bducjydPniyffvqpaYnTVrkRI0ZI9+7dZevWrZLeCFoA8H+mTZsmNWvWlDx58pg//i+//LJcu3bNrNuyZYv07t3bvKD4+PiYZdy4cQm6DvVjpV0Vuo3jca9evaRDhw5ux9MuQe0adH3x6dGjh+TNm1dKliwp7777bqIvIqNGjZJHH33UnGejRo3MuQGeorVS2k2nNVj3M3PmTPPzrcvatWtlw4YN4ufnZ9a1atVKduzYIUuXLjXdiefOnTO1VioyMtL8f+nSJfO7tGDBAsmfP3+ix9D96O/yL7/8Yro49RgrVqxw7kN/h9566y2ZN2+e2bZixYpmnxq0tMsxvRG0AOD/ZMuWTd5//33zDnfhwoXmnfPo0aPNOi3Q1TClf9z1D7YuGnji++GHH8z/+m5dt3E8TonXXnvNvKP+8ssvZf369SZA/fjjj27bDBkyRMLDwyU0NFT2798v//M//2NqWvRFBXgQWgvleBOR1BK/eF3DkP786c9hv3797nuMbt26mVop/Tl/7LHH5C9/+YtpsVJarzV16lRTU6UF6Lpea7Ycv5tKj/Hiiy/KM888k+QxZsyYIVWqVJFq1aqZEKe/M/omybEPDYY3btyQFi1aOEOfLtrCdfz4cUlvOdJ9jwCQSbkWnWtL1L/+9S/zR1/fhesfbH9/f/Nio6OhkutGVDoSKrnt4tOWs7lz58pnn30mQUFB5jkNe6VLl3Zuc/r0aRPg9P9SpUqZ5zTs6YgqfV7fpQNp9eqrr5qWneRo64/D+fPnpWnTpuZNyCeffJKiY/j7+5tFg1Djxo2lYMGCsnLlSlPYrkaOHGm68fRNiq7TEY5jxoxxHlff/GiB/DvvvGMea0mktlrlyJHDnIMWr+vv4KpVq0yA0xYw/V3REOnYh6OVevXq1aZl2JUGvPRG0AKA/7Nx40ZTu6Hv2q9cuWKGlesfa333a3cNlr6T1uHx2hXooDUvVatWdT4+cOCA6VLRd/qutCukcOHCtp4fvJ8GFMcbhfvRliwNWY6RhI7WotSwLMss+vPrSt/MON5IaDeiduPXq1fPPNbWXMcoRaWtv1OmTDFdjvFDk9Zp6XM6QvGLL74wrWeqevXqJlDpG5YmTZqI3QhaAPB/cwM9//zzMmjQIJk0aZIJOVo8q/MFaQB60KClL0T/OyDx/9MXgNTQd+LZs2c3hcH6vyvt+gAeBg1ZWltYrlw507L022+/Odc5WnF1G22Z/fTTT6Vhw4by66+/mukatHtQw9zZs2clJCTEFKk7ugeVdh1qV6T+vmhdlW6zbNky5897QECA27ns3r3bbPv44487n9u1a5c5vs59pf9rLaW2ejnKAHTuLW0J1pYzff7pp582tZfbt283pQE9e/Z0djHq71xUVJTcvHlT9u7d6wxqjrqylCBoAYCICS/6R1cL0B3vzvUPvCv94+r6bjopvr6+CbbTF5eDBw+6Pad/uHVbpZMr6sf6IlG2bFnz3OXLl+Xnn392vuuuW7eu2e/FixcTnT8IeBi0uFxDiC6uXdvK8WZC30QcPXrUtAY7Wpf+85//mDpH/bkuXry4qbPSlqhixYo5P18L5PWNjrZy6USj2mLVpk2bVJ2ftkLrXFoa7vQNiAa5RYsWuU1sOnHiRPM7qS3Yup2u01Yz17nAXnrpJbdRiPr7p06cOOEc5JISBC0AWY6+e3W8O3UoUqSIeXHQ0VPt2rUz72516Lcr/eOq73B19mp9EdBWrsRaunQ73eapp54yXRRaa9KsWTPzbl3f4QcGBppaLA1ejj/e+oKgrWdaEK/dgPri88Ybb7h1yWiXoRYT68hEDYT6udqaoMeqVauWtG3b1ravGeCgdVz3q+XS3wHXFtxSpUrJmjVr7rtvrcF60HPRNyY6UWlytHty+PDhZklKeo3mZdQhgCxH/4BqSHFd9B2vDgnXeg/thli8eLF5t+tKi361OF5nitZ3w2+//Xai+9cQpO/6tbbEEaR0GPk//vEP033xxBNPyNWrV01gcqVBTFuqNOg1b97cdGloDYwrrYfRz9PCZa3f0ikjdGSjoxUMQMbio7OWevokAAAAvBEtWgAAADYhaAEAANiEoAUAAGATghYAAIBNCFoAAAA2IWgBAADYhKAFAABgE4IWAACATQhaAAAANiFoAQAA2ISgBQAAIPb4fzZHeQAuVFSbAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results.plot(kind='scatter',x='Latitude',y='Longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folium map\n",
    "\n",
    "m = folium.Map(location=[results.Latitude.mean(), results.Longitude.mean()], zoom_start=13)\n",
    "\n",
    "folium.TileLayer('Esri.WorldImagery', name='Esri Satellite', control=True).add_to(m)\n",
    "\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "for idx, row in results.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row.Latitude, row.Longitude],\n",
    "        popup=row.name\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "m.save('.tmp/cluster_map.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[['Latitude', 'Longitude', 'Elevation']].to_csv(save_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing with Openvino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up inference on intel, make changes inn ultralytics/nn/autobackend.py:\n",
    "```\n",
    "- device_name = \"AUTO:NPU,GPU,CPU\" # CPU, GPU, NPU, AUTO,\"AUTO:GPU,NPU\"\n",
    "- inference_mode = \"LATENCY\" # OpenVINO inference modes are 'LATENCY', 'THROUGHPUT' (not recommended), or 'CUMULATIVE_THROUGHPUT'\n",
    "- LOGGER.info(f\"Using OpenVINO {inference_mode} mode for inference...\")\n",
    "- ov_compiled_model = core.compile_model(\n",
    "                ov_model,\n",
    "                device_name=device_name,  # AUTO selects best available device, do not modify\n",
    "                config={\"PERFORMANCE_HINT\": inference_mode,\n",
    "                        \"CACHE_DIR\": os.environ[\"OPENVINO_CACHE_MODEL\"]}, # make sure to set environment variable\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = r\"D:\\savmap_dataset_v2\\images_splits\\00a033fefe644429a1e0fcffe88f8b39_1.JPG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define detector\n",
    "# to speed up inference on intel, make\n",
    "model = Detector(path_to_weights=r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best_openvino_model\",\n",
    "                confidence_threshold=0.1,\n",
    "                overlap_ratio=0.1,\n",
    "                tilesize=1280,\n",
    "                device='CPU',\n",
    "                use_sliding_window=False,\n",
    "                is_yolo_obb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(IMAGE_PATH)\n",
    "\n",
    "while True:\n",
    "    start_time = time.perf_counter()\n",
    "    print(model.predict(image,return_coco=True,nms_iou=0.5))\n",
    "    end_time = time.perf_counter()\n",
    "    print(f\"Device took {end_time-start_time:.2f} seconds.\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with openvino\n",
    "import openvino as ov\n",
    "import openvino.properties.hint as hints\n",
    "from ultralytics.utils import DEFAULT_CFG\n",
    "from ultralytics.cfg import get_cfg\n",
    "from ultralytics.data.converter import coco80_to_coco91_class\n",
    "\n",
    "# load validator\n",
    "args = get_cfg(cfg=DEFAULT_CFG)\n",
    "det_model = YOLO(r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best.pt\")\n",
    "det_validator = det_model.task_map[det_model.task][\"validator\"](args=args)\n",
    "det_validator.is_coco = True\n",
    "det_validator.class_map = coco80_to_coco91_class()\n",
    "det_validator.names = det_model.model.names\n",
    "det_validator.metrics.names = det_validator.names\n",
    "det_validator.nc = det_model.model.model[-1].nc\n",
    "det_validator.stride = 32\n",
    "args = get_cfg(cfg=DEFAULT_CFG)\n",
    "det_model = YOLO(r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best.pt\")\n",
    "\n",
    "core = ov.Core()\n",
    "det_model_path = r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best_openvino_model\\best.xml\"\n",
    "det_ov_model = core.read_model(det_model_path)\n",
    "\n",
    "device = \"AUTO:NPU,GPU\" # CPU, NPU, GPU \"AUTO:NPU,GPU,CPU\" \n",
    "\n",
    "print(\"Available core devices: \",core.available_devices)\n",
    "\n",
    "# reshaping for batch prediction\n",
    "input_layer = det_ov_model.input(0)\n",
    "output_layer = det_ov_model.output(0)\n",
    "new_shape = ov.PartialShape([1, 3, 1280, 1280])\n",
    "det_ov_model.reshape({input_layer.any_name: new_shape})\n",
    "\n",
    "ov_config = {hints.performance_mode: hints.PerformanceMode.THROUGHPUT,\n",
    "             \"CACHE_DIR\": '../models/model_cache'}\n",
    "\n",
    "if (\"GPU\" in core.available_devices) and device==\"GPU\":\n",
    "    ov_config[\"GPU_DISABLE_WINOGRAD_CONVOLUTION\"] = \"YES\"\n",
    "det_compiled_model = core.compile_model(det_ov_model, device, ov_config)\n",
    "\n",
    "def infer(image):\n",
    "    image = det_validator.preprocess({\"img\":image,\"batch_idx\":torch.Tensor([0]),\n",
    "                                      \"cls\":torch.Tensor([0]),\n",
    "                                      \"bboxes\":torch.Tensor([0.,0.,0.,0.])})[\"img\"]\n",
    "    results = det_compiled_model(image)\n",
    "    preds = torch.from_numpy(results[det_compiled_model.output(0)])\n",
    "    return det_validator.postprocess(preds) #torch.from_numpy(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(IMAGE_PATH)\n",
    "# image = F.PILToTensor()(image)[None,:,:1280,:1280]\n",
    "# infer(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with pt\n",
    "# model = YOLO(r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best.pt\",task='obb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling input images\n",
    "# model(image/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with openvino\n",
    "# model_vino = YOLO(r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best_openvino_model\",task='obb')\n",
    "# model_vino(image/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sahi_model_obb = Detector(path_to_weights=r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best_openvino_model\",\n",
    "#                     confidence_threshold=0.6,\n",
    "#                     overlap_ratio=0.1,\n",
    "#                     tilesize=640,\n",
    "#                     is_yolo_obb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = r\"D:\\savmap_dataset_v2\\images\\0d1ba3c424ad4414ac37dbd0c93460ea.JPG\"\n",
    "# image = Image.open(image_path)\n",
    "# print(image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = sahi_model_obb.predict(image,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "# result.export_visuals('../.tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sahi inference calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "overlap_ratios = [0.1,0.2,0.3]\n",
    "tilesizes = [640,2*640,3*640]\n",
    "imgsz = [640,2*640,3*640]\n",
    "\n",
    "for ratio, tilesize, image_size in product(overlap_ratios,tilesizes,imgsz):\n",
    "    print(ratio,tilesize,image_size)\n",
    "    # Define detector\n",
    "    # to speed up inference on intel, make\n",
    "    model = Detector(path_to_weights=r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best_openvino_model\",\n",
    "                    confidence_threshold=0.1,\n",
    "                    overlap_ratio=0.1,\n",
    "                    tilesize=2000,\n",
    "                    imgsz=1280,\n",
    "                    device='CPU',\n",
    "                    use_sliding_window=True,\n",
    "                    is_yolo_obb=True)\n",
    "    \n",
    "    #TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO data_config.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from datalabeling.arguments import Arguments\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yaml\n",
    "with open(r\"D:\\PhD\\Data per camp\\DetectionDataset\\hard_samples\\train_ratio_20-seed_41.yaml\",'r') as file:\n",
    "    yolo_config = yaml.load(file,Loader=yaml.FullLoader)\n",
    "yolo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(os.path.join(yolo_config[\"path\"],yolo_config['train']),header=None,names=['paths'])['paths'].to_list()[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load label mapping\n",
    "args = Arguments()\n",
    "with open(r\"D:\\PhD\\Data per camp\\IdentificationDataset\\label_mapping.json\",'r') as file:\n",
    "    label_map = json.load(file)\n",
    "names = [p['name'] for p in label_map if p['name'] not in args.discard_labels ]\n",
    "label_map = dict(zip(range(len(names)),names))\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_studio_sdk import Client\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "def get_ls_parsed_config(ls_json_path:str):\n",
    "\n",
    "    ls_client = None\n",
    "    if ls_client is None:\n",
    "        # Connect to the Label Studio API and check the connection\n",
    "        LABEL_STUDIO_URL = os.getenv('LABEL_STUDIO_URL')\n",
    "        API_KEY = os.getenv(\"LABEL_STUDIO_API_KEY\")\n",
    "        labelstudio_client = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
    "\n",
    "    with open(ls_json_path,'r') as f:\n",
    "        ls_annotation = json.load(fp=f)\n",
    "    ids = set([annot['project'] for annot in ls_annotation])\n",
    "    assert len(ids)==1, \"annotations come from different project. Not allowed!\"\n",
    "    project_id = ids.pop()\n",
    "    project = labelstudio_client.get_project(id=project_id)\n",
    "\n",
    "    return project.parsed_label_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_config.update({'names':label_map,'nc':len(label_map)})\n",
    "yolo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\PhD\\Data per camp\\IdentificationDataset\\data_config.yaml\",'w') as file:\n",
    "    yaml.dump(yolo_config,file,default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize distribution per annotation project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datalabeling.dataset import load_coco_annotations\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "# from itertools import chain\n",
    "import traceback\n",
    "import os\n",
    "from pathlib import  Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path=r\"..\\.env\"\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "# Connect to the Label Studio API and check the connection\n",
    "LABEL_STUDIO_URL = os.getenv('LABEL_STUDIO_URL')\n",
    "API_KEY = os.getenv(\"LABEL_STUDIO_API_KEY\")\n",
    "labelstudio_client = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_project_stats(project_id:int,annotator_id = 0):\n",
    "    \n",
    "    project = labelstudio_client.get_project(id=project_id)\n",
    "    num_images = dict()\n",
    "    # Iterating \n",
    "    tasks = project.get_tasks()\n",
    "     # because there is\n",
    "    labels = []\n",
    "\n",
    "    for task in tasks:\n",
    "        try:\n",
    "            result = task['annotations'][annotator_id]['result']\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "        img_labels = []\n",
    "        for annot in result:\n",
    "            img_labels = annot['value']['rectanglelabels'] + img_labels\n",
    "        labels = labels + img_labels\n",
    "        # update stats holder\n",
    "        for label in set(img_labels):\n",
    "            try:\n",
    "                num_images[label] += 1\n",
    "            except:\n",
    "                num_images[label] = 1\n",
    "\n",
    "    stats = {f\"{k}\":labels.count(k) for k in set(labels)}\n",
    "    print(\"Number of instances for each label is:\\n\",stats,end=\"\\n\\n\")\n",
    "    print(\"Number of images for each label is:\\n\",num_images)\n",
    "\n",
    "    return stats, num_images\n",
    "\n",
    "# get stats\n",
    "for project_id in [93,]:\n",
    "    get_project_stats(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_dir = r\"D:\\savmap_dataset_v2\\annotated_py_paul\\coco-format\"\n",
    "dest_dir = Path(ls_dir).with_name(\"coco-format\")\n",
    "save_excel_path = Path(ls_dir).with_name(\"stats.xlsx\")\n",
    "\n",
    "# Uncomment to run if needed\n",
    "# convert_json_annotations_to_coco(input_dir=ls_dir,\n",
    "#                                  dest_dir_coco=str(dest_dir),\n",
    "#                                  ls_client=labelstudio_client,\n",
    "#                                  parse_ls_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_annotations_dict = load_coco_annotations(dest_dir)\n",
    "coco_annotations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_count(coco_annotation:dict):\n",
    "\n",
    "    result = Counter([annot['category_id'] for annot in coco_annotation['annotations']])\n",
    "\n",
    "    label_map = {cat['id']:cat['name'] for cat in coco_annotation['categories']}\n",
    "\n",
    "    result = {label_map[k]:v for k,v in result.items()}\n",
    "\n",
    "    return result\n",
    "\n",
    "label_stats = dict()\n",
    "\n",
    "for img_dir,coco_path in coco_annotations_dict.items():\n",
    "\n",
    "    with open(coco_path,'r') as f:\n",
    "        coco_annotation = json.load(fp=f)\n",
    "    \n",
    "    label_stats[img_dir] = get_labels_count(coco_annotation)\n",
    "\n",
    "label_stats = pd.DataFrame.from_dict(label_stats,orient='index').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_stats = label_stats.reset_index().rename(columns={'index':'file_name'})\n",
    "label_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_stats['file_name_short'] = label_stats['file_name'].apply(lambda x: Path(x).parent.name + '/' + Path(x).name)\n",
    "# label_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to save\n",
    "label_stats.to_excel(save_excel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize splits' distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*20, \"Detection\",'-'*20)\n",
    "\n",
    "path = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\configs\\yolo_configs\\dataset_identification-detection.yaml\"\n",
    "\n",
    "for split in  ('train','val','test'):   \n",
    "\n",
    "    # load yaml\n",
    "    with open(path,'r') as file:\n",
    "        yolo_config = yaml.load(file,Loader=yaml.FullLoader)\n",
    "\n",
    "    label_map = yolo_config['names']\n",
    "\n",
    "    path_dataset = os.path.join(yolo_config['path'],\n",
    "                                yolo_config[split][0]\n",
    "                                )\n",
    "\n",
    "    total_number_images = len(list(Path(path_dataset).glob('*')))\n",
    "\n",
    "    path_dataset = path_dataset.replace('images','labels')\n",
    "\n",
    "    total_number_of_positive_images = len(list(Path(path_dataset).glob('*')))\n",
    "\n",
    "    print(\"\\n\\nSplit:\", split)\n",
    "    print(\"Number of empty images: \", total_number_images-total_number_of_positive_images)\n",
    "    print(\"Number of non-empty images: \", total_number_of_positive_images)\n",
    "\n",
    "    labels = list()\n",
    "\n",
    "    for txtfile in Path(path_dataset).glob(\"*.txt\"):\n",
    "\n",
    "        df = pd.read_csv(txtfile,sep=\" \",header=None )\n",
    "        df['class'] = df.iloc[:,0].astype(int)    \n",
    "        df['image'] = txtfile.stem\n",
    "        labels.append(df)\n",
    "\n",
    "    df = pd.concat(labels,axis=0)\n",
    "    df['class'] = df['class'].map(label_map)\n",
    "\n",
    "    images_per_class = dict()\n",
    "    for cls in df['class'].unique():\n",
    "        num_imge = df.loc[df['class'] == cls,'image'].unique().shape[0]\n",
    "        images_per_class[cls] = num_imge\n",
    "\n",
    "    \n",
    "    print('images per class: ',images_per_class)\n",
    "    print('instances per class: ', df['class'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*20, \"Identification\",'-'*20)\n",
    "path = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\configs\\yolo_configs\\dataset_identification.yaml\"\n",
    "\n",
    "for split in  ('train','val','test'):   \n",
    "\n",
    "    # load yaml\n",
    "    with open(path,'r') as file:\n",
    "        yolo_config = yaml.load(file,Loader=yaml.FullLoader)\n",
    "\n",
    "    label_map = yolo_config['names']\n",
    "\n",
    "    path_dataset = os.path.join(yolo_config['path'],\n",
    "                                yolo_config[split][0]\n",
    "                                )\n",
    "\n",
    "    total_number_images = len(list(Path(path_dataset).glob('*')))\n",
    "\n",
    "    path_dataset = path_dataset.replace('images','labels')\n",
    "\n",
    "    total_number_of_positive_images = len(list(Path(path_dataset).glob('*')))\n",
    "\n",
    "    print(\"\\n\\nSplit:\", split)\n",
    "    print(\"Number of empty images: \", total_number_images-total_number_of_positive_images)\n",
    "    print(\"Number of non-empty images: \", total_number_of_positive_images)\n",
    "\n",
    "    labels = list()\n",
    "\n",
    "    for txtfile in Path(path_dataset).glob(\"*.txt\"):\n",
    "\n",
    "        df = pd.read_csv(txtfile,sep=\" \",header=None )\n",
    "        df['class'] = df.iloc[:,0].astype(int)    \n",
    "        df['image'] = txtfile.stem\n",
    "        labels.append(df)\n",
    "\n",
    "    df = pd.concat(labels,axis=0)\n",
    "    df['class'] = df['class'].map(label_map)\n",
    "\n",
    "    images_per_class = dict()\n",
    "    for cls in df['class'].unique():\n",
    "        num_imge = df.loc[df['class'] == cls,'image'].unique().shape[0]\n",
    "        images_per_class[cls] = num_imge\n",
    "\n",
    "    \n",
    "    print('images per class: ',images_per_class)\n",
    "    print('instances per class: ', df['class'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts().plot(kind='bar',figsize=(10,5),logy=True,title=f\"{split} label distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing metrics on Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO or ultralytics models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "# from pathlib import Path\n",
    "from datalabeling.train import remove_label_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [IMPORTANT] Run this cell to convert the datasets to yolo format\n",
    "!python  ../tools/build_dataset.py --obb-to-yolo --data-config-yaml \"..\\data\\dataset_identification.yaml\" --skip\n",
    "!python  ../tools/build_dataset.py --obb-to-yolo --data-config-yaml \"..\\data\\dataset_identification-detection.yaml\" --skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting results for yolov12s : Detection and Identification\n",
    "paths = [\"...\", # Identification model weights\n",
    "         \"../runs/mlflow/140168774036374062/a59eda79d9444ff4befc561ac21da6b4/artifacts/weights/best.pt\" # Detection model weights\n",
    "        ]\n",
    "\n",
    "dataconfigs = [\n",
    "                # r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification.yaml\",\n",
    "               r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification-detection.yaml\"\n",
    "              ]\n",
    "\n",
    "imgsz = 800\n",
    "iou_threshold=0.45\n",
    "conf_threshold=0.235\n",
    "splits = [\n",
    "            # \"val\", \n",
    "          \"test\",\n",
    "          ]\n",
    "\n",
    "# remove label.cache files\n",
    "for dataconfig in dataconfigs:\n",
    "    remove_label_cache(data_config_yaml=dataconfig)\n",
    "\n",
    "for split in splits:\n",
    "    for path,dataconfig in zip(paths,dataconfigs):\n",
    "        print(\"\\n\",'-'*20,split,'-'*20)\n",
    "        model = YOLO(path)\n",
    "        model.info()\n",
    "        \n",
    "        # Customize validation settings\n",
    "        validation_results = model.val(data=dataconfig,\n",
    "                                        imgsz=imgsz,\n",
    "                                        batch=64,\n",
    "                                        split=split,\n",
    "                                        conf=conf_threshold,\n",
    "                                        iou=iou_threshold,\n",
    "                                        device=\"cuda\"\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting results for yolov5s : Detection and Identification\n",
    "paths = [\"../runs/mlflow/140168774036374062/87718ce84ce04dacac6ab8c92328eae7/artifacts/weights/best.pt\", # Identification model weights\n",
    "         \"../runs/mlflow/140168774036374062/e5e3bf93d34f48f1bb7d0a648530bb45/artifacts/weights/best.pt\" # Detection model weights\n",
    "        ]\n",
    "\n",
    "dataconfigs = [r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification.yaml\",\n",
    "               r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification-detection.yaml\"\n",
    "              ]\n",
    "\n",
    "imgsz = 800\n",
    "iou_threshold=0.45\n",
    "conf_threshold=0.235\n",
    "splits = [\n",
    "            # \"val\", \n",
    "          \"test\",\n",
    "          ]\n",
    "\n",
    "# remove label.cache files\n",
    "for dataconfig in dataconfigs:\n",
    "    remove_label_cache(data_config_yaml=dataconfig)\n",
    "\n",
    "for split in splits:\n",
    "    for path,dataconfig in zip(paths,dataconfigs):\n",
    "        print(\"\\n\",'-'*20,split,'-'*20)\n",
    "        model = YOLO(path)\n",
    "        model.info()\n",
    "        \n",
    "        # Customize validation settings\n",
    "        validation_results = model.val(data=dataconfig,\n",
    "                                        imgsz=imgsz,\n",
    "                                        batch=64,\n",
    "                                        split=split,\n",
    "                                        conf=conf_threshold,\n",
    "                                        iou=iou_threshold,\n",
    "                                        device=\"cuda\"\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [IMPORTANT] Run this cell to convert the datasets to obb format\n",
    "!python  ../tools/build_dataset.py --yolo-to-obb --data-config-yaml \"..\\data\\dataset_identification.yaml\" --skip\n",
    "!python  ../tools/build_dataset.py --yolo-to-obb --data-config-yaml \"..\\data\\dataset_identification-detection.yaml\" --skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load yolov11s-obb\n",
    "paths = [\"../runs/mlflow/140168774036374062/34c709364c0e46dcb72c526de34a7fa4/artifacts/weights/best.pt\", # Identification\n",
    "         \"../runs/mlflow/140168774036374062/f5b7124be14c4c89b8edd26bcf7a9a76/artifacts/weights/best.pt\", # Detection\n",
    "        ]\n",
    "\n",
    "\n",
    "dataconfigs = [r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification.yaml\",\n",
    "               r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification-detection.yaml\"\n",
    "              ]\n",
    "\n",
    "imgsz = 800\n",
    "iou_threshold=0.45\n",
    "conf_threshold=0.235\n",
    "splits = [\n",
    "        #   \"val\", \n",
    "          \"test\",\n",
    "          ]\n",
    "\n",
    "# remove label.cache files\n",
    "for dataconfig in dataconfigs:\n",
    "    remove_label_cache(data_config_yaml=dataconfig)\n",
    "\n",
    "for split in splits:\n",
    "    for path,dataconfig in zip(paths,dataconfigs):\n",
    "        print(\"\\n\",'-'*20,split,'-'*20)\n",
    "        model = YOLO(path)\n",
    "        model.info()\n",
    "        \n",
    "        # Customize validation settings\n",
    "        validation_results = model.val(data=dataconfig,\n",
    "                                        imgsz=imgsz,\n",
    "                                        batch=64,\n",
    "                                        split=split,\n",
    "                                        conf=conf_threshold,\n",
    "                                        iou=iou_threshold,\n",
    "                                        device=\"cuda\"\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load yolov8s-obb\n",
    "paths = [\n",
    "         \"../runs/mlflow/140168774036374062/b883bd2b31f94f29807ea3b94e8ff8fc/artifacts/weights/best.pt\", # Identification\n",
    "         \"../runs/mlflow/140168774036374062/8a76c60253fc48788b5324096d035420/artifacts/weights/best.pt\"  # Detection\n",
    "        ]\n",
    "\n",
    "\n",
    "dataconfigs = [\n",
    "                r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification.yaml\",\n",
    "               r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification-detection.yaml\"\n",
    "              ]\n",
    "\n",
    "imgsz = 800\n",
    "iou_threshold=0.45\n",
    "conf_threshold=0.235\n",
    "splits = [\n",
    "        #   \"val\", \n",
    "          \"test\",\n",
    "          ]\n",
    "\n",
    "# remove label.cache files\n",
    "for dataconfig in dataconfigs:\n",
    "    remove_label_cache(data_config_yaml=dataconfig)\n",
    "\n",
    "for split in splits:\n",
    "    for path,dataconfig in zip(paths,dataconfigs):\n",
    "        print(\"\\n\",'-'*20,split,'-'*20)\n",
    "        model = YOLO(path)\n",
    "        model.info()\n",
    "        \n",
    "        # Customize validation settings\n",
    "        validation_results = model.val(data=dataconfig,\n",
    "                                        imgsz=imgsz,\n",
    "                                        batch=64,\n",
    "                                        split=split,\n",
    "                                        conf=conf_threshold,\n",
    "                                        iou=iou_threshold,\n",
    "                                        device=\"cuda\"\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "from datalabeling.annotator import Annotator\n",
    "\n",
    "for alias in [\"version9\", \"version6\"]:\n",
    "    print(\"-\"*10,alias,end=\"\\n\\n\")\n",
    "    name = \"obb-detector\" # detector, \"obb-detector\"\n",
    "    handler = Annotator(mlflow_model_alias=alias,\n",
    "                            mlflow_model_name=name,\n",
    "                            confidence_threshold=0.25,\n",
    "                            is_yolo_obb=name.strip() == \"obb-detector\",\n",
    "                            dotenv_path=\"../.env\")\n",
    "\n",
    "    yolo_model = handler.model.unwrap_python_model().detection_model.detection_model.model\n",
    "    validation_results = yolo_model.val(data=r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_hn.yaml\",\n",
    "                                    imgsz=1280,\n",
    "                                    batch=32,\n",
    "                                    conf=0.25,\n",
    "                                    iou=0.45,\n",
    "                                    device=\"cuda\"\n",
    "                                )\n",
    "    \n",
    "    print(validation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Herdnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datalabeling.train.herdnet import HerdnetData, HerdnetTrainer\n",
    "from datalabeling.arguments import Arguments\n",
    "import lightning as L\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowering matrix multiplication precision\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "args = Arguments()\n",
    "args.lr0 = 3e-4\n",
    "args.epochs = 15\n",
    "args.imgsz = 800\n",
    "args.batchsize = 8\n",
    "down_ratio = 2\n",
    "args.data_config_yaml = r\"D:\\datalabeling\\data\\data_config.yaml\"\n",
    "args.path_weights = r\"D:\\datalabeling\\models\\20220329_HerdNet_Ennedi_dataset_2023.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: load Prediction images\n",
    "with open(args.data_config_yaml, 'r') as file:\n",
    "    data_config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "images_path = os.path.join(data_config['path'],data_config['test'][0])\n",
    "images_path = list(Path(images_path).glob('*'))\n",
    "\n",
    "\n",
    "# Data\n",
    "datamodule = HerdnetData(data_config_yaml=args.data_config_yaml,\n",
    "                            patch_size=args.imgsz,\n",
    "                            batch_size=args.batchsize,\n",
    "                            down_ratio=down_ratio,\n",
    "                            train_empty_ratio=0.,\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,targets = datamodule.val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm(datamodule.val_dataloader()):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    batched = dict(img=torch.stack([p[0] for p in batch])\n",
    "                )\n",
    "    targets = [p[1] for p in batch]\n",
    "    keys = targets[0].keys()\n",
    "    \n",
    "    for k in keys:\n",
    "        if k == 'points':\n",
    "            batched[k] = torch.vstack([a[k] for a in targets])\n",
    "        elif k == 'labels':\n",
    "            batched[k] = torch.hstack([a[k] for a in targets])\n",
    "        elif (k == 'w') or (k == 'h'):\n",
    "            batched[k] = torch.hstack([torch.Tensor(a[k]) for a in targets])\n",
    "        else:\n",
    "            batched[k] = [a[k] for a in targets]    \n",
    "\n",
    "    return batched\n",
    "\n",
    "loader = DataLoader(datamodule.val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "batch = next(iter(loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in loader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{str(k):v for k,v in data_config['names'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a few\n",
    "# random.shuffle(images_path)\n",
    "images_path = images_path[:10]\n",
    "datamodule.set_predict_dataset(images_path=images_path,batchsize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "for img in datamodule.predict_dataloader():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "checkpoint_path = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\tools\\lightning-ckpts\\epoch=23-step=2040.ckpt\"\n",
    "herdnet_trainer = HerdnetTrainer.load_from_checkpoint(checkpoint_path=checkpoint_path,\n",
    "                                                            args=args,\n",
    "                                                            ce_weight=None,\n",
    "                                                            work_dir='../.tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(accelerator=\"auto\",profiler='simple')\n",
    "out = trainer.predict(model=herdnet_trainer,\n",
    "                datamodule=datamodule,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(out,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with SAM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import SAM\n",
    "from ultralytics.data.dataset import YOLODataset, YOLOConcatDataset\n",
    "from datalabeling.train.herdnet import HerdnetData, HerdnetTrainer\n",
    "from datalabeling.arguments import Arguments\n",
    "import lightning as L\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from ultralytics.data.utils import visualize_image_annotations\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.compile(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments()\n",
    "args.lr0 = 3e-4\n",
    "args.epochs = 15\n",
    "args.imgsz = 800 # Attention, it will resize the image and the targets\n",
    "args.batchsize = 8\n",
    "down_ratio = 1 # Attention, it will down sample the targets. i.e. x/down_ratio,y/down_ratio\n",
    "args.data_config_yaml = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification-detection.yaml\"\n",
    "args.path_weights = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\base_models_weights\\20220329_HerdNet_Ennedi_dataset_2023.pth\"\n",
    "# args.data_config_yaml = r\"D:\\datalabeling\\data\\data_config.yaml\"\n",
    "\n",
    "# Example: load Prediction images\n",
    "with open(args.data_config_yaml, 'r') as file:\n",
    "    data_config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "images_path = os.path.join(data_config['path'],data_config['test'][0])\n",
    "images_path = list(Path(images_path).glob('*'))\n",
    "\n",
    "\n",
    "# Data\n",
    "# datamodule = HerdnetData(data_config_yaml=args.data_config_yaml,\n",
    "#                             patch_size=args.imgsz,\n",
    "#                             batch_size=args.batchsize,\n",
    "#                             down_ratio=down_ratio,\n",
    "#                             train_empty_ratio=0.,\n",
    "#                             normalization='min_max' # 'standard', 'min_max'\n",
    "#                             )\n",
    "\n",
    "# datamodule.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "datasets = list()\n",
    "split='val'\n",
    "for path in data_config[split]:\n",
    "    images_path = os.path.join(data_config['path'], path)\n",
    "    dataset = YOLODataset(img_path=images_path,task='detect',data={'names':data_config['names']},augment=False,imgsz=args.imgsz,classes=None)\n",
    "    datasets.append(dataset)\n",
    "dataset = YOLOConcatDataset(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    if data[\"cls\"].nelement() == 0:\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points are (x,y)\n",
    "data['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['im_file'], data_config['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = data['im_file']\n",
    "label_path = Path(str(img_path).replace(\"images\",\"labels\")).with_suffix('.txt')\n",
    "label_map = data_config['names']\n",
    "visualize_image_annotations(data['im_file'],label_path,{0: 'wildlife'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model_sam = SAM(r\"..\\base_models_weights\\sam2.1_l.pt\")\n",
    "model_sam.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model, does not work well\n",
    "# model_fastsam = FastSAM(\"FastSAM-x.pt\")\n",
    "# model_fastsam.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastSAM\n",
    "# bboxes = bboxes = torch.cat([data['bboxes'][:,:2], data['bboxes'][:,:2] + data['bboxes'][:,2:]],1) #data['bboxes']\n",
    "# bboxes = (bboxes*640).long().tolist()\n",
    "# print(bboxes)\n",
    "# results_fastsam, = model_fastsam(img_path, device=\"cpu\", bboxes=bboxes[0], imgsz=640, conf=0.4, iou=0.9)\n",
    "# results_fastsam.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.ultralytics.com/models/sam-2/#sam-2-prediction-examples\n",
    "- *Run inference with bboxes prompt* (xmin,ymin,xmax,ymax)\n",
    "    ``results = model(\"path/to/image.jpg\", bboxes=[100, 100, 200, 200]) ``\n",
    "\n",
    "- *Run inference with single point*\n",
    "    ``results = model(points=[900, 370], labels=[1])``\n",
    "\n",
    "- *Run inference with multiple points*\n",
    "    ``results = model(points=[[400, 370], [900, 370]], labels=[1, 1])``\n",
    "\n",
    "- *Run inference with multiple points prompt per object*\n",
    "    ``results = model(points=[[[400, 370], [900, 370]]], labels=[[1, 1]])``\n",
    "\n",
    "- *Run inference with negative points prompt*\n",
    "    ``results = model(points=[[[400, 370], [900, 370]]], labels=[[1, 0]])``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with bboxes prompt\n",
    "bboxes = torch.cat([data['bboxes'][:,:2], data['bboxes'][:,:2] + data['bboxes'][:,2:]],1)\n",
    "bboxes = (bboxes*args.imgsz).long().tolist()\n",
    "labels = data[\"cls\"].long().ravel()+1 # account for background class\n",
    "print(bboxes, labels)\n",
    "results_sam, = model_sam(data['im_file'],\n",
    "                bboxes=bboxes,\n",
    "                labels=labels.tolist()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.view(-1,1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sam.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sam.masks.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = results_sam.masks.data.cpu() * labels.view(-1,1,1)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[0].min(), mask[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert masks into yolo segmentation format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datalabeling.dataset import create_yolo_seg_directory\n",
    "from datalabeling.dataset.converters import convert_segment_masks_to_yolo_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: load Prediction images\n",
    "with open(args.data_config_yaml, 'r') as file:\n",
    "    data_config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_file = \"./example.txt\"\n",
    "mask = mask.cpu().numpy()\n",
    "convert_segment_masks_to_yolo_seg(masks_sam2=mask, output_path=tmp_file, num_classes=data_config['nc'],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_yolo_seg_directory(\n",
    "            data_config_yaml=args.data_config_yaml,\n",
    "            imgsz=args.imgsz,\n",
    "            model_sam=model_sam,\n",
    "            device=args.device,\n",
    "            copy_images_dir=False, # set to True to copy images into segmentation directory\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing inference params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datalabeling.annotator import Detector\n",
    "from datalabeling.arguments import Arguments\n",
    "from datalabeling.dataset.sampling import (get_preds_targets, compute_detector_performance)    \n",
    "import yaml\n",
    "import os\n",
    "from hyperopt import tpe, hp, fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params \n",
    "args = Arguments()\n",
    "args.path_to_weights = r\"C:/Users/Machine Learning/Desktop/workspace-wildAI/datalabeling/runs/mlflow/140168774036374062/57daf3bcd99b4dd4b040cb4f8670960c/artifacts/weights/best.pt\"\n",
    "# args.confidence_threshold = 0.2\n",
    "# args.overlap_ratio = 0.1\n",
    "args.use_sliding_window = True\n",
    "args.device = \"cuda\"\n",
    "args.is_yolo_obb = True\n",
    "args.pred_results_dir = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\.tmp\"\n",
    "args.data_config_yaml = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_hn.yaml\"\n",
    "args.hn_uncertainty_method = \"entropy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load groundtruth\n",
    "with open(args.data_config_yaml,'r') as file:\n",
    "    yolo_config = yaml.load(file,Loader=yaml.FullLoader)\n",
    "\n",
    "split='val'\n",
    "images_path = [os.path.join(yolo_config['path'],yolo_config[split][i]) for i in range(len(yolo_config[split]))]\n",
    "images_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params:dict):\n",
    "\n",
    "    # Define detector\n",
    "    model = Detector(path_to_weights=args.path_to_weights,\n",
    "                        confidence_threshold=params['confidence_threshold'],\n",
    "                        overlap_ratio=params['overlap_ratio'],\n",
    "                        tilesize=params['tilesize'],\n",
    "                        imgsz=params['imgsz'],\n",
    "                        use_sliding_window=args.use_sliding_window,\n",
    "                        device=args.device,\n",
    "                        is_yolo_obb=args.is_yolo_obb\n",
    "                    )\n",
    "\n",
    "    df_results, df_labels, col_names = get_preds_targets(images_dirs=images_path,\n",
    "                                                        pred_results_dir=args.pred_results_dir,\n",
    "                                                        detector=model,\n",
    "                                                        load_results=False,\n",
    "                                                        save_tag=f\"{params['imgsz']}-{params['tilesize']}-{params['overlap_ratio']}-{params['confidence_threshold']}\"\n",
    "                                                        )\n",
    "\n",
    "    df_results_per_img = compute_detector_performance(df_results,df_labels,col_names)\n",
    "    # df_results_per_img = get_uncertainty(df_results_per_img=df_results_per_img,mode=args.hn_uncertainty_method)\n",
    "\n",
    "    # minizing loss -> maximize map50 and map75\n",
    "    loss = -1.0*df_results_per_img[\"map50\"].mean() - df_results_per_img[\"map75\"].mean() #+ df_results_per_img[\"uncertainty\"].mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "                'confidence_threshold': hp.uniform('x', 0.1, 0.7),\n",
    "                'overlap_ratio': hp.uniform('y', 0, 0.25),\n",
    "                'tilesize': hp.choice(label='tilesize',options=[640, 2*640]),\n",
    "                'imgsz': hp.choice(label='imgsz',options=[640, 2*640, 3*640, 4*640]),\n",
    "            }\n",
    "\n",
    "best = fmin(\n",
    "    fn=objective, # Objective Function to optimize\n",
    "    space=search_space, # Hyperparameter's Search Space\n",
    "    algo=tpe.suggest, # Optimization algorithm (representative TPE)\n",
    "    max_evals=2 # Number of optimization attempts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset label format conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_label_format(loaded_df:pd.DataFrame)->str:\n",
    "    \"\"\"checks label format\n",
    "\n",
    "    Args:\n",
    "        loaded_df (pd.DataFrame): target values\n",
    "\n",
    "    Raises:\n",
    "        NotImplementedError: when the format is not yolo or yolo-obb\n",
    "\n",
    "    Returns:\n",
    "        str: yolo or yolo-obb\n",
    "    \"\"\"\n",
    "\n",
    "    num_features = len(loaded_df.columns)\n",
    "\n",
    "    if num_features == 5:\n",
    "        return \"yolo\"\n",
    "    elif num_features == 9:\n",
    "        return \"yolo-obb\"\n",
    "    else:\n",
    "        raise NotImplementedError(f\"The number of features ({num_features}) in the label file is wrong. Check yolo or yolo-obb format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = r\"D:\\PhD\\Data per camp\\DetectionDataset\\Rep 1\\train\\labels\\DJI_20231002150401_0009_0_48_0_1271_640_1911.txt\"\n",
    "df = pd.read_csv(label_path,sep=' ',header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(df.iloc[:,0].dtype, np.dtypes.IntDType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_label_format(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['id','x1','y1','x2','y2','x3','y3','x4','y4']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute test metrics for ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.models.yolo.detect import DetectionValidator\n",
    "from pathlib import Path\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from datalabeling.train.utils import remove_label_cache\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.metrics import ConfusionMatrix, DetMetrics, box_iou\n",
    "from ultralytics.data import converter\n",
    "\n",
    "class CustomValidator(DetectionValidator):\n",
    "    \"\"\"From https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/detect/val.py\n",
    "    Adapted to compute confusion matrix for a given iou threshold\n",
    "    \"\"\"\n",
    "\n",
    "    def init_metrics(self, model):\n",
    "        \"\"\"\n",
    "        Initialize evaluation metrics for YOLO detection validation.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): Model to validate.\n",
    "        \"\"\"\n",
    "        val = self.data.get(self.args.split, \"\")  # validation path\n",
    "        self.is_coco = (\n",
    "            isinstance(val, str)\n",
    "            and \"coco\" in val\n",
    "            and (val.endswith(f\"{os.sep}val2017.txt\") or val.endswith(f\"{os.sep}test-dev2017.txt\"))\n",
    "        )  # is COCO\n",
    "        self.is_lvis = isinstance(val, str) and \"lvis\" in val and not self.is_coco  # is LVIS\n",
    "        self.class_map = converter.coco80_to_coco91_class() if self.is_coco else list(range(1, len(model.names) + 1))\n",
    "        self.args.save_json |= self.args.val and (self.is_coco or self.is_lvis) and not self.training  # run final val\n",
    "        self.names = model.names\n",
    "        self.nc = len(model.names)\n",
    "        self.end2end = getattr(model, \"end2end\", False)\n",
    "        self.metrics.names = self.names\n",
    "        self.metrics.plot = self.args.plots\n",
    "        self.confusion_matrix = ConfusionMatrix(nc=self.nc, conf=self.args.conf,iou_thres=self.args.iou, task='detect')\n",
    "        self.seen = 0\n",
    "        self.jdict = []\n",
    "        self.stats = dict(tp=[], conf=[], pred_cls=[], target_cls=[], target_img=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example \n",
    "args = dict(model=\"../runs/mlflow/140168774036374062/757205b53b454043a0a6481efb72fddd/artifacts/weights/best.pt\", \n",
    "            data=r\"..\\configs\\yolo_configs\\dataset_identification.yaml\"\n",
    "        )\n",
    "validator = CustomValidator(args=args,save_dir=Path(\"./.tmp/\"))\n",
    "\n",
    "name = \"yolo12-X#Identif#mlflow-757205b53b454043a0a6481efb72fddd\"\n",
    "split= 'val'\n",
    "iou_threshold = 0.6\n",
    "conf_threshold = 0.2\n",
    "max_det = 100\n",
    "augment = False\n",
    "imgsz = 800\n",
    "save_dir = './.tmp_runs_yolo'\n",
    "batchsize=16\n",
    "\n",
    "# set args\n",
    "validator.args.conf = conf_threshold\n",
    "validator.args.iou = iou_threshold\n",
    "validator.args.mode = 'val'\n",
    "validator.args.imgsz = imgsz\n",
    "validator.args.batch = batchsize\n",
    "validator.args.device = 'cuda'\n",
    "validator.args.augment = augment\n",
    "validator.args.split = split\n",
    "validator.args.name = name + \"#\" + split + f\"#{round(conf_threshold*100)}#{round(iou_threshold*100)}#{augment}#{max_det}-\"\n",
    "validator.args.project = save_dir\n",
    "validator.args.max_det = max_det\n",
    "validator.args.plots = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(validator:CustomValidator):\n",
    "\n",
    "    remove_label_cache(data_config_yaml=validator.args.data)\n",
    "\n",
    "    results = validator()\n",
    "    \n",
    "    cf_matrix = validator.confusion_matrix.matrix\n",
    "    labels = list(validator.names.values()) + [\"background\"]\n",
    "\n",
    "    for i,label in enumerate(labels + [\"background\"]):\n",
    "\n",
    "        if label == 'background':\n",
    "            break\n",
    "\n",
    "        tp = cf_matrix[i,i]\n",
    "        actual_positive = cf_matrix[:,i].sum()\n",
    "        predicted_positive  = cf_matrix[i,:].sum()\n",
    "        # fp = predicted_positive - tp\n",
    "        # fn = actual_positive - tp\n",
    "\n",
    "        precision = tp/(predicted_positive + 1e-8)\n",
    "        recall = tp/(actual_positive + 1e-8)\n",
    "        f1score = 2*precision*recall / (precision + recall + 1e-8)\n",
    "\n",
    "        results = dict(precision=round(precision,4), recall=round(recall,4), f1score=round(f1score,4))\n",
    "\n",
    "        print(f\"results for {label} : \", results,end='\\n')\n",
    "\n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(12, 9), tight_layout=True)\n",
    "    # seaborn.set_theme(font_scale=1.0)  # for label size\n",
    "    # ticklabels = labels\n",
    "\n",
    "    # seaborn.heatmap(\n",
    "    #     cf_matrix,\n",
    "    #     ax=ax,\n",
    "    #     annot=True,\n",
    "    #     annot_kws={\"size\": 8},\n",
    "    #     cmap=\"Blues\",\n",
    "    #     fmt=\".0f\",\n",
    "    #     square=True,\n",
    "    #     vmin=0.0,\n",
    "    #     xticklabels=ticklabels,\n",
    "    #     yticklabels=ticklabels,\n",
    "    # ).set_facecolor((1, 1, 1))\n",
    "    # title = \"Confusion Matrix\"\n",
    "    # ax.set_xlabel(\"True\")\n",
    "    # ax.set_ylabel(\"Predicted\")\n",
    "    # ax.set_title(title)\n",
    "\n",
    "    # plot_fname = Path(validator.save_dir) / f\"{title.lower().replace(' ', '_')}.png\"\n",
    "    # fig.savefig(plot_fname, dpi=250)\n",
    "    # plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = validate(validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.confusion_matrix.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validator.confusion_matrix.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf_matrix = validator.confusion_matrix.matrix\n",
    "# labels = list(validator.names.values()) + [\"background\"]\n",
    "\n",
    "# for i,label in enumerate(labels + [\"background\"]):\n",
    "\n",
    "#     if label == 'background':\n",
    "#         break\n",
    "\n",
    "#     tp = cf_matrix[i,i]\n",
    "#     actual_p = cf_matrix[:,i].sum()\n",
    "#     predicted_p  = cf_matrix[i,:].sum()\n",
    "#     fp = predicted_p - tp\n",
    "#     fn = actual_p - tp\n",
    "\n",
    "#     precision = tp/(predicted_p + 1e-8)\n",
    "#     recall = tp/(actual_p + 1e-8)\n",
    "#     f1score = 2*precision*recall / (precision + recall + 1e-8)\n",
    "\n",
    "#     results = dict(precision=round(precision,4), recall=round(recall,4), f1score=round(f1score,4))\n",
    "\n",
    "#     print(f\"results for {label} : \", results,end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validator.confusion_matrix.plot(normalize=False,save_dir=validator.save_dir,names=validator.names.values(),on_plot=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, figsize=(12, 9), tight_layout=True)\n",
    "# seaborn.set_theme(font_scale=1.0)  # for label size\n",
    "# ticklabels = labels\n",
    "\n",
    "# seaborn.heatmap(\n",
    "#     cf_matrix,\n",
    "#     ax=ax,\n",
    "#     annot=True,\n",
    "#     annot_kws={\"size\": 8},\n",
    "#     cmap=\"Blues\",\n",
    "#     fmt=\".0f\",\n",
    "#     square=True,\n",
    "#     vmin=0.0,\n",
    "#     xticklabels=ticklabels,\n",
    "#     yticklabels=ticklabels,\n",
    "# ).set_facecolor((1, 1, 1))\n",
    "# title = \"Confusion Matrix\"\n",
    "# ax.set_xlabel(\"True\")\n",
    "# ax.set_ylabel(\"Predicted\")\n",
    "# ax.set_title(title)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute test metrics for PP-YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchmetrics.detection import MeanAveragePrecision, IntersectionOverUnion\n",
    "import torch\n",
    "from tqdm import  tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics.utils.metrics import ConfusionMatrix\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install faster-coco-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(load_path):\n",
    "    with open(load_path, encoding='utf8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\runs_ppd\\test\\ppyoloe_plus_sod_crn_l_largesize_80e_visdronetr_empty_ratio_0.33_freeze_0.5\\bbox.json\"\n",
    "gt_path = r\"D:\\PhD\\Data per camp\\DetectionDataset\\Identification-split\\coco-dataset\\annotations\\annotations_test.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = load_json(preds_path)\n",
    "gt = load_json(gt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame.from_records(preds)\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.DataFrame.from_records(gt['annotations'])\n",
    "gt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(preds_path:str,gt_path:str, conf_matrix:ConfusionMatrix):\n",
    "\n",
    "    preds = load_json(preds_path)\n",
    "    preds = pd.DataFrame.from_records(preds)\n",
    "\n",
    "    gt = load_json(gt_path)\n",
    "    gt = pd.DataFrame.from_records(gt['annotations'])\n",
    "\n",
    "\n",
    "    mAP_computer = MeanAveragePrecision(box_format='xywh',iou_type='bbox',average='macro',\n",
    "                                        iou_thresholds=[0.5,0.6],\n",
    "                                        rec_thresholds=None,\n",
    "                                        class_metrics=True,\n",
    "                                        extended_summary=True)\n",
    "\n",
    "    for image_id in tqdm(preds.image_id.unique()):\n",
    "\n",
    "        mask_pred = preds['image_id'] == image_id\n",
    "        bbox_preds = preds.loc[mask_pred,'bbox'].to_list()\n",
    "        labels_pred = preds.loc[mask_pred,'category_id'].to_list()\n",
    "        scores = preds.loc[mask_pred,'score'].to_list()\n",
    "        pred_ = [dict(boxes=torch.Tensor(bbox_preds), labels=torch.Tensor(labels_pred).long(), scores=torch.Tensor(scores))]\n",
    "\n",
    "        mask_gt = gt['image_id'] == image_id\n",
    "        bbox_gt = gt.loc[mask_gt,'bbox'].to_list()\n",
    "        labels = gt.loc[mask_gt,'category_id'].to_list()\n",
    "        gt_ = [dict(boxes=torch.Tensor(bbox_gt),labels=torch.Tensor(labels).long())]\n",
    "        \n",
    "        mAP_computer.update(pred_,gt_)\n",
    "\n",
    "        # compute confusion matrix\n",
    "        \n",
    "        pred_bboxes = pred_[0]['boxes']\n",
    "        pred_bboxes[:,2:] = pred_bboxes[:,:2] + pred_bboxes[:,2:]\n",
    "        pred_scores = pred_[0]['scores'].view(-1,1)\n",
    "        labels = pred_[0]['labels'].view(-1,1)\n",
    "        dets = torch.hstack([pred_bboxes, pred_scores, labels])\n",
    "\n",
    "        gt_bboxes = gt_[0]['boxes']\n",
    "        if gt_bboxes.shape[0] != 0:\n",
    "            gt_bboxes[:,2:] = gt_bboxes[:,:2] + gt_bboxes[:,2:]\n",
    "        conf_matrix.process_batch(detections=dets, \n",
    "                                gt_bboxes=gt_bboxes, \n",
    "                                gt_cls=gt_[0]['labels']\n",
    "                            )\n",
    "                                    \n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "        #     raise ValueError\n",
    "            \n",
    "    results = mAP_computer.compute()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*20,'Detection','-'*20)\n",
    "\n",
    "preds_path = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\runs_ppd\\test\\ppyoloe_plus_sod_crn_l_largesize_80e_visdronetr_empty_ratio_0.33_freeze_0.5\\bbox.json\"\n",
    "gt_path = r\"D:\\PhD\\Data per camp\\DetectionDataset\\Identification-split\\coco-dataset\\annotations\\annotations_test.json\"\n",
    "\n",
    "conf_matrix = ConfusionMatrix(nc=len(cls_ids), conf=0.25, iou_thres=0.6)\n",
    "\n",
    "results = get_metrics(preds_path=preds_path,gt_path=gt_path,conf_matrix=conf_matrix)\n",
    "\n",
    "# results.keys()\n",
    "\n",
    "print('mAP50', results['map_50'].item())\n",
    "\n",
    "gt = load_json(gt_path)\n",
    "cls_ids = [i['id'] for i in gt['categories']]\n",
    "cls_names = [i['name'] for i in gt['categories']]\n",
    "\n",
    "\n",
    "labels = cls_names + [\"background\"]\n",
    "results_per_cls = dict()\n",
    "\n",
    "for i,label in enumerate(labels):\n",
    "\n",
    "    if label == 'background':\n",
    "        break\n",
    "\n",
    "    tp = conf_matrix.matrix[i,i]\n",
    "    actual_positive = conf_matrix.matrix[:,i].sum()\n",
    "    predicted_positive  = conf_matrix.matrix[i,:].sum()\n",
    "    # fp = predicted_positive - tp\n",
    "    # fn = actual_positive - tp\n",
    "\n",
    "    precision = tp/(predicted_positive + 1e-8)\n",
    "    recall = tp/(actual_positive + 1e-8)\n",
    "    f1score = 2*precision*recall / (precision + recall + 1e-8)\n",
    "\n",
    "    results = dict(precision=round(precision,4), recall=round(recall,4), f1score=round(f1score,4))\n",
    "\n",
    "    results_per_cls[label] = results\n",
    "\n",
    "    print(f\"results for {label} : \", results,end='\\n')\n",
    "\n",
    "with open(Path(preds_path).parent/'metrics_updated.json','w') as file:\n",
    "    json.dump(results_per_cls,file,indent=2)\n",
    "\n",
    "# save confusion matrix\n",
    "conf_matrix.plot(False,save_dir=Path(preds_path).parent,names=cls_names)\n",
    "\n",
    "# rec_thrs = torch.linspace(0.0, 1.00, round(1.00 / 0.01) + 1)\n",
    "\n",
    "# for cls_id in cls_ids: # class with index 0\n",
    "\n",
    "#     print(\"\\nClass: \",gt['categories'][cls_id]['name'])\n",
    "\n",
    "#     mAP = results['map_per_class'].item()\n",
    "#     print(\"AP@60 for\",gt['categories'][cls_id]['name'],': ', round(mAP,4))\n",
    "\n",
    "#     iou_id = 1 # 0.6\n",
    "#     recall = results['recall'][iou_id,cls_id,0,-1]\n",
    "#     print('Recall:', recall.item())\n",
    "\n",
    "#     recall_idx = (rec_thrs < recall).sum().item()\n",
    "#     recall_idx\n",
    "\n",
    "#     precision = results['precision'][iou_id,recall_idx-1,cls_id,0,-1]\n",
    "#     print('Precision:', precision.item())\n",
    "\n",
    "#     f1_score = 2 * recall*precision/(recall + precision)\n",
    "#     print('f1_score:', f1_score.item())\n",
    "\n",
    "    # plt.plot(rec_thrs,results['precision'][iou_id,:,cls_id,0,-1])\n",
    "    # plt.xlabel('Recall')\n",
    "    # plt.ylabel('Precision')\n",
    "    # plt.title(f\"Precision-Recall curve: mAP50={results['map_50']:.4f}\")\n",
    "    \n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_names, conf_matrix.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*20,'Identification','-'*20)\n",
    "\n",
    "preds_path = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\runs_ppd\\test\\ppyoloe_plus_sod_crn_l_largesize_80e_visdrone_identif-tr_empty_ratio_0.1_freeze_0.75\\bbox.json\"\n",
    "\n",
    "gt_path = r\"D:\\PhD\\Data per camp\\IdentificationDataset\\coco-dataset\\annotations\\annotations_test.json\"\n",
    "\n",
    "gt = load_json(gt_path)\n",
    "cls_ids = [i['id'] for i in gt['categories']]\n",
    "cls_names = [i['name'] for i in gt['categories']]\n",
    "\n",
    "conf_matrix = ConfusionMatrix(nc=len(cls_ids), conf=0.25, iou_thres=0.6)\n",
    "\n",
    "results = get_metrics(preds_path=preds_path,gt_path=gt_path,conf_matrix=conf_matrix)\n",
    "\n",
    "# results.keys()\n",
    "\n",
    "print('mAP50', results['map_50'].item())\n",
    "\n",
    "rec_thrs = torch.linspace(0.0, 1.00, round(1.00 / 0.01) + 1)\n",
    "\n",
    "labels = cls_names + [\"background\"]\n",
    "results_per_cls = dict()\n",
    "\n",
    "for i,label in enumerate(labels):\n",
    "\n",
    "    if label == 'background':\n",
    "        break\n",
    "\n",
    "    tp = conf_matrix.matrix[i,i]\n",
    "    actual_positive = conf_matrix.matrix[:,i].sum()\n",
    "    predicted_positive  = conf_matrix.matrix[i,:].sum()\n",
    "    # fp = predicted_positive - tp\n",
    "    # fn = actual_positive - tp\n",
    "\n",
    "    precision = tp/(predicted_positive + 1e-8)\n",
    "    recall = tp/(actual_positive + 1e-8)\n",
    "    f1score = 2*precision*recall / (precision + recall + 1e-8)\n",
    "\n",
    "    results = dict(precision=round(precision,4), recall=round(recall,4), f1score=round(f1score,4))\n",
    "\n",
    "    results_per_cls[label] = results\n",
    "\n",
    "    print(f\"results for {label} : \", results,end='\\n')\n",
    "\n",
    "with open(Path(preds_path).parent/'metrics_updated.json','w') as file:\n",
    "    json.dump(results_per_cls,file,indent=2)\n",
    "\n",
    "# save confusion matrix\n",
    "conf_matrix.plot(False,save_dir=Path(preds_path).parent,names=cls_names)\n",
    "\n",
    "\n",
    "# for cls_id in cls_ids: # class with index 0\n",
    "\n",
    "#     label = gt['categories'][cls_id]['name']\n",
    "\n",
    "#     print(\"\\nClass: \",label)\n",
    "\n",
    "#     mAP = results['map_per_class'][cls_id].item()\n",
    "#     print(\"AP@60 for\",label,': ', round(mAP,4))\n",
    "\n",
    "#     iou_id = 1 # 0.6\n",
    "#     recall = results['recall'][iou_id,cls_id,0,-1]\n",
    "#     print('Recall:', round(recall.item(),4))\n",
    "\n",
    "#     recall_idx = (rec_thrs < recall).sum().item()\n",
    "#     recall_idx\n",
    "\n",
    "#     precision = results['precision'][iou_id,recall_idx-1,cls_id,0,-1]\n",
    "#     print('Precision:', round(precision.item(),4))\n",
    "\n",
    "#     f1_score = 2 * recall*precision/(recall + precision)\n",
    "#     print('f1_score:', round(f1_score.item(),4))\n",
    "\n",
    "#     plt.plot(rec_thrs,results['precision'][iou_id,:,cls_id,0,-1])\n",
    "#     plt.xlabel('Recall')\n",
    "#     plt.ylabel('Precision')\n",
    "#     plt.title(f\"Precision-Recall curve for {label}: mAP50={results['map_50']:.4f}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
