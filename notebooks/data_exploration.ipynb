{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get GPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\datalabeling\\.venv\\Lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datalabeling.common.annotation_utils import GPSUtils\n",
    "from pathlib import Path\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gps_coords(image_dir:str,\n",
    "                   image_paths:list[str]=None,\n",
    "                   exts:list[str] = ['*.jpg','*.jpeg','*.png',]):\n",
    "\n",
    "    exts = [e.lower() for e in exts] + [e.capitalize() for e in exts]\n",
    "\n",
    "    if image_paths is None:\n",
    "        image_paths = chain.from_iterable([Path(image_dir).glob(ext) for ext in exts])\n",
    "\n",
    "    gps_coords = [GPSUtils.get_gps_coord(file_name=path,return_as_decimal=True)[0] for path in image_paths]\n",
    "    \n",
    "    gps_coords = pd.DataFrame(data=gps_coords,\n",
    "                 columns=[\"Latitude\", \"Longitude\", \"Elevation\"])\n",
    "    \n",
    "    return gps_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GPSVersionID': [2, 3, 0, 0],\n",
       " 'GPSLatitudeRef': 'S',\n",
       " 'GPSLatitude': (23.0, 12.0, 38.622960045019695),\n",
       " 'GPSLongitudeRef': 'E',\n",
       " 'GPSLongitude': (18.0, 21.0, 29.31192005710207),\n",
       " 'GPSAltitude': '1496.6827309236949',\n",
       " 'GPSTrack': 79.89536266349585}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_exif = GPSUtils.get_exif(r\"D:\\savmap_dataset_v2\\raw\\images\\0aff71b5a50d4357bfcca7cf5f614b7d.JPG\")\n",
    "\n",
    "gps_info = GPSUtils.get_gps_info(extracted_exif)\n",
    "\n",
    "gps_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gps = get_gps_coords(image_dir=r\"D:\\savmap_dataset_v2\\raw\\images\",\n",
    "                        image_paths=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Elevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-23.185786</td>\n",
       "      <td>18.380313</td>\n",
       "      <td>1489.539400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-23.184062</td>\n",
       "      <td>18.376709</td>\n",
       "      <td>1486.842700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-23.215928</td>\n",
       "      <td>18.360828</td>\n",
       "      <td>1496.319588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-23.215876</td>\n",
       "      <td>18.355488</td>\n",
       "      <td>1495.295409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-23.221646</td>\n",
       "      <td>18.395676</td>\n",
       "      <td>1480.894410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>-23.215110</td>\n",
       "      <td>18.430430</td>\n",
       "      <td>1478.588015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>-23.193977</td>\n",
       "      <td>18.384050</td>\n",
       "      <td>1461.046392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>-23.181181</td>\n",
       "      <td>18.380090</td>\n",
       "      <td>1488.729800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>-23.209417</td>\n",
       "      <td>18.430249</td>\n",
       "      <td>1479.213622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>-23.210628</td>\n",
       "      <td>18.429554</td>\n",
       "      <td>1484.437008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1308 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Latitude  Longitude    Elevation\n",
       "0    -23.185786  18.380313  1489.539400\n",
       "1    -23.184062  18.376709  1486.842700\n",
       "2    -23.215928  18.360828  1496.319588\n",
       "3    -23.215876  18.355488  1495.295409\n",
       "4    -23.221646  18.395676  1480.894410\n",
       "...         ...        ...          ...\n",
       "1303 -23.215110  18.430430  1478.588015\n",
       "1304 -23.193977  18.384050  1461.046392\n",
       "1305 -23.181181  18.380090  1488.729800\n",
       "1306 -23.209417  18.430249  1479.213622\n",
       "1307 -23.210628  18.429554  1484.437008\n",
       "\n",
       "[1308 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get parent image from sliced image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os\n",
    "from itertools import product\n",
    "import torchvision, PIL\n",
    "\n",
    "def get_coordinates(img_path:str,tile_w:int,tile_h:int,overlaping_factor:float):\n",
    "    \n",
    "    pil_img = PIL.Image.open(img_path)\n",
    "    img_tensor = torchvision.transforms.ToTensor()(pil_img)\n",
    "\n",
    "    image_width=img_tensor.shape[2]\n",
    "    image_height=img_tensor.shape[1]\n",
    "\n",
    "    # x limits\n",
    "    lim = math.ceil((image_width-tile_w)/((1-overlaping_factor)*tile_w))\n",
    "    x_right = [math.floor(tile_w + i*(1-overlaping_factor)*tile_w) for i in range(lim)]\n",
    "    x_coords = [(x-tile_w,x) for x in x_right]\n",
    "    if len(x_coords)>0:\n",
    "        left,right = x_coords[-1]\n",
    "        x_coords[-1] = (left,image_width) # extending to remaining pixels\n",
    "\n",
    "    # y limits\n",
    "    lim = math.ceil((image_height-tile_h)/((1-overlaping_factor)*tile_h))\n",
    "    y_bottom = [math.floor(tile_h + i*(1-overlaping_factor)*tile_h) for i in range(lim)]\n",
    "    y_coords = [(y-tile_h,y) for y in y_bottom]\n",
    "    if len(y_coords)>0:\n",
    "        top,bottom = y_coords[-1]\n",
    "        y_coords[-1] = (top,image_height) # extending to remaining pixels\n",
    "\n",
    "    # tiles coordinates\n",
    "    if len(y_coords)>0 and len(x_coords)>0:\n",
    "        pass\n",
    "    elif len(y_coords) == 0:\n",
    "        y_coords = [(0,image_height),]\n",
    "    elif len(x_coords) == 0:\n",
    "        x_coords = [(0,image_width),]\n",
    "\n",
    "    coordinates = product(x_coords,y_coords)\n",
    "\n",
    "    return list(coordinates)\n",
    "\n",
    "def get_save_paths(\n",
    "    batch:list,\n",
    "    basename: str,\n",
    "    dest_folder: str\n",
    "    ) -> None:\n",
    "    ''' Save mini-batch tensors into image files\n",
    "\n",
    "    Use torchvision save_image function,\n",
    "    see https://pytorch.org/vision/stable/utils.html#torchvision.utils.save_image\n",
    "\n",
    "    Args:\n",
    "        batch (list): mini-batch tensor\n",
    "        basename (str) : parent image name, with extension\n",
    "        dest_folder (str): destination folder path\n",
    "    '''\n",
    "    paths = list()\n",
    "    base_wo_extension, extension = basename.split('.')[0], basename.split('.')[1]\n",
    "    for i, b in enumerate(range(len(batch))):\n",
    "        full_path = '_'.join([base_wo_extension, str(i) + '.']) + extension\n",
    "        save_path = os.path.join(dest_folder, full_path)\n",
    "\n",
    "        get_save_paths.append(save_path)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords =  get_coordinates(img_path:str,tile_w=2000,tile_h=2000,overlaping_factor=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image tiling for annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meanings of arguments\n",
    "- ```-ratioheight``` : proportion of tile  w.r.t height of image. Example 0.5 means dividing the image in two bands w.r.t height.\n",
    "- ```-ratiowidth``` : proportion of tile w.r.t to width of image. Example 1.0 means the width of the tile is the same as the image.\n",
    "- ```-overlapfactor``` : percentage of overlap. It should be less than 1.\n",
    "- ```-rmheight``` : percentage of height to remove or crop at bottom and top\n",
    "- ```-rmwidth``` : percentage of width to remove or crop on each side of the image\n",
    "- ```-pattern``` : \"**/*.JPG\" will get all .JPG images in directory and subdirectories. On windows it will get both .JPG and .jpg. On unix it will only get .JPG images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New script for tiling data\n",
    "# images_to_tile = r\"D:\\PhD\\Data per camp\\Extra training data\\savmap_dataset_v2\\raw_data\\images\"\n",
    "# destination_directory = r\"D:\\PhD\\Data per camp\\Extra training data\\savmap_dataset_v2\\raw_data\\images-tiled\"\n",
    "!python ../../HerdNet/tools/patcher.py \"D:\\PhD\\Harvard data\\Satara_east\\Selection\" 0 0 0 -overlapfactor 0.1  -ratiowidth 0.5 -ratioheight 0.5 -rmheight 0.1 -rmwidth 0.1 -dest \"D:\\PhD\\Harvard data\\Satara_east\\Selection - tiled\" -pattern \"**/*.JPG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-annotating data for Labelstudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "from datalabeling.ml import Annotator\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a JSON file to be uuploaded to Label studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "# provide correct alias, \"pt\", \"onnx\"\n",
    "# alias = \"last\" # the aliases are found in mlflow tracker UI, use \"last-1\" to use the previous model\n",
    "# name = \"obb-detector\" # detector, \"obb-detector\"\n",
    "# handler = Annotator(mlflow_model_alias=alias,\n",
    "#                     mlflow_model_name=name,\n",
    "#                     is_yolo_obb= name.strip() == \"obb-detector\",\n",
    "#                     # dotenv_path=\"../.env\"\n",
    "#                     )\n",
    "# path_img_dir=r\"D:\\PhD\\Africa Parks\\Liuwa aerial survey_ALL\\CENSUS 2019\\DAY 2 CENSUS 2019_CONVERTED\\AP 2019 day 2 - tiled\"\n",
    "# root=\"D:\\\\\"\n",
    "# save_json_path = os.path.join(Path(path_img_dir).parent, f\"{Path(path_img_dir).name}_preannotation_label-studio.json\")\n",
    "\n",
    "# # build and saves json\n",
    "# directory_preds = handler.build_upload_json(path_img_dir=path_img_dir,\n",
    "#                                             root=root,\n",
    "#                                             save_json_path=save_json_path,\n",
    "#                                             pattern=\"**/*.JPG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-annotating an existing project using Label studio API\n",
    "It seems that it will not work well (i.e. filtering) with older projects created prior to Label studio software update.\n",
    "It is the **recommended way of pre-annotating data in Labelstudio**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide correct alias, \"pt\", \"onnx\"\n",
    "aliases = [\"yolov12s\",]\n",
    "project_id = 96 # insert correct project_id by loooking at the url\n",
    "for alias in aliases:\n",
    "    name = \"detector\" # detector, \"obb-detector\"\n",
    "    handler = Annotator(mlflow_model_alias=alias,\n",
    "                        mlflow_model_name=name,\n",
    "                        confidence_threshold=0.15,\n",
    "                        is_yolo_obb=name.strip() == \"obb-detector\",\n",
    "                        dotenv_path=\"../.env\")\n",
    "    handler.upload_predictions(project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before running the script below, make sure you have exported the annotations so you CAN revert back!!!**\n",
    "- CLEANING ANNOTATIONS that have been mistakenly saved with label=\"wildlife\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Cleaning annotations - NO WAY BACK\n",
    "name = \"obb-detector\"\n",
    "handler = Annotator(mlflow_model_alias=\"version6\",\n",
    "                        mlflow_model_name=name,\n",
    "                        confidence_threshold=0.25,\n",
    "                        is_yolo_obb=name.strip() == \"obb-detector\",\n",
    "                        dotenv_path=\"../.env\")\n",
    "\n",
    "# Select project\n",
    "project_id = 88\n",
    "project = handler.labelstudio_client.get_project(id=project_id)\n",
    "\n",
    "# Delete annotations saved with label \"wildlife\" assigned by the predictor\n",
    "tasks = project.get_tasks()\n",
    "for task in tqdm(tasks,desc=\"correcting annotations\"):\n",
    "        task_id = task['id']\n",
    "        img_url = task['data']['image']\n",
    "\n",
    "        if len(task[\"annotations\"][0]['result'])>1:\n",
    "            results_to_keep = []\n",
    "            annot_id = task[\"annotations\"][0][\"id\"]\n",
    "            for annot in task['annotations'][0]['result']:\n",
    "                if annot['value']['rectanglelabels'][0] != 'wildlife':\n",
    "                    results_to_keep.append(annot)\n",
    "                    # print(annot['value'],annot['id'],end=\"\\n\")\n",
    "            # print(f\"Updating annotations {annot_id} from task {task_id}.\")\n",
    "            # print(results_to_keep)\n",
    "            project.update_annotation(annot_id,result=results_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(task['annotations']), len(task['annotations'][0]['result']), task['id'], task[\"annotations\"][0][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task['annotations'][0]['result'][0] #['value']['rectanglelabels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_keep = []\n",
    "for annot in task['annotations'][0]['result']:\n",
    "    if annot['value']['rectanglelabels'][0] != 'wildlife':\n",
    "        results_to_keep.append(annot)\n",
    "        print(annot['value'],annot['id'],end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.update_annotation(annotation_id=...,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up inference on intel, make changes inn ultralytics/nn/autobackend.py:\n",
    "```\n",
    "- device_name = \"AUTO:NPU,GPU,CPU\" # CPU, GPU, NPU, AUTO,\"AUTO:GPU,NPU\"\n",
    "- inference_mode = \"LATENCY\" # OpenVINO inference modes are 'LATENCY', 'THROUGHPUT' (not recommended), or 'CUMULATIVE_THROUGHPUT'\n",
    "- LOGGER.info(f\"Using OpenVINO {inference_mode} mode for inference...\")\n",
    "- ov_compiled_model = core.compile_model(\n",
    "                ov_model,\n",
    "                device_name=device_name,  # AUTO selects best available device, do not modify\n",
    "                config={\"PERFORMANCE_HINT\": inference_mode,\n",
    "                        \"CACHE_DIR\": os.environ[\"OPENVINO_CACHE_MODEL\"]}, # make sure to set environment variable\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using path_to_weights\n",
    "# go to ultralytics.nn.autobackend to modify ov_compiled device to \"AUTO:NPU,GPU,CPU\"\n",
    "\n",
    "use_sliding_window=True\n",
    "\n",
    "handler = Annotator(path_to_weights=r\"D:\\datalabeling\\base_models_weights\\best.pt\",\n",
    "                    is_yolo_obb=True,\n",
    "                    tilesize=640,\n",
    "                    overlapratio=0.1,\n",
    "                    use_sliding_window=use_sliding_window,\n",
    "                    confidence_threshold=0.5,\n",
    "                    # device=\"NPU\", # \"cpu\", \"cuda\"\n",
    "                    tag_to_append=f\"-sahi:{use_sliding_window}\",\n",
    "                    dotenv_path=\"../.env\")\n",
    "\n",
    "project_id = 3 # insert correct project_id by loooking at the url\n",
    "top_n=10\n",
    "handler.upload_predictions(project_id=project_id,top_n=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_studio_ml.utils import get_local_path\n",
    "from urllib.parse import unquote\n",
    "path = unquote(\"/data/local-files/?d=savmap_dataset_v2%5Cimages_splits%5C003a34ee6b7841e6851b8fe511ebe102_0.JPG\")\n",
    "get_local_path(path,download_resources=False)#,os.path.exists(get_local_path(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "from datalabeling.ml import Detector, Annotator\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load env variable, loads model cache location!!\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datalabeling.annotator'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m dotenv_path=\u001b[33m\"\u001b[39m\u001b[33m../.env\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m name = \u001b[33m\"\u001b[39m\u001b[33mobb-detector\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# detector, \"obb-detector\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m handler = \u001b[43mAnnotator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlflow_model_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43malias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mmlflow_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mconfidence_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mis_yolo_obb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mobb-detector\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mdotenv_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdotenv_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\workspace-wildAI\\datalabeling\\src\\datalabeling\\ml\\interface.py:74\u001b[39m, in \u001b[36mAnnotator.__init__\u001b[39m\u001b[34m(self, dotenv_path, path_to_weights, mlflow_model_alias, mlflow_model_name, tilesize, overlapratio, device, use_sliding_window, is_yolo_obb, confidence_threshold, tag_to_append)\u001b[39m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28mself\u001b[39m.modelversion = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m + tag_to_append\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mself\u001b[39m.modelURI = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodels:/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpyfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodelURI\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = Detector(\n\u001b[32m     77\u001b[39m         path_to_weights=path_to_weights,\n\u001b[32m     78\u001b[39m         confidence_threshold=confidence_threshold,\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m         is_yolo_obb=is_yolo_obb,\n\u001b[32m     84\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\.venv\\Lib\\site-packages\\mlflow\\tracing\\provider.py:202\u001b[39m, in \u001b[36mtrace_disabled.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    200\u001b[39m disable()\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    204\u001b[39m     enable()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\.venv\\Lib\\site-packages\\mlflow\\pyfunc\\__init__.py:1028\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_uri, suppress_warnings, dst_path, model_config)\u001b[39m\n\u001b[32m   1020\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m conf[MAIN] == _DATABRICKS_FS_LOADER_MODULE \u001b[38;5;129;01mand\u001b[39;00m e.name.startswith(\u001b[33m\"\u001b[39m\u001b[33mdatabricks\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1021\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[32m   1022\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mNote: mlflow.pyfunc.load_model is not supported for Feature Store models. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1026\u001b[39m             BAD_REQUEST,\n\u001b[32m   1027\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1029\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1030\u001b[39m     \u001b[38;5;66;03m# clean up the dependencies schema which is set to global state after loading the model.\u001b[39;00m\n\u001b[32m   1031\u001b[39m     \u001b[38;5;66;03m# This avoids the schema being used by other models loaded in the same process.\u001b[39;00m\n\u001b[32m   1032\u001b[39m     _clear_dependencies_schemas()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\.venv\\Lib\\site-packages\\mlflow\\pyfunc\\__init__.py:1013\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_uri, suppress_warnings, dst_path, model_config)\u001b[39m\n\u001b[32m   1011\u001b[39m         model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path, model_config)\n\u001b[32m   1012\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m         model_impl = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mMAIN\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_load_pyfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1015\u001b[39m     \u001b[38;5;66;03m# This error message is particularly for the case when the error is caused by module\u001b[39;00m\n\u001b[32m   1016\u001b[39m     \u001b[38;5;66;03m# \"databricks.feature_store.mlflow_model\". But depending on the environment, the offending\u001b[39;00m\n\u001b[32m   1017\u001b[39m     \u001b[38;5;66;03m# module might be \"databricks\", \"databricks.feature_store\" or full package. So we will\u001b[39;00m\n\u001b[32m   1018\u001b[39m     \u001b[38;5;66;03m# raise the error with the following note if \"databricks\" presents in the error. All non-\u001b[39;00m\n\u001b[32m   1019\u001b[39m     \u001b[38;5;66;03m# databricks moduel errors will just be re-raised.\u001b[39;00m\n\u001b[32m   1020\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m conf[MAIN] == _DATABRICKS_FS_LOADER_MODULE \u001b[38;5;129;01mand\u001b[39;00m e.name.startswith(\u001b[33m\"\u001b[39m\u001b[33mdatabricks\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\.venv\\Lib\\site-packages\\mlflow\\pyfunc\\model.py:550\u001b[39m, in \u001b[36m_load_pyfunc\u001b[39m\u001b[34m(model_path, model_config)\u001b[39m\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_pyfunc\u001b[39m(model_path: \u001b[38;5;28mstr\u001b[39m, model_config: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     context, python_model, signature = \u001b[43m_load_context_model_and_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _PythonModelPyfuncWrapper(\n\u001b[32m    552\u001b[39m         python_model=python_model,\n\u001b[32m    553\u001b[39m         context=context,\n\u001b[32m    554\u001b[39m         signature=signature,\n\u001b[32m    555\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\.venv\\Lib\\site-packages\\mlflow\\pyfunc\\model.py:533\u001b[39m, in \u001b[36m_load_context_model_and_signature\u001b[39m\u001b[34m(model_path, model_config)\u001b[39m\n\u001b[32m    531\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\u001b[33m\"\u001b[39m\u001b[33mPython model path was not specified in the model configuration\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os.path.join(model_path, python_model_subpath), \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m         python_model = \u001b[43mcloudpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m artifacts = {}\n\u001b[32m    536\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m saved_artifact_name, saved_artifact_info \u001b[38;5;129;01min\u001b[39;00m pyfunc_config.get(\n\u001b[32m    537\u001b[39m     CONFIG_KEY_ARTIFACTS, {}\n\u001b[32m    538\u001b[39m ).items():\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datalabeling.annotator'"
     ]
    }
   ],
   "source": [
    "# provide correct alias, \"pt\", \"onnx\"\n",
    "alias = \"version18\"\n",
    "project_id = 96 # insert correct project_id by loooking at the url\n",
    "\n",
    "dotenv_path=\"../.env\"\n",
    "\n",
    "name = \"obb-detector\" # detector, \"obb-detector\"\n",
    "\n",
    "handler = Annotator(mlflow_model_alias=alias,\n",
    "                    mlflow_model_name=name,\n",
    "                    confidence_threshold=0.15,\n",
    "                    is_yolo_obb=name.strip() == \"obb-detector\",\n",
    "                    dotenv_path=dotenv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"D:\\PhD\\Data per camp\\Dry season\\Leopard rock\\Camp 1-8\\Rep 1\\DJI_202310021005_001\"\n",
    "save_path = Path(image_dir).parent / \"detection_gps.csv\"\n",
    "\n",
    "image_paths = list(Path(image_dir).glob('*.jpg'))\n",
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = handler.predict_directory(\n",
    "        path_to_dir = None,\n",
    "        images_paths = image_paths,\n",
    "        return_gps = True,\n",
    "        return_coco = False,\n",
    "        as_dataframe = True,\n",
    "        save_path = None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['Latitude', 'Longitude', 'Elevation']].to_csv(save_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing with Openvino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up inference on intel, make changes inn ultralytics/nn/autobackend.py:\n",
    "```\n",
    "- device_name = \"AUTO:NPU,GPU,CPU\" # CPU, GPU, NPU, AUTO,\"AUTO:GPU,NPU\"\n",
    "- inference_mode = \"LATENCY\" # OpenVINO inference modes are 'LATENCY', 'THROUGHPUT' (not recommended), or 'CUMULATIVE_THROUGHPUT'\n",
    "- LOGGER.info(f\"Using OpenVINO {inference_mode} mode for inference...\")\n",
    "- ov_compiled_model = core.compile_model(\n",
    "                ov_model,\n",
    "                device_name=device_name,  # AUTO selects best available device, do not modify\n",
    "                config={\"PERFORMANCE_HINT\": inference_mode,\n",
    "                        \"CACHE_DIR\": os.environ[\"OPENVINO_CACHE_MODEL\"]}, # make sure to set environment variable\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = r\"D:\\savmap_dataset_v2\\images_splits\\00a033fefe644429a1e0fcffe88f8b39_1.JPG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define detector\n",
    "# to speed up inference on intel, make\n",
    "model = Detector(path_to_weights=r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best_openvino_model\",\n",
    "                confidence_threshold=0.1,\n",
    "                overlap_ratio=0.1,\n",
    "                tilesize=1280,\n",
    "                device='CPU',\n",
    "                use_sliding_window=False,\n",
    "                is_yolo_obb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(IMAGE_PATH)\n",
    "\n",
    "while True:\n",
    "    start_time = time.perf_counter()\n",
    "    print(model.predict(image,return_coco=True,nms_iou=0.5))\n",
    "    end_time = time.perf_counter()\n",
    "    print(f\"Device took {end_time-start_time:.2f} seconds.\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with openvino\n",
    "import openvino as ov\n",
    "import openvino.properties.hint as hints\n",
    "from ultralytics.utils import DEFAULT_CFG\n",
    "from ultralytics.cfg import get_cfg\n",
    "from ultralytics.data.converter import coco80_to_coco91_class\n",
    "\n",
    "# load validator\n",
    "args = get_cfg(cfg=DEFAULT_CFG)\n",
    "det_model = YOLO(r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best.pt\")\n",
    "det_validator = det_model.task_map[det_model.task][\"validator\"](args=args)\n",
    "det_validator.is_coco = True\n",
    "det_validator.class_map = coco80_to_coco91_class()\n",
    "det_validator.names = det_model.model.names\n",
    "det_validator.metrics.names = det_validator.names\n",
    "det_validator.nc = det_model.model.model[-1].nc\n",
    "det_validator.stride = 32\n",
    "args = get_cfg(cfg=DEFAULT_CFG)\n",
    "det_model = YOLO(r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best.pt\")\n",
    "\n",
    "core = ov.Core()\n",
    "det_model_path = r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best_openvino_model\\best.xml\"\n",
    "det_ov_model = core.read_model(det_model_path)\n",
    "\n",
    "device = \"AUTO:NPU,GPU\" # CPU, NPU, GPU \"AUTO:NPU,GPU,CPU\" \n",
    "\n",
    "print(\"Available core devices: \",core.available_devices)\n",
    "\n",
    "# reshaping for batch prediction\n",
    "input_layer = det_ov_model.input(0)\n",
    "output_layer = det_ov_model.output(0)\n",
    "new_shape = ov.PartialShape([1, 3, 1280, 1280])\n",
    "det_ov_model.reshape({input_layer.any_name: new_shape})\n",
    "\n",
    "ov_config = {hints.performance_mode: hints.PerformanceMode.THROUGHPUT,\n",
    "             \"CACHE_DIR\": '../models/model_cache'}\n",
    "\n",
    "if (\"GPU\" in core.available_devices) and device==\"GPU\":\n",
    "    ov_config[\"GPU_DISABLE_WINOGRAD_CONVOLUTION\"] = \"YES\"\n",
    "det_compiled_model = core.compile_model(det_ov_model, device, ov_config)\n",
    "\n",
    "def infer(image):\n",
    "    image = det_validator.preprocess({\"img\":image,\"batch_idx\":torch.Tensor([0]),\n",
    "                                      \"cls\":torch.Tensor([0]),\n",
    "                                      \"bboxes\":torch.Tensor([0.,0.,0.,0.])})[\"img\"]\n",
    "    results = det_compiled_model(image)\n",
    "    preds = torch.from_numpy(results[det_compiled_model.output(0)])\n",
    "    return det_validator.postprocess(preds) #torch.from_numpy(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(IMAGE_PATH)\n",
    "# image = F.PILToTensor()(image)[None,:,:1280,:1280]\n",
    "# infer(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with pt\n",
    "# model = YOLO(r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best.pt\",task='obb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling input images\n",
    "# model(image/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with openvino\n",
    "# model_vino = YOLO(r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best_openvino_model\",task='obb')\n",
    "# model_vino(image/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sahi_model_obb = Detector(path_to_weights=r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best_openvino_model\",\n",
    "#                     confidence_threshold=0.6,\n",
    "#                     overlap_ratio=0.1,\n",
    "#                     tilesize=640,\n",
    "#                     is_yolo_obb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = r\"D:\\savmap_dataset_v2\\images\\0d1ba3c424ad4414ac37dbd0c93460ea.JPG\"\n",
    "# image = Image.open(image_path)\n",
    "# print(image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = sahi_model_obb.predict(image,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "# result.export_visuals('../.tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sahi inference calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "overlap_ratios = [0.1,0.2,0.3]\n",
    "tilesizes = [640,2*640,3*640]\n",
    "imgsz = [640,2*640,3*640]\n",
    "\n",
    "for ratio, tilesize, image_size in product(overlap_ratios,tilesizes,imgsz):\n",
    "    print(ratio,tilesize,image_size)\n",
    "    # Define detector\n",
    "    # to speed up inference on intel, make\n",
    "    model = Detector(path_to_weights=r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best_openvino_model\",\n",
    "                    confidence_threshold=0.1,\n",
    "                    overlap_ratio=0.1,\n",
    "                    tilesize=2000,\n",
    "                    imgsz=1280,\n",
    "                    device='CPU',\n",
    "                    use_sliding_window=True,\n",
    "                    is_yolo_obb=True)\n",
    "    \n",
    "    #TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO data_config.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from datalabeling.arguments import Arguments\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yaml\n",
    "with open(r\"D:\\PhD\\Data per camp\\DetectionDataset\\hard_samples\\train_ratio_20-seed_41.yaml\",'r') as file:\n",
    "    yolo_config = yaml.load(file,Loader=yaml.FullLoader)\n",
    "yolo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(os.path.join(yolo_config[\"path\"],yolo_config['train']),header=None,names=['paths'])['paths'].to_list()[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load label mapping\n",
    "args = Arguments()\n",
    "with open(r\"D:\\PhD\\Data per camp\\IdentificationDataset\\label_mapping.json\",'r') as file:\n",
    "    label_map = json.load(file)\n",
    "names = [p['name'] for p in label_map if p['name'] not in args.discard_labels ]\n",
    "label_map = dict(zip(range(len(names)),names))\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_studio_sdk import Client\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "def get_ls_parsed_config(ls_json_path:str):\n",
    "\n",
    "    ls_client = None\n",
    "    if ls_client is None:\n",
    "        # Connect to the Label Studio API and check the connection\n",
    "        LABEL_STUDIO_URL = os.getenv('LABEL_STUDIO_URL')\n",
    "        API_KEY = os.getenv(\"LABEL_STUDIO_API_KEY\")\n",
    "        labelstudio_client = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
    "\n",
    "    with open(ls_json_path,'r') as f:\n",
    "        ls_annotation = json.load(fp=f)\n",
    "    ids = set([annot['project'] for annot in ls_annotation])\n",
    "    assert len(ids)==1, \"annotations come from different project. Not allowed!\"\n",
    "    project_id = ids.pop()\n",
    "    project = labelstudio_client.get_project(id=project_id)\n",
    "\n",
    "    return project.parsed_label_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_config.update({'names':label_map,'nc':len(label_map)})\n",
    "yolo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\PhD\\Data per camp\\IdentificationDataset\\data_config.yaml\",'w') as file:\n",
    "    yaml.dump(yolo_config,file,default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize distribution per annotation project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datalabeling.dataset import load_coco_annotations\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "# from itertools import chain\n",
    "import traceback\n",
    "import os\n",
    "from pathlib import  Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path=r\"..\\.env\"\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "# Connect to the Label Studio API and check the connection\n",
    "LABEL_STUDIO_URL = os.getenv('LABEL_STUDIO_URL')\n",
    "API_KEY = os.getenv(\"LABEL_STUDIO_API_KEY\")\n",
    "labelstudio_client = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_project_stats(project_id:int,annotator_id = 0):\n",
    "    \n",
    "    project = labelstudio_client.get_project(id=project_id)\n",
    "    num_images = dict()\n",
    "    # Iterating \n",
    "    tasks = project.get_tasks()\n",
    "     # because there is\n",
    "    labels = []\n",
    "\n",
    "    for task in tasks:\n",
    "        try:\n",
    "            result = task['annotations'][annotator_id]['result']\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "        img_labels = []\n",
    "        for annot in result:\n",
    "            img_labels = annot['value']['rectanglelabels'] + img_labels\n",
    "        labels = labels + img_labels\n",
    "        # update stats holder\n",
    "        for label in set(img_labels):\n",
    "            try:\n",
    "                num_images[label] += 1\n",
    "            except:\n",
    "                num_images[label] = 1\n",
    "\n",
    "    stats = {f\"{k}\":labels.count(k) for k in set(labels)}\n",
    "    print(\"Number of instances for each label is:\\n\",stats,end=\"\\n\\n\")\n",
    "    print(\"Number of images for each label is:\\n\",num_images)\n",
    "\n",
    "    return stats, num_images\n",
    "\n",
    "# get stats\n",
    "for project_id in [93,]:\n",
    "    get_project_stats(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_dir = r\"D:\\savmap_dataset_v2\\annotated_py_paul\\coco-format\"\n",
    "dest_dir = Path(ls_dir).with_name(\"coco-format\")\n",
    "save_excel_path = Path(ls_dir).with_name(\"stats.xlsx\")\n",
    "\n",
    "# Uncomment to run if needed\n",
    "# convert_json_annotations_to_coco(input_dir=ls_dir,\n",
    "#                                  dest_dir_coco=str(dest_dir),\n",
    "#                                  ls_client=labelstudio_client,\n",
    "#                                  parse_ls_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_annotations_dict = load_coco_annotations(dest_dir)\n",
    "coco_annotations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_count(coco_annotation:dict):\n",
    "\n",
    "    result = Counter([annot['category_id'] for annot in coco_annotation['annotations']])\n",
    "\n",
    "    label_map = {cat['id']:cat['name'] for cat in coco_annotation['categories']}\n",
    "\n",
    "    result = {label_map[k]:v for k,v in result.items()}\n",
    "\n",
    "    return result\n",
    "\n",
    "label_stats = dict()\n",
    "\n",
    "for img_dir,coco_path in coco_annotations_dict.items():\n",
    "\n",
    "    with open(coco_path,'r') as f:\n",
    "        coco_annotation = json.load(fp=f)\n",
    "    \n",
    "    label_stats[img_dir] = get_labels_count(coco_annotation)\n",
    "\n",
    "label_stats = pd.DataFrame.from_dict(label_stats,orient='index').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_stats = label_stats.reset_index().rename(columns={'index':'file_name'})\n",
    "label_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_stats['file_name_short'] = label_stats['file_name'].apply(lambda x: Path(x).parent.name + '/' + Path(x).name)\n",
    "# label_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to save\n",
    "label_stats.to_excel(save_excel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize splits' distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*20, \"Detection\",'-'*20)\n",
    "\n",
    "path = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\configs\\yolo_configs\\dataset_identification-detection.yaml\"\n",
    "\n",
    "for split in  ('train','val','test'):   \n",
    "\n",
    "    # load yaml\n",
    "    with open(path,'r') as file:\n",
    "        yolo_config = yaml.load(file,Loader=yaml.FullLoader)\n",
    "\n",
    "    label_map = yolo_config['names']\n",
    "\n",
    "    path_dataset = os.path.join(yolo_config['path'],\n",
    "                                yolo_config[split][0]\n",
    "                                )\n",
    "\n",
    "    total_number_images = len(list(Path(path_dataset).glob('*')))\n",
    "\n",
    "    path_dataset = path_dataset.replace('images','labels')\n",
    "\n",
    "    total_number_of_positive_images = len(list(Path(path_dataset).glob('*')))\n",
    "\n",
    "    print(\"\\n\\nSplit:\", split)\n",
    "    print(\"Number of empty images: \", total_number_images-total_number_of_positive_images)\n",
    "    print(\"Number of non-empty images: \", total_number_of_positive_images)\n",
    "\n",
    "    labels = list()\n",
    "\n",
    "    for txtfile in Path(path_dataset).glob(\"*.txt\"):\n",
    "\n",
    "        df = pd.read_csv(txtfile,sep=\" \",header=None )\n",
    "        df['class'] = df.iloc[:,0].astype(int)    \n",
    "        df['image'] = txtfile.stem\n",
    "        labels.append(df)\n",
    "\n",
    "    df = pd.concat(labels,axis=0)\n",
    "    df['class'] = df['class'].map(label_map)\n",
    "\n",
    "    images_per_class = dict()\n",
    "    for cls in df['class'].unique():\n",
    "        num_imge = df.loc[df['class'] == cls,'image'].unique().shape[0]\n",
    "        images_per_class[cls] = num_imge\n",
    "\n",
    "    \n",
    "    print('images per class: ',images_per_class)\n",
    "    print('instances per class: ', df['class'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*20, \"Identification\",'-'*20)\n",
    "path = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\configs\\yolo_configs\\dataset_identification.yaml\"\n",
    "\n",
    "for split in  ('train','val','test'):   \n",
    "\n",
    "    # load yaml\n",
    "    with open(path,'r') as file:\n",
    "        yolo_config = yaml.load(file,Loader=yaml.FullLoader)\n",
    "\n",
    "    label_map = yolo_config['names']\n",
    "\n",
    "    path_dataset = os.path.join(yolo_config['path'],\n",
    "                                yolo_config[split][0]\n",
    "                                )\n",
    "\n",
    "    total_number_images = len(list(Path(path_dataset).glob('*')))\n",
    "\n",
    "    path_dataset = path_dataset.replace('images','labels')\n",
    "\n",
    "    total_number_of_positive_images = len(list(Path(path_dataset).glob('*')))\n",
    "\n",
    "    print(\"\\n\\nSplit:\", split)\n",
    "    print(\"Number of empty images: \", total_number_images-total_number_of_positive_images)\n",
    "    print(\"Number of non-empty images: \", total_number_of_positive_images)\n",
    "\n",
    "    labels = list()\n",
    "\n",
    "    for txtfile in Path(path_dataset).glob(\"*.txt\"):\n",
    "\n",
    "        df = pd.read_csv(txtfile,sep=\" \",header=None )\n",
    "        df['class'] = df.iloc[:,0].astype(int)    \n",
    "        df['image'] = txtfile.stem\n",
    "        labels.append(df)\n",
    "\n",
    "    df = pd.concat(labels,axis=0)\n",
    "    df['class'] = df['class'].map(label_map)\n",
    "\n",
    "    images_per_class = dict()\n",
    "    for cls in df['class'].unique():\n",
    "        num_imge = df.loc[df['class'] == cls,'image'].unique().shape[0]\n",
    "        images_per_class[cls] = num_imge\n",
    "\n",
    "    \n",
    "    print('images per class: ',images_per_class)\n",
    "    print('instances per class: ', df['class'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts().plot(kind='bar',figsize=(10,5),logy=True,title=f\"{split} label distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing metrics on Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO or ultralytics models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "# from pathlib import Path\n",
    "from datalabeling.train import remove_label_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [IMPORTANT] Run this cell to convert the datasets to yolo format\n",
    "!python  ../tools/build_dataset.py --obb-to-yolo --data-config-yaml \"..\\data\\dataset_identification.yaml\" --skip\n",
    "!python  ../tools/build_dataset.py --obb-to-yolo --data-config-yaml \"..\\data\\dataset_identification-detection.yaml\" --skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting results for yolov12s : Detection and Identification\n",
    "paths = [\"...\", # Identification model weights\n",
    "         \"../runs/mlflow/140168774036374062/a59eda79d9444ff4befc561ac21da6b4/artifacts/weights/best.pt\" # Detection model weights\n",
    "        ]\n",
    "\n",
    "dataconfigs = [\n",
    "                # r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification.yaml\",\n",
    "               r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification-detection.yaml\"\n",
    "              ]\n",
    "\n",
    "imgsz = 800\n",
    "iou_threshold=0.45\n",
    "conf_threshold=0.235\n",
    "splits = [\n",
    "            # \"val\", \n",
    "          \"test\",\n",
    "          ]\n",
    "\n",
    "# remove label.cache files\n",
    "for dataconfig in dataconfigs:\n",
    "    remove_label_cache(data_config_yaml=dataconfig)\n",
    "\n",
    "for split in splits:\n",
    "    for path,dataconfig in zip(paths,dataconfigs):\n",
    "        print(\"\\n\",'-'*20,split,'-'*20)\n",
    "        model = YOLO(path)\n",
    "        model.info()\n",
    "        \n",
    "        # Customize validation settings\n",
    "        validation_results = model.val(data=dataconfig,\n",
    "                                        imgsz=imgsz,\n",
    "                                        batch=64,\n",
    "                                        split=split,\n",
    "                                        conf=conf_threshold,\n",
    "                                        iou=iou_threshold,\n",
    "                                        device=\"cuda\"\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting results for yolov5s : Detection and Identification\n",
    "paths = [\"../runs/mlflow/140168774036374062/87718ce84ce04dacac6ab8c92328eae7/artifacts/weights/best.pt\", # Identification model weights\n",
    "         \"../runs/mlflow/140168774036374062/e5e3bf93d34f48f1bb7d0a648530bb45/artifacts/weights/best.pt\" # Detection model weights\n",
    "        ]\n",
    "\n",
    "dataconfigs = [r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification.yaml\",\n",
    "               r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification-detection.yaml\"\n",
    "              ]\n",
    "\n",
    "imgsz = 800\n",
    "iou_threshold=0.45\n",
    "conf_threshold=0.235\n",
    "splits = [\n",
    "            # \"val\", \n",
    "          \"test\",\n",
    "          ]\n",
    "\n",
    "# remove label.cache files\n",
    "for dataconfig in dataconfigs:\n",
    "    remove_label_cache(data_config_yaml=dataconfig)\n",
    "\n",
    "for split in splits:\n",
    "    for path,dataconfig in zip(paths,dataconfigs):\n",
    "        print(\"\\n\",'-'*20,split,'-'*20)\n",
    "        model = YOLO(path)\n",
    "        model.info()\n",
    "        \n",
    "        # Customize validation settings\n",
    "        validation_results = model.val(data=dataconfig,\n",
    "                                        imgsz=imgsz,\n",
    "                                        batch=64,\n",
    "                                        split=split,\n",
    "                                        conf=conf_threshold,\n",
    "                                        iou=iou_threshold,\n",
    "                                        device=\"cuda\"\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [IMPORTANT] Run this cell to convert the datasets to obb format\n",
    "!python  ../tools/build_dataset.py --yolo-to-obb --data-config-yaml \"..\\data\\dataset_identification.yaml\" --skip\n",
    "!python  ../tools/build_dataset.py --yolo-to-obb --data-config-yaml \"..\\data\\dataset_identification-detection.yaml\" --skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load yolov11s-obb\n",
    "paths = [\"../runs/mlflow/140168774036374062/34c709364c0e46dcb72c526de34a7fa4/artifacts/weights/best.pt\", # Identification\n",
    "         \"../runs/mlflow/140168774036374062/f5b7124be14c4c89b8edd26bcf7a9a76/artifacts/weights/best.pt\", # Detection\n",
    "        ]\n",
    "\n",
    "\n",
    "dataconfigs = [r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification.yaml\",\n",
    "               r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification-detection.yaml\"\n",
    "              ]\n",
    "\n",
    "imgsz = 800\n",
    "iou_threshold=0.45\n",
    "conf_threshold=0.235\n",
    "splits = [\n",
    "        #   \"val\", \n",
    "          \"test\",\n",
    "          ]\n",
    "\n",
    "# remove label.cache files\n",
    "for dataconfig in dataconfigs:\n",
    "    remove_label_cache(data_config_yaml=dataconfig)\n",
    "\n",
    "for split in splits:\n",
    "    for path,dataconfig in zip(paths,dataconfigs):\n",
    "        print(\"\\n\",'-'*20,split,'-'*20)\n",
    "        model = YOLO(path)\n",
    "        model.info()\n",
    "        \n",
    "        # Customize validation settings\n",
    "        validation_results = model.val(data=dataconfig,\n",
    "                                        imgsz=imgsz,\n",
    "                                        batch=64,\n",
    "                                        split=split,\n",
    "                                        conf=conf_threshold,\n",
    "                                        iou=iou_threshold,\n",
    "                                        device=\"cuda\"\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load yolov8s-obb\n",
    "paths = [\n",
    "         \"../runs/mlflow/140168774036374062/b883bd2b31f94f29807ea3b94e8ff8fc/artifacts/weights/best.pt\", # Identification\n",
    "         \"../runs/mlflow/140168774036374062/8a76c60253fc48788b5324096d035420/artifacts/weights/best.pt\"  # Detection\n",
    "        ]\n",
    "\n",
    "\n",
    "dataconfigs = [\n",
    "                r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification.yaml\",\n",
    "               r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification-detection.yaml\"\n",
    "              ]\n",
    "\n",
    "imgsz = 800\n",
    "iou_threshold=0.45\n",
    "conf_threshold=0.235\n",
    "splits = [\n",
    "        #   \"val\", \n",
    "          \"test\",\n",
    "          ]\n",
    "\n",
    "# remove label.cache files\n",
    "for dataconfig in dataconfigs:\n",
    "    remove_label_cache(data_config_yaml=dataconfig)\n",
    "\n",
    "for split in splits:\n",
    "    for path,dataconfig in zip(paths,dataconfigs):\n",
    "        print(\"\\n\",'-'*20,split,'-'*20)\n",
    "        model = YOLO(path)\n",
    "        model.info()\n",
    "        \n",
    "        # Customize validation settings\n",
    "        validation_results = model.val(data=dataconfig,\n",
    "                                        imgsz=imgsz,\n",
    "                                        batch=64,\n",
    "                                        split=split,\n",
    "                                        conf=conf_threshold,\n",
    "                                        iou=iou_threshold,\n",
    "                                        device=\"cuda\"\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "from datalabeling.annotator import Annotator\n",
    "\n",
    "for alias in [\"version9\", \"version6\"]:\n",
    "    print(\"-\"*10,alias,end=\"\\n\\n\")\n",
    "    name = \"obb-detector\" # detector, \"obb-detector\"\n",
    "    handler = Annotator(mlflow_model_alias=alias,\n",
    "                            mlflow_model_name=name,\n",
    "                            confidence_threshold=0.25,\n",
    "                            is_yolo_obb=name.strip() == \"obb-detector\",\n",
    "                            dotenv_path=\"../.env\")\n",
    "\n",
    "    yolo_model = handler.model.unwrap_python_model().detection_model.detection_model.model\n",
    "    validation_results = yolo_model.val(data=r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_hn.yaml\",\n",
    "                                    imgsz=1280,\n",
    "                                    batch=32,\n",
    "                                    conf=0.25,\n",
    "                                    iou=0.45,\n",
    "                                    device=\"cuda\"\n",
    "                                )\n",
    "    \n",
    "    print(validation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Herdnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datalabeling.train.herdnet import HerdnetData, HerdnetTrainer\n",
    "from datalabeling.arguments import Arguments\n",
    "import lightning as L\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowering matrix multiplication precision\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "args = Arguments()\n",
    "args.lr0 = 3e-4\n",
    "args.epochs = 15\n",
    "args.imgsz = 800\n",
    "args.batchsize = 8\n",
    "down_ratio = 2\n",
    "args.data_config_yaml = r\"D:\\datalabeling\\data\\data_config.yaml\"\n",
    "args.path_weights = r\"D:\\datalabeling\\models\\20220329_HerdNet_Ennedi_dataset_2023.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: load Prediction images\n",
    "with open(args.data_config_yaml, 'r') as file:\n",
    "    data_config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "images_path = os.path.join(data_config['path'],data_config['test'][0])\n",
    "images_path = list(Path(images_path).glob('*'))\n",
    "\n",
    "\n",
    "# Data\n",
    "datamodule = HerdnetData(data_config_yaml=args.data_config_yaml,\n",
    "                            patch_size=args.imgsz,\n",
    "                            batch_size=args.batchsize,\n",
    "                            down_ratio=down_ratio,\n",
    "                            train_empty_ratio=0.,\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,targets = datamodule.val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm(datamodule.val_dataloader()):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    batched = dict(img=torch.stack([p[0] for p in batch])\n",
    "                )\n",
    "    targets = [p[1] for p in batch]\n",
    "    keys = targets[0].keys()\n",
    "    \n",
    "    for k in keys:\n",
    "        if k == 'points':\n",
    "            batched[k] = torch.vstack([a[k] for a in targets])\n",
    "        elif k == 'labels':\n",
    "            batched[k] = torch.hstack([a[k] for a in targets])\n",
    "        elif (k == 'w') or (k == 'h'):\n",
    "            batched[k] = torch.hstack([torch.Tensor(a[k]) for a in targets])\n",
    "        else:\n",
    "            batched[k] = [a[k] for a in targets]    \n",
    "\n",
    "    return batched\n",
    "\n",
    "loader = DataLoader(datamodule.val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "batch = next(iter(loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in loader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{str(k):v for k,v in data_config['names'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a few\n",
    "# random.shuffle(images_path)\n",
    "images_path = images_path[:10]\n",
    "datamodule.set_predict_dataset(images_path=images_path,batchsize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "for img in datamodule.predict_dataloader():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "checkpoint_path = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\tools\\lightning-ckpts\\epoch=23-step=2040.ckpt\"\n",
    "herdnet_trainer = HerdnetTrainer.load_from_checkpoint(checkpoint_path=checkpoint_path,\n",
    "                                                            args=args,\n",
    "                                                            ce_weight=None,\n",
    "                                                            work_dir='../.tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(accelerator=\"auto\",profiler='simple')\n",
    "out = trainer.predict(model=herdnet_trainer,\n",
    "                datamodule=datamodule,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(out,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with SAM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import SAM\n",
    "from ultralytics.data.dataset import YOLODataset, YOLOConcatDataset\n",
    "from datalabeling.train.herdnet import HerdnetData, HerdnetTrainer\n",
    "from datalabeling.arguments import Arguments\n",
    "import lightning as L\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from ultralytics.data.utils import visualize_image_annotations\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.compile(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments()\n",
    "args.lr0 = 3e-4\n",
    "args.epochs = 15\n",
    "args.imgsz = 800 # Attention, it will resize the image and the targets\n",
    "args.batchsize = 8\n",
    "down_ratio = 1 # Attention, it will down sample the targets. i.e. x/down_ratio,y/down_ratio\n",
    "args.data_config_yaml = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification-detection.yaml\"\n",
    "args.path_weights = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\base_models_weights\\20220329_HerdNet_Ennedi_dataset_2023.pth\"\n",
    "# args.data_config_yaml = r\"D:\\datalabeling\\data\\data_config.yaml\"\n",
    "\n",
    "# Example: load Prediction images\n",
    "with open(args.data_config_yaml, 'r') as file:\n",
    "    data_config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "images_path = os.path.join(data_config['path'],data_config['test'][0])\n",
    "images_path = list(Path(images_path).glob('*'))\n",
    "\n",
    "\n",
    "# Data\n",
    "# datamodule = HerdnetData(data_config_yaml=args.data_config_yaml,\n",
    "#                             patch_size=args.imgsz,\n",
    "#                             batch_size=args.batchsize,\n",
    "#                             down_ratio=down_ratio,\n",
    "#                             train_empty_ratio=0.,\n",
    "#                             normalization='min_max' # 'standard', 'min_max'\n",
    "#                             )\n",
    "\n",
    "# datamodule.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "datasets = list()\n",
    "split='val'\n",
    "for path in data_config[split]:\n",
    "    images_path = os.path.join(data_config['path'], path)\n",
    "    dataset = YOLODataset(img_path=images_path,task='detect',data={'names':data_config['names']},augment=False,imgsz=args.imgsz,classes=None)\n",
    "    datasets.append(dataset)\n",
    "dataset = YOLOConcatDataset(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    if data[\"cls\"].nelement() == 0:\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points are (x,y)\n",
    "data['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['im_file'], data_config['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = data['im_file']\n",
    "label_path = Path(str(img_path).replace(\"images\",\"labels\")).with_suffix('.txt')\n",
    "label_map = data_config['names']\n",
    "visualize_image_annotations(data['im_file'],label_path,{0: 'wildlife'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model_sam = SAM(r\"..\\base_models_weights\\sam2.1_l.pt\")\n",
    "model_sam.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model, does not work well\n",
    "# model_fastsam = FastSAM(\"FastSAM-x.pt\")\n",
    "# model_fastsam.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastSAM\n",
    "# bboxes = bboxes = torch.cat([data['bboxes'][:,:2], data['bboxes'][:,:2] + data['bboxes'][:,2:]],1) #data['bboxes']\n",
    "# bboxes = (bboxes*640).long().tolist()\n",
    "# print(bboxes)\n",
    "# results_fastsam, = model_fastsam(img_path, device=\"cpu\", bboxes=bboxes[0], imgsz=640, conf=0.4, iou=0.9)\n",
    "# results_fastsam.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.ultralytics.com/models/sam-2/#sam-2-prediction-examples\n",
    "- *Run inference with bboxes prompt* (xmin,ymin,xmax,ymax)\n",
    "    ``results = model(\"path/to/image.jpg\", bboxes=[100, 100, 200, 200]) ``\n",
    "\n",
    "- *Run inference with single point*\n",
    "    ``results = model(points=[900, 370], labels=[1])``\n",
    "\n",
    "- *Run inference with multiple points*\n",
    "    ``results = model(points=[[400, 370], [900, 370]], labels=[1, 1])``\n",
    "\n",
    "- *Run inference with multiple points prompt per object*\n",
    "    ``results = model(points=[[[400, 370], [900, 370]]], labels=[[1, 1]])``\n",
    "\n",
    "- *Run inference with negative points prompt*\n",
    "    ``results = model(points=[[[400, 370], [900, 370]]], labels=[[1, 0]])``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with bboxes prompt\n",
    "bboxes = torch.cat([data['bboxes'][:,:2], data['bboxes'][:,:2] + data['bboxes'][:,2:]],1)\n",
    "bboxes = (bboxes*args.imgsz).long().tolist()\n",
    "labels = data[\"cls\"].long().ravel()+1 # account for background class\n",
    "print(bboxes, labels)\n",
    "results_sam, = model_sam(data['im_file'],\n",
    "                bboxes=bboxes,\n",
    "                labels=labels.tolist()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.view(-1,1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sam.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sam.masks.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = results_sam.masks.data.cpu() * labels.view(-1,1,1)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[0].min(), mask[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert masks into yolo segmentation format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datalabeling.dataset import create_yolo_seg_directory\n",
    "from datalabeling.dataset.converters import convert_segment_masks_to_yolo_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: load Prediction images\n",
    "with open(args.data_config_yaml, 'r') as file:\n",
    "    data_config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_file = \"./example.txt\"\n",
    "mask = mask.cpu().numpy()\n",
    "convert_segment_masks_to_yolo_seg(masks_sam2=mask, output_path=tmp_file, num_classes=data_config['nc'],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_yolo_seg_directory(\n",
    "            data_config_yaml=args.data_config_yaml,\n",
    "            imgsz=args.imgsz,\n",
    "            model_sam=model_sam,\n",
    "            device=args.device,\n",
    "            copy_images_dir=False, # set to True to copy images into segmentation directory\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing inference params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datalabeling.annotator import Detector\n",
    "from datalabeling.arguments import Arguments\n",
    "from datalabeling.dataset.sampling import (get_preds_targets, compute_detector_performance)    \n",
    "import yaml\n",
    "import os\n",
    "from hyperopt import tpe, hp, fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params \n",
    "args = Arguments()\n",
    "args.path_to_weights = r\"C:/Users/Machine Learning/Desktop/workspace-wildAI/datalabeling/runs/mlflow/140168774036374062/57daf3bcd99b4dd4b040cb4f8670960c/artifacts/weights/best.pt\"\n",
    "# args.confidence_threshold = 0.2\n",
    "# args.overlap_ratio = 0.1\n",
    "args.use_sliding_window = True\n",
    "args.device = \"cuda\"\n",
    "args.is_yolo_obb = True\n",
    "args.pred_results_dir = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\.tmp\"\n",
    "args.data_config_yaml = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_hn.yaml\"\n",
    "args.hn_uncertainty_method = \"entropy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load groundtruth\n",
    "with open(args.data_config_yaml,'r') as file:\n",
    "    yolo_config = yaml.load(file,Loader=yaml.FullLoader)\n",
    "\n",
    "split='val'\n",
    "images_path = [os.path.join(yolo_config['path'],yolo_config[split][i]) for i in range(len(yolo_config[split]))]\n",
    "images_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params:dict):\n",
    "\n",
    "    # Define detector\n",
    "    model = Detector(path_to_weights=args.path_to_weights,\n",
    "                        confidence_threshold=params['confidence_threshold'],\n",
    "                        overlap_ratio=params['overlap_ratio'],\n",
    "                        tilesize=params['tilesize'],\n",
    "                        imgsz=params['imgsz'],\n",
    "                        use_sliding_window=args.use_sliding_window,\n",
    "                        device=args.device,\n",
    "                        is_yolo_obb=args.is_yolo_obb\n",
    "                    )\n",
    "\n",
    "    df_results, df_labels, col_names = get_preds_targets(images_dirs=images_path,\n",
    "                                                        pred_results_dir=args.pred_results_dir,\n",
    "                                                        detector=model,\n",
    "                                                        load_results=False,\n",
    "                                                        save_tag=f\"{params['imgsz']}-{params['tilesize']}-{params['overlap_ratio']}-{params['confidence_threshold']}\"\n",
    "                                                        )\n",
    "\n",
    "    df_results_per_img = compute_detector_performance(df_results,df_labels,col_names)\n",
    "    # df_results_per_img = get_uncertainty(df_results_per_img=df_results_per_img,mode=args.hn_uncertainty_method)\n",
    "\n",
    "    # minizing loss -> maximize map50 and map75\n",
    "    loss = -1.0*df_results_per_img[\"map50\"].mean() - df_results_per_img[\"map75\"].mean() #+ df_results_per_img[\"uncertainty\"].mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "                'confidence_threshold': hp.uniform('x', 0.1, 0.7),\n",
    "                'overlap_ratio': hp.uniform('y', 0, 0.25),\n",
    "                'tilesize': hp.choice(label='tilesize',options=[640, 2*640]),\n",
    "                'imgsz': hp.choice(label='imgsz',options=[640, 2*640, 3*640, 4*640]),\n",
    "            }\n",
    "\n",
    "best = fmin(\n",
    "    fn=objective, # Objective Function to optimize\n",
    "    space=search_space, # Hyperparameter's Search Space\n",
    "    algo=tpe.suggest, # Optimization algorithm (representative TPE)\n",
    "    max_evals=2 # Number of optimization attempts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset label format conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_label_format(loaded_df:pd.DataFrame)->str:\n",
    "    \"\"\"checks label format\n",
    "\n",
    "    Args:\n",
    "        loaded_df (pd.DataFrame): target values\n",
    "\n",
    "    Raises:\n",
    "        NotImplementedError: when the format is not yolo or yolo-obb\n",
    "\n",
    "    Returns:\n",
    "        str: yolo or yolo-obb\n",
    "    \"\"\"\n",
    "\n",
    "    num_features = len(loaded_df.columns)\n",
    "\n",
    "    if num_features == 5:\n",
    "        return \"yolo\"\n",
    "    elif num_features == 9:\n",
    "        return \"yolo-obb\"\n",
    "    else:\n",
    "        raise NotImplementedError(f\"The number of features ({num_features}) in the label file is wrong. Check yolo or yolo-obb format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = r\"D:\\PhD\\Data per camp\\DetectionDataset\\Rep 1\\train\\labels\\DJI_20231002150401_0009_0_48_0_1271_640_1911.txt\"\n",
    "df = pd.read_csv(label_path,sep=' ',header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(df.iloc[:,0].dtype, np.dtypes.IntDType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_label_format(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['id','x1','y1','x2','y2','x3','y3','x4','y4']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute test metrics for ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.models.yolo.detect import DetectionValidator\n",
    "from pathlib import Path\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from datalabeling.train.utils import remove_label_cache\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.metrics import ConfusionMatrix, DetMetrics, box_iou\n",
    "from ultralytics.data import converter\n",
    "\n",
    "class CustomValidator(DetectionValidator):\n",
    "    \"\"\"From https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/detect/val.py\n",
    "    Adapted to compute confusion matrix for a given iou threshold\n",
    "    \"\"\"\n",
    "\n",
    "    def init_metrics(self, model):\n",
    "        \"\"\"\n",
    "        Initialize evaluation metrics for YOLO detection validation.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): Model to validate.\n",
    "        \"\"\"\n",
    "        val = self.data.get(self.args.split, \"\")  # validation path\n",
    "        self.is_coco = (\n",
    "            isinstance(val, str)\n",
    "            and \"coco\" in val\n",
    "            and (val.endswith(f\"{os.sep}val2017.txt\") or val.endswith(f\"{os.sep}test-dev2017.txt\"))\n",
    "        )  # is COCO\n",
    "        self.is_lvis = isinstance(val, str) and \"lvis\" in val and not self.is_coco  # is LVIS\n",
    "        self.class_map = converter.coco80_to_coco91_class() if self.is_coco else list(range(1, len(model.names) + 1))\n",
    "        self.args.save_json |= self.args.val and (self.is_coco or self.is_lvis) and not self.training  # run final val\n",
    "        self.names = model.names\n",
    "        self.nc = len(model.names)\n",
    "        self.end2end = getattr(model, \"end2end\", False)\n",
    "        self.metrics.names = self.names\n",
    "        self.metrics.plot = self.args.plots\n",
    "        self.confusion_matrix = ConfusionMatrix(nc=self.nc, conf=self.args.conf,iou_thres=self.args.iou, task='detect')\n",
    "        self.seen = 0\n",
    "        self.jdict = []\n",
    "        self.stats = dict(tp=[], conf=[], pred_cls=[], target_cls=[], target_img=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example \n",
    "args = dict(model=\"../runs/mlflow/140168774036374062/757205b53b454043a0a6481efb72fddd/artifacts/weights/best.pt\", \n",
    "            data=r\"..\\configs\\yolo_configs\\dataset_identification.yaml\"\n",
    "        )\n",
    "validator = CustomValidator(args=args,save_dir=Path(\"./.tmp/\"))\n",
    "\n",
    "name = \"yolo12-X#Identif#mlflow-757205b53b454043a0a6481efb72fddd\"\n",
    "split= 'val'\n",
    "iou_threshold = 0.6\n",
    "conf_threshold = 0.2\n",
    "max_det = 100\n",
    "augment = False\n",
    "imgsz = 800\n",
    "save_dir = './.tmp_runs_yolo'\n",
    "batchsize=16\n",
    "\n",
    "# set args\n",
    "validator.args.conf = conf_threshold\n",
    "validator.args.iou = iou_threshold\n",
    "validator.args.mode = 'val'\n",
    "validator.args.imgsz = imgsz\n",
    "validator.args.batch = batchsize\n",
    "validator.args.device = 'cuda'\n",
    "validator.args.augment = augment\n",
    "validator.args.split = split\n",
    "validator.args.name = name + \"#\" + split + f\"#{round(conf_threshold*100)}#{round(iou_threshold*100)}#{augment}#{max_det}-\"\n",
    "validator.args.project = save_dir\n",
    "validator.args.max_det = max_det\n",
    "validator.args.plots = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(validator:CustomValidator):\n",
    "\n",
    "    remove_label_cache(data_config_yaml=validator.args.data)\n",
    "\n",
    "    results = validator()\n",
    "    \n",
    "    cf_matrix = validator.confusion_matrix.matrix\n",
    "    labels = list(validator.names.values()) + [\"background\"]\n",
    "\n",
    "    for i,label in enumerate(labels + [\"background\"]):\n",
    "\n",
    "        if label == 'background':\n",
    "            break\n",
    "\n",
    "        tp = cf_matrix[i,i]\n",
    "        actual_positive = cf_matrix[:,i].sum()\n",
    "        predicted_positive  = cf_matrix[i,:].sum()\n",
    "        # fp = predicted_positive - tp\n",
    "        # fn = actual_positive - tp\n",
    "\n",
    "        precision = tp/(predicted_positive + 1e-8)\n",
    "        recall = tp/(actual_positive + 1e-8)\n",
    "        f1score = 2*precision*recall / (precision + recall + 1e-8)\n",
    "\n",
    "        results = dict(precision=round(precision,4), recall=round(recall,4), f1score=round(f1score,4))\n",
    "\n",
    "        print(f\"results for {label} : \", results,end='\\n')\n",
    "\n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(12, 9), tight_layout=True)\n",
    "    # seaborn.set_theme(font_scale=1.0)  # for label size\n",
    "    # ticklabels = labels\n",
    "\n",
    "    # seaborn.heatmap(\n",
    "    #     cf_matrix,\n",
    "    #     ax=ax,\n",
    "    #     annot=True,\n",
    "    #     annot_kws={\"size\": 8},\n",
    "    #     cmap=\"Blues\",\n",
    "    #     fmt=\".0f\",\n",
    "    #     square=True,\n",
    "    #     vmin=0.0,\n",
    "    #     xticklabels=ticklabels,\n",
    "    #     yticklabels=ticklabels,\n",
    "    # ).set_facecolor((1, 1, 1))\n",
    "    # title = \"Confusion Matrix\"\n",
    "    # ax.set_xlabel(\"True\")\n",
    "    # ax.set_ylabel(\"Predicted\")\n",
    "    # ax.set_title(title)\n",
    "\n",
    "    # plot_fname = Path(validator.save_dir) / f\"{title.lower().replace(' ', '_')}.png\"\n",
    "    # fig.savefig(plot_fname, dpi=250)\n",
    "    # plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = validate(validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.confusion_matrix.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validator.confusion_matrix.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf_matrix = validator.confusion_matrix.matrix\n",
    "# labels = list(validator.names.values()) + [\"background\"]\n",
    "\n",
    "# for i,label in enumerate(labels + [\"background\"]):\n",
    "\n",
    "#     if label == 'background':\n",
    "#         break\n",
    "\n",
    "#     tp = cf_matrix[i,i]\n",
    "#     actual_p = cf_matrix[:,i].sum()\n",
    "#     predicted_p  = cf_matrix[i,:].sum()\n",
    "#     fp = predicted_p - tp\n",
    "#     fn = actual_p - tp\n",
    "\n",
    "#     precision = tp/(predicted_p + 1e-8)\n",
    "#     recall = tp/(actual_p + 1e-8)\n",
    "#     f1score = 2*precision*recall / (precision + recall + 1e-8)\n",
    "\n",
    "#     results = dict(precision=round(precision,4), recall=round(recall,4), f1score=round(f1score,4))\n",
    "\n",
    "#     print(f\"results for {label} : \", results,end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validator.confusion_matrix.plot(normalize=False,save_dir=validator.save_dir,names=validator.names.values(),on_plot=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, figsize=(12, 9), tight_layout=True)\n",
    "# seaborn.set_theme(font_scale=1.0)  # for label size\n",
    "# ticklabels = labels\n",
    "\n",
    "# seaborn.heatmap(\n",
    "#     cf_matrix,\n",
    "#     ax=ax,\n",
    "#     annot=True,\n",
    "#     annot_kws={\"size\": 8},\n",
    "#     cmap=\"Blues\",\n",
    "#     fmt=\".0f\",\n",
    "#     square=True,\n",
    "#     vmin=0.0,\n",
    "#     xticklabels=ticklabels,\n",
    "#     yticklabels=ticklabels,\n",
    "# ).set_facecolor((1, 1, 1))\n",
    "# title = \"Confusion Matrix\"\n",
    "# ax.set_xlabel(\"True\")\n",
    "# ax.set_ylabel(\"Predicted\")\n",
    "# ax.set_title(title)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute test metrics for PP-YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchmetrics.detection import MeanAveragePrecision, IntersectionOverUnion\n",
    "import torch\n",
    "from tqdm import  tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics.utils.metrics import ConfusionMatrix\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install faster-coco-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(load_path):\n",
    "    with open(load_path, encoding='utf8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\runs_ppd\\test\\ppyoloe_plus_sod_crn_l_largesize_80e_visdronetr_empty_ratio_0.33_freeze_0.5\\bbox.json\"\n",
    "gt_path = r\"D:\\PhD\\Data per camp\\DetectionDataset\\Identification-split\\coco-dataset\\annotations\\annotations_test.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = load_json(preds_path)\n",
    "gt = load_json(gt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame.from_records(preds)\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.DataFrame.from_records(gt['annotations'])\n",
    "gt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(preds_path:str,gt_path:str, conf_matrix:ConfusionMatrix):\n",
    "\n",
    "    preds = load_json(preds_path)\n",
    "    preds = pd.DataFrame.from_records(preds)\n",
    "\n",
    "    gt = load_json(gt_path)\n",
    "    gt = pd.DataFrame.from_records(gt['annotations'])\n",
    "\n",
    "\n",
    "    mAP_computer = MeanAveragePrecision(box_format='xywh',iou_type='bbox',average='macro',\n",
    "                                        iou_thresholds=[0.5,0.6],\n",
    "                                        rec_thresholds=None,\n",
    "                                        class_metrics=True,\n",
    "                                        extended_summary=True)\n",
    "\n",
    "    for image_id in tqdm(preds.image_id.unique()):\n",
    "\n",
    "        mask_pred = preds['image_id'] == image_id\n",
    "        bbox_preds = preds.loc[mask_pred,'bbox'].to_list()\n",
    "        labels_pred = preds.loc[mask_pred,'category_id'].to_list()\n",
    "        scores = preds.loc[mask_pred,'score'].to_list()\n",
    "        pred_ = [dict(boxes=torch.Tensor(bbox_preds), labels=torch.Tensor(labels_pred).long(), scores=torch.Tensor(scores))]\n",
    "\n",
    "        mask_gt = gt['image_id'] == image_id\n",
    "        bbox_gt = gt.loc[mask_gt,'bbox'].to_list()\n",
    "        labels = gt.loc[mask_gt,'category_id'].to_list()\n",
    "        gt_ = [dict(boxes=torch.Tensor(bbox_gt),labels=torch.Tensor(labels).long())]\n",
    "        \n",
    "        mAP_computer.update(pred_,gt_)\n",
    "\n",
    "        # compute confusion matrix\n",
    "        \n",
    "        pred_bboxes = pred_[0]['boxes']\n",
    "        pred_bboxes[:,2:] = pred_bboxes[:,:2] + pred_bboxes[:,2:]\n",
    "        pred_scores = pred_[0]['scores'].view(-1,1)\n",
    "        labels = pred_[0]['labels'].view(-1,1)\n",
    "        dets = torch.hstack([pred_bboxes, pred_scores, labels])\n",
    "\n",
    "        gt_bboxes = gt_[0]['boxes']\n",
    "        if gt_bboxes.shape[0] != 0:\n",
    "            gt_bboxes[:,2:] = gt_bboxes[:,:2] + gt_bboxes[:,2:]\n",
    "        conf_matrix.process_batch(detections=dets, \n",
    "                                gt_bboxes=gt_bboxes, \n",
    "                                gt_cls=gt_[0]['labels']\n",
    "                            )\n",
    "                                    \n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "        #     raise ValueError\n",
    "            \n",
    "    results = mAP_computer.compute()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*20,'Detection','-'*20)\n",
    "\n",
    "preds_path = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\runs_ppd\\test\\ppyoloe_plus_sod_crn_l_largesize_80e_visdronetr_empty_ratio_0.33_freeze_0.5\\bbox.json\"\n",
    "gt_path = r\"D:\\PhD\\Data per camp\\DetectionDataset\\Identification-split\\coco-dataset\\annotations\\annotations_test.json\"\n",
    "\n",
    "conf_matrix = ConfusionMatrix(nc=len(cls_ids), conf=0.25, iou_thres=0.6)\n",
    "\n",
    "results = get_metrics(preds_path=preds_path,gt_path=gt_path,conf_matrix=conf_matrix)\n",
    "\n",
    "# results.keys()\n",
    "\n",
    "print('mAP50', results['map_50'].item())\n",
    "\n",
    "gt = load_json(gt_path)\n",
    "cls_ids = [i['id'] for i in gt['categories']]\n",
    "cls_names = [i['name'] for i in gt['categories']]\n",
    "\n",
    "\n",
    "labels = cls_names + [\"background\"]\n",
    "results_per_cls = dict()\n",
    "\n",
    "for i,label in enumerate(labels):\n",
    "\n",
    "    if label == 'background':\n",
    "        break\n",
    "\n",
    "    tp = conf_matrix.matrix[i,i]\n",
    "    actual_positive = conf_matrix.matrix[:,i].sum()\n",
    "    predicted_positive  = conf_matrix.matrix[i,:].sum()\n",
    "    # fp = predicted_positive - tp\n",
    "    # fn = actual_positive - tp\n",
    "\n",
    "    precision = tp/(predicted_positive + 1e-8)\n",
    "    recall = tp/(actual_positive + 1e-8)\n",
    "    f1score = 2*precision*recall / (precision + recall + 1e-8)\n",
    "\n",
    "    results = dict(precision=round(precision,4), recall=round(recall,4), f1score=round(f1score,4))\n",
    "\n",
    "    results_per_cls[label] = results\n",
    "\n",
    "    print(f\"results for {label} : \", results,end='\\n')\n",
    "\n",
    "with open(Path(preds_path).parent/'metrics_updated.json','w') as file:\n",
    "    json.dump(results_per_cls,file,indent=2)\n",
    "\n",
    "# save confusion matrix\n",
    "conf_matrix.plot(False,save_dir=Path(preds_path).parent,names=cls_names)\n",
    "\n",
    "# rec_thrs = torch.linspace(0.0, 1.00, round(1.00 / 0.01) + 1)\n",
    "\n",
    "# for cls_id in cls_ids: # class with index 0\n",
    "\n",
    "#     print(\"\\nClass: \",gt['categories'][cls_id]['name'])\n",
    "\n",
    "#     mAP = results['map_per_class'].item()\n",
    "#     print(\"AP@60 for\",gt['categories'][cls_id]['name'],': ', round(mAP,4))\n",
    "\n",
    "#     iou_id = 1 # 0.6\n",
    "#     recall = results['recall'][iou_id,cls_id,0,-1]\n",
    "#     print('Recall:', recall.item())\n",
    "\n",
    "#     recall_idx = (rec_thrs < recall).sum().item()\n",
    "#     recall_idx\n",
    "\n",
    "#     precision = results['precision'][iou_id,recall_idx-1,cls_id,0,-1]\n",
    "#     print('Precision:', precision.item())\n",
    "\n",
    "#     f1_score = 2 * recall*precision/(recall + precision)\n",
    "#     print('f1_score:', f1_score.item())\n",
    "\n",
    "    # plt.plot(rec_thrs,results['precision'][iou_id,:,cls_id,0,-1])\n",
    "    # plt.xlabel('Recall')\n",
    "    # plt.ylabel('Precision')\n",
    "    # plt.title(f\"Precision-Recall curve: mAP50={results['map_50']:.4f}\")\n",
    "    \n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_names, conf_matrix.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*20,'Identification','-'*20)\n",
    "\n",
    "preds_path = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\runs_ppd\\test\\ppyoloe_plus_sod_crn_l_largesize_80e_visdrone_identif-tr_empty_ratio_0.1_freeze_0.75\\bbox.json\"\n",
    "\n",
    "gt_path = r\"D:\\PhD\\Data per camp\\IdentificationDataset\\coco-dataset\\annotations\\annotations_test.json\"\n",
    "\n",
    "gt = load_json(gt_path)\n",
    "cls_ids = [i['id'] for i in gt['categories']]\n",
    "cls_names = [i['name'] for i in gt['categories']]\n",
    "\n",
    "conf_matrix = ConfusionMatrix(nc=len(cls_ids), conf=0.25, iou_thres=0.6)\n",
    "\n",
    "results = get_metrics(preds_path=preds_path,gt_path=gt_path,conf_matrix=conf_matrix)\n",
    "\n",
    "# results.keys()\n",
    "\n",
    "print('mAP50', results['map_50'].item())\n",
    "\n",
    "rec_thrs = torch.linspace(0.0, 1.00, round(1.00 / 0.01) + 1)\n",
    "\n",
    "labels = cls_names + [\"background\"]\n",
    "results_per_cls = dict()\n",
    "\n",
    "for i,label in enumerate(labels):\n",
    "\n",
    "    if label == 'background':\n",
    "        break\n",
    "\n",
    "    tp = conf_matrix.matrix[i,i]\n",
    "    actual_positive = conf_matrix.matrix[:,i].sum()\n",
    "    predicted_positive  = conf_matrix.matrix[i,:].sum()\n",
    "    # fp = predicted_positive - tp\n",
    "    # fn = actual_positive - tp\n",
    "\n",
    "    precision = tp/(predicted_positive + 1e-8)\n",
    "    recall = tp/(actual_positive + 1e-8)\n",
    "    f1score = 2*precision*recall / (precision + recall + 1e-8)\n",
    "\n",
    "    results = dict(precision=round(precision,4), recall=round(recall,4), f1score=round(f1score,4))\n",
    "\n",
    "    results_per_cls[label] = results\n",
    "\n",
    "    print(f\"results for {label} : \", results,end='\\n')\n",
    "\n",
    "with open(Path(preds_path).parent/'metrics_updated.json','w') as file:\n",
    "    json.dump(results_per_cls,file,indent=2)\n",
    "\n",
    "# save confusion matrix\n",
    "conf_matrix.plot(False,save_dir=Path(preds_path).parent,names=cls_names)\n",
    "\n",
    "\n",
    "# for cls_id in cls_ids: # class with index 0\n",
    "\n",
    "#     label = gt['categories'][cls_id]['name']\n",
    "\n",
    "#     print(\"\\nClass: \",label)\n",
    "\n",
    "#     mAP = results['map_per_class'][cls_id].item()\n",
    "#     print(\"AP@60 for\",label,': ', round(mAP,4))\n",
    "\n",
    "#     iou_id = 1 # 0.6\n",
    "#     recall = results['recall'][iou_id,cls_id,0,-1]\n",
    "#     print('Recall:', round(recall.item(),4))\n",
    "\n",
    "#     recall_idx = (rec_thrs < recall).sum().item()\n",
    "#     recall_idx\n",
    "\n",
    "#     precision = results['precision'][iou_id,recall_idx-1,cls_id,0,-1]\n",
    "#     print('Precision:', round(precision.item(),4))\n",
    "\n",
    "#     f1_score = 2 * recall*precision/(recall + precision)\n",
    "#     print('f1_score:', round(f1_score.item(),4))\n",
    "\n",
    "#     plt.plot(rec_thrs,results['precision'][iou_id,:,cls_id,0,-1])\n",
    "#     plt.xlabel('Recall')\n",
    "#     plt.ylabel('Precision')\n",
    "#     plt.title(f\"Precision-Recall curve for {label}: mAP50={results['map_50']:.4f}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
