{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image tiling for annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meanings of arguments\n",
    "- ```-ratioheight``` : proportion of tile  w.r.t height of image. Example 0.5 means dividing the image in two bands w.r.t height.\n",
    "- ```-ratiowidth``` : proportion of tile w.r.t to width of image. Example 1.0 means the width of the tile is the same as the image.\n",
    "- ```-overlapfactor``` : percentage of overlap. It should be less than 1.\n",
    "- ```-rmheight``` : percentage of height to remove or crop at bottom and top\n",
    "- ```-rmwidth``` : percentage of width to remove or crop on each side of the image\n",
    "- ```-pattern``` : \"**/*.JPG\" will get all .JPG images in directory and subdirectories. On windows it will get both .JPG and .jpg. On unix it will only get .JPG images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New script for tiling data\n",
    "# images_to_tile = r\"D:\\PhD\\Data per camp\\Extra training data\\savmap_dataset_v2\\raw_data\\images\"\n",
    "# destination_directory = r\"D:\\PhD\\Data per camp\\Extra training data\\savmap_dataset_v2\\raw_data\\images-tiled\"\n",
    "!python ../../HerdNet/tools/patcher.py \"D:\\PhD\\Harvard data\\Buffalo_camp\\cam0\" 0 0 0 -overlapfactor 0.1  -ratiowidth 0.5 -ratioheight 0.5 -rmheight 0.1 -rmwidth 0.1 -dest \"D:\\PhD\\Harvard data\\Buffalo_camp\\cam0-tiled\" -pattern \"**/*.JPG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-annotating data for Labelstudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "from datalabeling.annotator import Annotator\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a JSON file to be uuploaded to Label studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# provide correct alias, \"pt\", \"onnx\"\n",
    "alias = \"last\" # the aliases are found in mlflow tracker UI, use \"last-1\" to use the previous model\n",
    "name = \"obb-detector\" # detector, \"obb-detector\"\n",
    "handler = Annotator(mlflow_model_alias=alias,\n",
    "                    mlflow_model_name=name,\n",
    "                    is_yolo_obb= name.strip() == \"obb-detector\",\n",
    "                    # dotenv_path=\"../.env\"\n",
    "                    )\n",
    "path_img_dir=r\"D:\\PhD\\Africa Parks\\Liuwa aerial survey_ALL\\CENSUS 2019\\DAY 2 CENSUS 2019_CONVERTED\\AP 2019 day 2 - tiled\"\n",
    "root=\"D:\\\\\"\n",
    "save_json_path = os.path.join(Path(path_img_dir).parent, f\"{Path(path_img_dir).name}_preannotation_label-studio.json\")\n",
    "\n",
    "# build and saves json\n",
    "directory_preds = handler.build_upload_json(path_img_dir=path_img_dir,\n",
    "                                            root=root,\n",
    "                                            save_json_path=save_json_path,\n",
    "                                            pattern=\"**/*.JPG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-annotating an existing project using Label studio API\n",
    "It seems that it will not work well (i.e. filtering) with older projects created prior to Label studio software update.\n",
    "It is the **recommended way of pre-annotating data in Labelstudio**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide correct alias, \"pt\", \"onnx\"\n",
    "aliases = [\"version14\",]\n",
    "project_id = 95 # insert correct project_id by loooking at the url\n",
    "for alias in aliases:\n",
    "    name = \"obb-detector\" # detector, \"obb-detector\"\n",
    "    handler = Annotator(mlflow_model_alias=alias,\n",
    "                        mlflow_model_name=name,\n",
    "                        confidence_threshold=0.15,\n",
    "                        is_yolo_obb=name.strip() == \"obb-detector\",\n",
    "                        dotenv_path=\"../.env\")\n",
    "    handler.upload_predictions(project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before running the script below, make sure you have exported the annotations so you CAN revert back!!!**\n",
    "- CLEANING ANNOTATIONS that have been mistakenly saved with label=\"wildlife\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Cleaning annotations - NO WAY BACK\n",
    "name = \"obb-detector\"\n",
    "handler = Annotator(mlflow_model_alias=\"version6\",\n",
    "                        mlflow_model_name=name,\n",
    "                        confidence_threshold=0.25,\n",
    "                        is_yolo_obb=name.strip() == \"obb-detector\",\n",
    "                        dotenv_path=\"../.env\")\n",
    "\n",
    "# Select project\n",
    "project_id = 88\n",
    "project = handler.labelstudio_client.get_project(id=project_id)\n",
    "\n",
    "# Delete annotations saved with label \"wildlife\" assigned by the predictor\n",
    "tasks = project.get_tasks()\n",
    "for task in tqdm(tasks,desc=\"correcting annotations\"):\n",
    "        task_id = task['id']\n",
    "        img_url = task['data']['image']\n",
    "\n",
    "        if len(task[\"annotations\"][0]['result'])>1:\n",
    "            results_to_keep = []\n",
    "            annot_id = task[\"annotations\"][0][\"id\"]\n",
    "            for annot in task['annotations'][0]['result']:\n",
    "                if annot['value']['rectanglelabels'][0] != 'wildlife':\n",
    "                    results_to_keep.append(annot)\n",
    "                    # print(annot['value'],annot['id'],end=\"\\n\")\n",
    "            # print(f\"Updating annotations {annot_id} from task {task_id}.\")\n",
    "            # print(results_to_keep)\n",
    "            project.update_annotation(annot_id,result=results_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(task['annotations']), len(task['annotations'][0]['result']), task['id'], task[\"annotations\"][0][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task['annotations'][0]['result'][0] #['value']['rectanglelabels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_keep = []\n",
    "for annot in task['annotations'][0]['result']:\n",
    "    if annot['value']['rectanglelabels'][0] != 'wildlife':\n",
    "        results_to_keep.append(annot)\n",
    "        print(annot['value'],annot['id'],end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.update_annotation(annotation_id=...,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up inference on intel, make changes inn ultralytics/nn/autobackend.py:\n",
    "```\n",
    "- device_name = \"AUTO:NPU,GPU,CPU\" # CPU, GPU, NPU, AUTO,\"AUTO:GPU,NPU\"\n",
    "- inference_mode = \"LATENCY\" # OpenVINO inference modes are 'LATENCY', 'THROUGHPUT' (not recommended), or 'CUMULATIVE_THROUGHPUT'\n",
    "- LOGGER.info(f\"Using OpenVINO {inference_mode} mode for inference...\")\n",
    "- ov_compiled_model = core.compile_model(\n",
    "                ov_model,\n",
    "                device_name=device_name,  # AUTO selects best available device, do not modify\n",
    "                config={\"PERFORMANCE_HINT\": inference_mode,\n",
    "                        \"CACHE_DIR\": os.environ[\"OPENVINO_CACHE_MODEL\"]}, # make sure to set environment variable\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using path_to_weights\n",
    "# go to ultralytics.nn.autobackend to modify ov_compiled device to \"AUTO:NPU,GPU,CPU\"\n",
    "\n",
    "use_sliding_window=True\n",
    "\n",
    "handler = Annotator(path_to_weights=r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best_openvino_model\",\n",
    "                    is_yolo_obb=True,\n",
    "                    tilesize=1280,\n",
    "                    overlapratio=0.1,\n",
    "                    use_sliding_window=use_sliding_window,\n",
    "                    confidence_threshold=0.5,\n",
    "                    device=\"NPU\", # \"cpu\", \"cuda\"\n",
    "                    tag_to_append=f\"-sahi:{use_sliding_window}\",\n",
    "                    dotenv_path=\"../.env\")\n",
    "\n",
    "project_id = 3 # insert correct project_id by loooking at the url\n",
    "top_n=10\n",
    "handler.upload_predictions(project_id=project_id,top_n=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_studio_ml.utils import get_local_path\n",
    "from urllib.parse import unquote, quote\n",
    "import os\n",
    "path = unquote(\"/data/local-files/?d=savmap_dataset_v2%5Cimages_splits%5C003a34ee6b7841e6851b8fe511ebe102_0.JPG\")\n",
    "get_local_path(path,download_resources=False)#,os.path.exists(get_local_path(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with Sahi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "from datalabeling.annotator import Detector\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load env variable, loads model cache location!!\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = r\"D:\\savmap_dataset_v2\\images_splits\\00a033fefe644429a1e0fcffe88f8b39_1.JPG\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing with Openvino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up inference on intel, make changes inn ultralytics/nn/autobackend.py:\n",
    "```\n",
    "- device_name = \"AUTO:NPU,GPU,CPU\" # CPU, GPU, NPU, AUTO,\"AUTO:GPU,NPU\"\n",
    "- inference_mode = \"LATENCY\" # OpenVINO inference modes are 'LATENCY', 'THROUGHPUT' (not recommended), or 'CUMULATIVE_THROUGHPUT'\n",
    "- LOGGER.info(f\"Using OpenVINO {inference_mode} mode for inference...\")\n",
    "- ov_compiled_model = core.compile_model(\n",
    "                ov_model,\n",
    "                device_name=device_name,  # AUTO selects best available device, do not modify\n",
    "                config={\"PERFORMANCE_HINT\": inference_mode,\n",
    "                        \"CACHE_DIR\": os.environ[\"OPENVINO_CACHE_MODEL\"]}, # make sure to set environment variable\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define detector\n",
    "# to speed up inference on intel, make\n",
    "model = Detector(path_to_weights=r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best_openvino_model\",\n",
    "                confidence_threshold=0.1,\n",
    "                overlap_ratio=0.1,\n",
    "                tilesize=1280,\n",
    "                device='CPU',\n",
    "                use_sliding_window=False,\n",
    "                is_yolo_obb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(IMAGE_PATH)\n",
    "\n",
    "while True:\n",
    "    start_time = time.perf_counter()\n",
    "    print(model.predict(image,return_coco=True,nms_iou=0.5))\n",
    "    end_time = time.perf_counter()\n",
    "    print(f\"Device took {end_time-start_time:.2f} seconds.\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with openvino\n",
    "import openvino as ov\n",
    "import openvino.properties.hint as hints\n",
    "import torch\n",
    "import torchvision.transforms as F\n",
    "from ultralytics.utils import DEFAULT_CFG\n",
    "from ultralytics.cfg import get_cfg\n",
    "from ultralytics.data.converter import coco80_to_coco91_class\n",
    "\n",
    "# load validator\n",
    "args = get_cfg(cfg=DEFAULT_CFG)\n",
    "det_model = YOLO(r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best.pt\")\n",
    "det_validator = det_model.task_map[det_model.task][\"validator\"](args=args)\n",
    "det_validator.is_coco = True\n",
    "det_validator.class_map = coco80_to_coco91_class()\n",
    "det_validator.names = det_model.model.names\n",
    "det_validator.metrics.names = det_validator.names\n",
    "det_validator.nc = det_model.model.model[-1].nc\n",
    "det_validator.stride = 32\n",
    "args = get_cfg(cfg=DEFAULT_CFG)\n",
    "det_model = YOLO(r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best.pt\")\n",
    "\n",
    "core = ov.Core()\n",
    "det_model_path = r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best_openvino_model\\best.xml\"\n",
    "det_ov_model = core.read_model(det_model_path)\n",
    "\n",
    "device = \"AUTO:NPU,GPU\" # CPU, NPU, GPU \"AUTO:NPU,GPU,CPU\" \n",
    "\n",
    "print(\"Available core devices: \",core.available_devices)\n",
    "\n",
    "# reshaping for batch prediction\n",
    "input_layer = det_ov_model.input(0)\n",
    "output_layer = det_ov_model.output(0)\n",
    "new_shape = ov.PartialShape([1, 3, 1280, 1280])\n",
    "det_ov_model.reshape({input_layer.any_name: new_shape})\n",
    "\n",
    "ov_config = {hints.performance_mode: hints.PerformanceMode.THROUGHPUT,\n",
    "             \"CACHE_DIR\": '../models/model_cache'}\n",
    "\n",
    "if (\"GPU\" in core.available_devices) and device==\"GPU\":\n",
    "    ov_config[\"GPU_DISABLE_WINOGRAD_CONVOLUTION\"] = \"YES\"\n",
    "det_compiled_model = core.compile_model(det_ov_model, device, ov_config)\n",
    "\n",
    "def infer(image):\n",
    "    image = det_validator.preprocess({\"img\":image,\"batch_idx\":torch.Tensor([0]),\n",
    "                                      \"cls\":torch.Tensor([0]),\n",
    "                                      \"bboxes\":torch.Tensor([0.,0.,0.,0.])})[\"img\"]\n",
    "    results = det_compiled_model(image)\n",
    "    preds = torch.from_numpy(results[det_compiled_model.output(0)])\n",
    "    return det_validator.postprocess(preds) #torch.from_numpy(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(IMAGE_PATH)\n",
    "# image = F.PILToTensor()(image)[None,:,:1280,:1280]\n",
    "# infer(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with pt\n",
    "# model = YOLO(r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best.pt\",task='obb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling input images\n",
    "# model(image/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with openvino\n",
    "# model_vino = YOLO(r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best_openvino_model\",task='obb')\n",
    "# model_vino(image/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sahi_model_obb = Detector(path_to_weights=r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best_openvino_model\",\n",
    "#                     confidence_threshold=0.6,\n",
    "#                     overlap_ratio=0.1,\n",
    "#                     tilesize=640,\n",
    "#                     is_yolo_obb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = r\"D:\\savmap_dataset_v2\\images\\0d1ba3c424ad4414ac37dbd0c93460ea.JPG\"\n",
    "# image = Image.open(image_path)\n",
    "# print(image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = sahi_model_obb.predict(image,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "# result.export_visuals('../.tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sahi inference calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "overlap_ratios = [0.1,0.2,0.3]\n",
    "tilesizes = [640,2*640,3*640]\n",
    "imgsz = [640,2*640,3*640]\n",
    "\n",
    "for ratio, tilesize, image_size in product(overlap_ratios,tilesizes,imgsz):\n",
    "    print(ratio,tilesize,image_size)\n",
    "    # Define detector\n",
    "    # to speed up inference on intel, make\n",
    "    model = Detector(path_to_weights=r\"C:\\Users\\FADELCO\\OneDrive\\Bureau\\datalabeling\\models\\best_openvino_model\",\n",
    "                    confidence_threshold=0.1,\n",
    "                    overlap_ratio=0.1,\n",
    "                    tilesize=2000,\n",
    "                    imgsz=1280,\n",
    "                    device='CPU',\n",
    "                    use_sliding_window=True,\n",
    "                    is_yolo_obb=True)\n",
    "    \n",
    "    #TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO data_config.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from datalabeling.arguments import Arguments\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yaml\n",
    "with open(r\"D:\\PhD\\Data per camp\\DetectionDataset\\hard_samples\\train_ratio_20-seed_41.yaml\",'r') as file:\n",
    "    yolo_config = yaml.load(file,Loader=yaml.FullLoader)\n",
    "yolo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(os.path.join(yolo_config[\"path\"],yolo_config['train']),header=None,names=['paths'])['paths'].to_list()[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load label mapping\n",
    "args = Arguments()\n",
    "with open(r\"D:\\PhD\\Data per camp\\IdentificationDataset\\label_mapping.json\",'r') as file:\n",
    "    label_map = json.load(file)\n",
    "names = [p['name'] for p in label_map if p['name'] not in args.discard_labels ]\n",
    "label_map = dict(zip(range(len(names)),names))\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_studio_sdk import Client\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "def get_ls_parsed_config(ls_json_path:str):\n",
    "\n",
    "    ls_client = None\n",
    "    if ls_client is None:\n",
    "        # Connect to the Label Studio API and check the connection\n",
    "        LABEL_STUDIO_URL = os.getenv('LABEL_STUDIO_URL')\n",
    "        API_KEY = os.getenv(\"LABEL_STUDIO_API_KEY\")\n",
    "        labelstudio_client = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
    "\n",
    "    with open(ls_json_path,'r') as f:\n",
    "        ls_annotation = json.load(fp=f)\n",
    "    ids = set([annot['project'] for annot in ls_annotation])\n",
    "    assert len(ids)==1, \"annotations come from different project. Not allowed!\"\n",
    "    project_id = ids.pop()\n",
    "    project = labelstudio_client.get_project(id=project_id)\n",
    "\n",
    "    return project.parsed_label_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_config.update({'names':label_map,'nc':len(label_map)})\n",
    "yolo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\PhD\\Data per camp\\IdentificationDataset\\data_config.yaml\",'w') as file:\n",
    "    yaml.dump(yolo_config,file,default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize distribution per annotation project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datalabeling.dataset import convert_json_annotations_to_coco, load_coco_annotations\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "from label_studio_sdk import Client\n",
    "# from itertools import chain\n",
    "import traceback\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path=r\"..\\.env\"\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "# Connect to the Label Studio API and check the connection\n",
    "LABEL_STUDIO_URL = os.getenv('LABEL_STUDIO_URL')\n",
    "API_KEY = os.getenv(\"LABEL_STUDIO_API_KEY\")\n",
    "labelstudio_client = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_project_stats(project_id:int,annotator_id = 0):\n",
    "    \n",
    "    project = labelstudio_client.get_project(id=project_id)\n",
    "    num_images = dict()\n",
    "    # Iterating \n",
    "    tasks = project.get_tasks()\n",
    "     # because there is\n",
    "    labels = []\n",
    "\n",
    "    for task in tasks:\n",
    "        try:\n",
    "            result = task['annotations'][annotator_id]['result']\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "        img_labels = []\n",
    "        for annot in result:\n",
    "            img_labels = annot['value']['rectanglelabels'] + img_labels\n",
    "        labels = labels + img_labels\n",
    "        # update stats holder\n",
    "        for label in set(img_labels):\n",
    "            try:\n",
    "                num_images[label] += 1\n",
    "            except:\n",
    "                num_images[label] = 1\n",
    "\n",
    "    stats = {f\"{k}\":labels.count(k) for k in set(labels)}\n",
    "    print(\"Number of instances for each label is:\\n\",stats,end=\"\\n\\n\")\n",
    "    print(\"Number of images for each label is:\\n\",num_images)\n",
    "\n",
    "    return stats, num_images\n",
    "\n",
    "# get stats\n",
    "for project_id in [93,]:\n",
    "    get_project_stats(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_dir = r\"D:\\PhD\\Data per camp\\Exported annotations and labels\\Wet season - Rep 1\\all\\labelstudio\"\n",
    "dest_dir = Path(ls_dir).with_name(\"coco-format\")\n",
    "save_excel_path = Path(ls_dir).with_name(\"stats.xlsx\")\n",
    "\n",
    "# Uncomment to run if needed\n",
    "# convert_json_annotations_to_coco(input_dir=ls_dir,\n",
    "#                                  dest_dir_coco=str(dest_dir),\n",
    "#                                  ls_client=labelstudio_client,\n",
    "#                                  parse_ls_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_annotations_dict = load_coco_annotations(dest_dir)\n",
    "coco_annotations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_count(coco_annotation:dict):\n",
    "\n",
    "    result = Counter([annot['category_id'] for annot in coco_annotation['annotations']])\n",
    "\n",
    "    label_map = {cat['id']:cat['name'] for cat in coco_annotation['categories']}\n",
    "\n",
    "    result = {label_map[k]:v for k,v in result.items()}\n",
    "\n",
    "    return result\n",
    "\n",
    "label_stats = dict()\n",
    "\n",
    "for img_dir,coco_path in coco_annotations_dict.items():\n",
    "\n",
    "    with open(coco_path,'r') as f:\n",
    "        coco_annotation = json.load(fp=f)\n",
    "    \n",
    "    label_stats[img_dir] = get_labels_count(coco_annotation)\n",
    "\n",
    "label_stats = pd.DataFrame.from_dict(label_stats,orient='index').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to save\n",
    "label_stats.to_excel(save_excel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize splits' distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yaml\n",
    "with open(r\"D:\\PhD\\Data per camp\\Extra training data\\WAID\\data_config.yaml\",'r') as file:\n",
    "    yolo_config = yaml.load(file,Loader=yaml.FullLoader)\n",
    "yolo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = yolo_config['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "\n",
    "path_dataset = os.path.join(yolo_config['path'],yolo_config[split][0])\n",
    "path_dataset = path_dataset.replace('images','labels')\n",
    "\n",
    "path_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list()\n",
    "\n",
    "for txtfile in Path(path_dataset).glob(\"*.txt\"):\n",
    "\n",
    "    df = pd.read_csv(txtfile,sep=\" \",names = ['class','x','y','w','h'] )\n",
    "    df['class'] = df['class'].astype(int)    \n",
    "    df['image'] = txtfile.stem\n",
    "    labels.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(labels,axis=0)\n",
    "df['class'] = df['class'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_per_class = dict()\n",
    "for cls in df['class'].unique():\n",
    "    num_imge = df.loc[df['class'] == cls,'image'].unique().shape[0]\n",
    "    images_per_class[cls] = num_imge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Split:\", split)\n",
    "print(images_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Split:',split)\n",
    "print(df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts().plot(kind='bar',figsize=(10,5),logy=True,title=f\"{split} label distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing metrics on Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO or ultralytics models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "# from pathlib import Path\n",
    "from datalabeling.train import remove_label_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [IMPORTANT] Run this cell to convert the datasets to yolo format\n",
    "!python  ../tools/build_dataset.py --obb-to-yolo --data-config-yaml \"..\\data\\dataset_identification.yaml\" --skip\n",
    "!python  ../tools/build_dataset.py --obb-to-yolo --data-config-yaml \"..\\data\\dataset_identification-detection.yaml\" --skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting results for yolov12s : Detection and Identification\n",
    "paths = [\"...\", # Identification model weights\n",
    "         \"../runs/mlflow/140168774036374062/a59eda79d9444ff4befc561ac21da6b4/artifacts/weights/best.pt\" # Detection model weights\n",
    "        ]\n",
    "\n",
    "dataconfigs = [\n",
    "                # r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification.yaml\",\n",
    "               r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification-detection.yaml\"\n",
    "              ]\n",
    "\n",
    "imgsz = 800\n",
    "iou_threshold=0.45\n",
    "conf_threshold=0.235\n",
    "splits = [\n",
    "            # \"val\", \n",
    "          \"test\",\n",
    "          ]\n",
    "\n",
    "# remove label.cache files\n",
    "for dataconfig in dataconfigs:\n",
    "    remove_label_cache(data_config_yaml=dataconfig)\n",
    "\n",
    "for split in splits:\n",
    "    for path,dataconfig in zip(paths,dataconfigs):\n",
    "        print(\"\\n\",'-'*20,split,'-'*20)\n",
    "        model = YOLO(path)\n",
    "        model.info()\n",
    "        \n",
    "        # Customize validation settings\n",
    "        validation_results = model.val(data=dataconfig,\n",
    "                                        imgsz=imgsz,\n",
    "                                        batch=64,\n",
    "                                        split=split,\n",
    "                                        conf=conf_threshold,\n",
    "                                        iou=iou_threshold,\n",
    "                                        device=\"cuda\"\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting results for yolov5s : Detection and Identification\n",
    "paths = [\"../runs/mlflow/140168774036374062/87718ce84ce04dacac6ab8c92328eae7/artifacts/weights/best.pt\", # Identification model weights\n",
    "         \"../runs/mlflow/140168774036374062/e5e3bf93d34f48f1bb7d0a648530bb45/artifacts/weights/best.pt\" # Detection model weights\n",
    "        ]\n",
    "\n",
    "dataconfigs = [r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification.yaml\",\n",
    "               r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification-detection.yaml\"\n",
    "              ]\n",
    "\n",
    "imgsz = 800\n",
    "iou_threshold=0.45\n",
    "conf_threshold=0.235\n",
    "splits = [\n",
    "            # \"val\", \n",
    "          \"test\",\n",
    "          ]\n",
    "\n",
    "# remove label.cache files\n",
    "for dataconfig in dataconfigs:\n",
    "    remove_label_cache(data_config_yaml=dataconfig)\n",
    "\n",
    "for split in splits:\n",
    "    for path,dataconfig in zip(paths,dataconfigs):\n",
    "        print(\"\\n\",'-'*20,split,'-'*20)\n",
    "        model = YOLO(path)\n",
    "        model.info()\n",
    "        \n",
    "        # Customize validation settings\n",
    "        validation_results = model.val(data=dataconfig,\n",
    "                                        imgsz=imgsz,\n",
    "                                        batch=64,\n",
    "                                        split=split,\n",
    "                                        conf=conf_threshold,\n",
    "                                        iou=iou_threshold,\n",
    "                                        device=\"cuda\"\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [IMPORTANT] Run this cell to convert the datasets to obb format\n",
    "!python  ../tools/build_dataset.py --yolo-to-obb --data-config-yaml \"..\\data\\dataset_identification.yaml\" --skip\n",
    "!python  ../tools/build_dataset.py --yolo-to-obb --data-config-yaml \"..\\data\\dataset_identification-detection.yaml\" --skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load yolov11s-obb\n",
    "paths = [\"../runs/mlflow/140168774036374062/34c709364c0e46dcb72c526de34a7fa4/artifacts/weights/best.pt\", # Identification\n",
    "         \"../runs/mlflow/140168774036374062/f5b7124be14c4c89b8edd26bcf7a9a76/artifacts/weights/best.pt\", # Detection\n",
    "        ]\n",
    "\n",
    "\n",
    "dataconfigs = [r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification.yaml\",\n",
    "               r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification-detection.yaml\"\n",
    "              ]\n",
    "\n",
    "imgsz = 800\n",
    "iou_threshold=0.45\n",
    "conf_threshold=0.235\n",
    "splits = [\n",
    "        #   \"val\", \n",
    "          \"test\",\n",
    "          ]\n",
    "\n",
    "# remove label.cache files\n",
    "for dataconfig in dataconfigs:\n",
    "    remove_label_cache(data_config_yaml=dataconfig)\n",
    "\n",
    "for split in splits:\n",
    "    for path,dataconfig in zip(paths,dataconfigs):\n",
    "        print(\"\\n\",'-'*20,split,'-'*20)\n",
    "        model = YOLO(path)\n",
    "        model.info()\n",
    "        \n",
    "        # Customize validation settings\n",
    "        validation_results = model.val(data=dataconfig,\n",
    "                                        imgsz=imgsz,\n",
    "                                        batch=64,\n",
    "                                        split=split,\n",
    "                                        conf=conf_threshold,\n",
    "                                        iou=iou_threshold,\n",
    "                                        device=\"cuda\"\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load yolov8s-obb\n",
    "paths = [\n",
    "         \"../runs/mlflow/140168774036374062/b883bd2b31f94f29807ea3b94e8ff8fc/artifacts/weights/best.pt\", # Identification\n",
    "         \"../runs/mlflow/140168774036374062/8a76c60253fc48788b5324096d035420/artifacts/weights/best.pt\"  # Detection\n",
    "        ]\n",
    "\n",
    "\n",
    "dataconfigs = [\n",
    "                r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification.yaml\",\n",
    "               r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification-detection.yaml\"\n",
    "              ]\n",
    "\n",
    "imgsz = 800\n",
    "iou_threshold=0.45\n",
    "conf_threshold=0.235\n",
    "splits = [\n",
    "        #   \"val\", \n",
    "          \"test\",\n",
    "          ]\n",
    "\n",
    "# remove label.cache files\n",
    "for dataconfig in dataconfigs:\n",
    "    remove_label_cache(data_config_yaml=dataconfig)\n",
    "\n",
    "for split in splits:\n",
    "    for path,dataconfig in zip(paths,dataconfigs):\n",
    "        print(\"\\n\",'-'*20,split,'-'*20)\n",
    "        model = YOLO(path)\n",
    "        model.info()\n",
    "        \n",
    "        # Customize validation settings\n",
    "        validation_results = model.val(data=dataconfig,\n",
    "                                        imgsz=imgsz,\n",
    "                                        batch=64,\n",
    "                                        split=split,\n",
    "                                        conf=conf_threshold,\n",
    "                                        iou=iou_threshold,\n",
    "                                        device=\"cuda\"\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "from datalabeling.annotator import Annotator\n",
    "\n",
    "for alias in [\"version9\", \"version6\"]:\n",
    "    print(\"-\"*10,alias,end=\"\\n\\n\")\n",
    "    name = \"obb-detector\" # detector, \"obb-detector\"\n",
    "    handler = Annotator(mlflow_model_alias=alias,\n",
    "                            mlflow_model_name=name,\n",
    "                            confidence_threshold=0.25,\n",
    "                            is_yolo_obb=name.strip() == \"obb-detector\",\n",
    "                            dotenv_path=\"../.env\")\n",
    "\n",
    "    yolo_model = handler.model.unwrap_python_model().detection_model.detection_model.model\n",
    "    validation_results = yolo_model.val(data=r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_hn.yaml\",\n",
    "                                    imgsz=1280,\n",
    "                                    batch=32,\n",
    "                                    conf=0.25,\n",
    "                                    iou=0.45,\n",
    "                                    device=\"cuda\"\n",
    "                                )\n",
    "    \n",
    "    print(validation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Herdnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datalabeling.train.herdnet import HerdnetData, HerdnetTrainer\n",
    "from datalabeling.arguments import Arguments\n",
    "import lightning as L\n",
    "import os, yaml\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowering matrix multiplication precision\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "args = Arguments()\n",
    "args.lr0 = 3e-4\n",
    "args.epochs = 15\n",
    "args.imgsz = 800\n",
    "args.batchsize = 8\n",
    "down_ratio = 2\n",
    "args.data_config_yaml = r\"D:\\datalabeling\\data\\data_config.yaml\"\n",
    "args.path_weights = r\"D:\\datalabeling\\models\\20220329_HerdNet_Ennedi_dataset_2023.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: load Prediction images\n",
    "with open(args.data_config_yaml, 'r') as file:\n",
    "    data_config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "images_path = os.path.join(data_config['path'],data_config['test'][0])\n",
    "images_path = list(Path(images_path).glob('*'))\n",
    "\n",
    "\n",
    "# Data\n",
    "datamodule = HerdnetData(data_config_yaml=args.data_config_yaml,\n",
    "                            patch_size=args.imgsz,\n",
    "                            batch_size=args.batchsize,\n",
    "                            down_ratio=down_ratio,\n",
    "                            train_empty_ratio=0.,\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,targets = datamodule.val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm(datamodule.val_dataloader()):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    batched = dict(img=torch.stack([p[0] for p in batch])\n",
    "                )\n",
    "    targets = [p[1] for p in batch]\n",
    "    keys = targets[0].keys()\n",
    "    \n",
    "    for k in keys:\n",
    "        if k == 'points':\n",
    "            batched[k] = torch.vstack([a[k] for a in targets])\n",
    "        elif k == 'labels':\n",
    "            batched[k] = torch.hstack([a[k] for a in targets])\n",
    "        elif (k == 'w') or (k == 'h'):\n",
    "            batched[k] = torch.hstack([torch.Tensor(a[k]) for a in targets])\n",
    "        else:\n",
    "            batched[k] = [a[k] for a in targets]    \n",
    "\n",
    "    return batched\n",
    "\n",
    "loader = DataLoader(datamodule.val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "batch = next(iter(loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in loader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{str(k):v for k,v in data_config['names'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a few\n",
    "# random.shuffle(images_path)\n",
    "images_path = images_path[:10]\n",
    "datamodule.set_predict_dataset(images_path=images_path,batchsize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "for img in datamodule.predict_dataloader():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "checkpoint_path = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\tools\\lightning-ckpts\\epoch=23-step=2040.ckpt\"\n",
    "herdnet_trainer = HerdnetTrainer.load_from_checkpoint(checkpoint_path=checkpoint_path,\n",
    "                                                            args=args,\n",
    "                                                            ce_weight=None,\n",
    "                                                            work_dir='../.tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(accelerator=\"auto\",profiler='simple')\n",
    "out = trainer.predict(model=herdnet_trainer,\n",
    "                datamodule=datamodule,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(out,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with SAM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FADELCO\\miniconda3\\envs\\label-backend\\Lib\\site-packages\\albumentations\\__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.21). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import SAM\n",
    "\n",
    "from datalabeling.train.herdnet import HerdnetData, HerdnetTrainer\n",
    "from datalabeling.arguments import Arguments\n",
    "import lightning as L\n",
    "import os, yaml\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/sam2.1_l.pt to 'sam2.1_l.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 428M/428M [00:24<00:00, 18.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Load a model\n",
    "model = SAM(\"sam2.1_l.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary: 854 layers, 224,446,642 parameters, 224,446,642 gradients\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(854, 224446642, 224446642, 0.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display model information (optional)\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "concatenating datasets:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting groundtruths: 668it [00:01, 498.73it/s]\n",
      "concatenating datasets: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 0 empty images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting groundtruths: 1356it [00:03, 433.85it/s]:00<?, ?it/s]\n",
      "concatenating datasets: 100%|██████████| 1/1 [00:03<00:00,  3.25s/it]\n"
     ]
    }
   ],
   "source": [
    "args = Arguments()\n",
    "args.lr0 = 3e-4\n",
    "args.epochs = 15\n",
    "args.imgsz = 640 # Attention, it will resize the image and the targets\n",
    "args.batchsize = 8\n",
    "down_ratio = 1 # Attention, it will down sample the targets. i.e. x/down_ratio,y/down_ratio\n",
    "args.data_config_yaml = r\"D:\\datalabeling\\data\\data_config.yaml\"\n",
    "args.path_weights = r\"D:\\datalabeling\\models\\20220329_HerdNet_Ennedi_dataset_2023.pth\"\n",
    "args.data_config_yaml = r\"D:\\datalabeling\\data\\data_config.yaml\"\n",
    "\n",
    "# Example: load Prediction images\n",
    "with open(args.data_config_yaml, 'r') as file:\n",
    "    data_config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "images_path = os.path.join(data_config['path'],data_config['test'][0])\n",
    "images_path = list(Path(images_path).glob('*'))\n",
    "\n",
    "\n",
    "# Data\n",
    "datamodule = HerdnetData(data_config_yaml=args.data_config_yaml,\n",
    "                            patch_size=args.imgsz,\n",
    "                            batch_size=args.batchsize,\n",
    "                            down_ratio=down_ratio,\n",
    "                            train_empty_ratio=0.,\n",
    "                            )\n",
    "\n",
    "datamodule.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, targets = datamodule.val_dataset[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 640, 640])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([6]),\n",
       " 'w': [45.0],\n",
       " 'h': [39.0],\n",
       " 'points': tensor([[357., 353.]]),\n",
       " 'image_id': [25],\n",
       " 'image_name': ['D:\\\\general_dataset\\\\tiled-data\\\\test\\\\images\\\\04e8092d743bef891386f3e0ce82155f12aa4035_53.JPG']}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# points are (x,y)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\general_dataset\\tiled-data\\test\\images\\04e8092d743bef891386f3e0ce82155f12aa4035_53.JPG: 640x640 1 0, 1997.3ms\n",
      "Speed: 30.7ms preprocess, 1997.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "# Run inference with bboxes prompt\n",
    "results, = model(targets['image_name'][0],\n",
    "                 imgsz=640,\n",
    "                points=targets['points'].tolist(), # list[(x,y)]\n",
    "                labels=targets['labels'].tolist(),\n",
    "                save_dir=r\"D:\\datalabeling\\.tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[134., 622.,  44.,  34.],\n",
       "        [460., 597.,  26.,  34.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.boxes.xywh # relative to the original shape of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Results object with attributes:\n",
       "\n",
       "boxes: ultralytics.engine.results.Boxes object\n",
       "keypoints: None\n",
       "masks: ultralytics.engine.results.Masks object\n",
       "names: {0: '0', 1: '1'}\n",
       "obb: None\n",
       "orig_img: array([[[180, 200, 218],\n",
       "        [174, 194, 212],\n",
       "        [167, 186, 207],\n",
       "        ...,\n",
       "        [126, 140, 169],\n",
       "        [119, 133, 162],\n",
       "        [113, 127, 156]],\n",
       "\n",
       "       [[157, 177, 195],\n",
       "        [155, 175, 193],\n",
       "        [154, 173, 194],\n",
       "        ...,\n",
       "        [124, 138, 167],\n",
       "        [118, 132, 161],\n",
       "        [112, 126, 155]],\n",
       "\n",
       "       [[137, 154, 175],\n",
       "        [138, 155, 176],\n",
       "        [137, 156, 177],\n",
       "        ...,\n",
       "        [118, 133, 165],\n",
       "        [116, 129, 161],\n",
       "        [112, 125, 157]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 61,  57,  86],\n",
       "        [ 59,  56,  82],\n",
       "        [ 59,  53,  78],\n",
       "        ...,\n",
       "        [ 83,  83, 107],\n",
       "        [ 85,  83, 105],\n",
       "        [ 84,  83, 103]],\n",
       "\n",
       "       [[ 68,  65,  91],\n",
       "        [ 66,  63,  89],\n",
       "        [ 66,  60,  85],\n",
       "        ...,\n",
       "        [ 86,  83, 108],\n",
       "        [ 87,  85, 107],\n",
       "        [ 86,  84, 106]],\n",
       "\n",
       "       [[ 75,  72,  98],\n",
       "        [ 72,  69,  94],\n",
       "        [ 71,  65,  90],\n",
       "        ...,\n",
       "        [ 89,  86, 111],\n",
       "        [ 90,  87, 112],\n",
       "        [ 91,  89, 111]]], dtype=uint8)\n",
       "orig_shape: (640, 640)\n",
       "path: 'D:\\\\general_dataset\\\\tiled-data\\\\test\\\\images\\\\018f5ab5b7516a47ff2ac48a9fc08353b533c30f_23.JPG'\n",
       "probs: None\n",
       "save_dir: 'runs\\\\segment\\\\predict'\n",
       "speed: {'preprocess': 14.757156372070312, 'inference': 3832.204580307007, 'postprocess': 0.0}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 640, 640])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.masks.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 640, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.orig_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../.tmp/example.jpg'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.save(filename=\"../.tmp/example.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with bboxes prompt\n",
    "results = model(\"path/to/image.jpg\", bboxes=[100, 100, 200, 200])\n",
    "\n",
    "# Run inference with single point\n",
    "results = model(points=[900, 370], labels=[1])\n",
    "\n",
    "# Run inference with multiple points\n",
    "results = model(points=[[400, 370], [900, 370]], labels=[1, 1])\n",
    "\n",
    "# Run inference with multiple points prompt per object\n",
    "results = model(points=[[[400, 370], [900, 370]]], labels=[[1, 1]])\n",
    "\n",
    "# Run inference with negative points prompt\n",
    "results = model(points=[[[400, 370], [900, 370]]], labels=[[1, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing inference params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datalabeling.annotator import Detector\n",
    "from datalabeling.arguments import Arguments\n",
    "from datalabeling.dataset.sampling import (get_preds_targets, compute_detector_performance, get_uncertainty)    \n",
    "import yaml, os\n",
    "from hyperopt import tpe, hp, fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params \n",
    "args = Arguments()\n",
    "args.path_to_weights = r\"C:/Users/Machine Learning/Desktop/workspace-wildAI/datalabeling/runs/mlflow/140168774036374062/57daf3bcd99b4dd4b040cb4f8670960c/artifacts/weights/best.pt\"\n",
    "# args.confidence_threshold = 0.2\n",
    "# args.overlap_ratio = 0.1\n",
    "args.use_sliding_window = True\n",
    "args.device = \"cuda\"\n",
    "args.is_yolo_obb = True\n",
    "args.pred_results_dir = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\.tmp\"\n",
    "args.data_config_yaml = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_hn.yaml\"\n",
    "args.hn_uncertainty_method = \"entropy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load groundtruth\n",
    "with open(args.data_config_yaml,'r') as file:\n",
    "    yolo_config = yaml.load(file,Loader=yaml.FullLoader)\n",
    "\n",
    "split='val'\n",
    "images_path = [os.path.join(yolo_config['path'],yolo_config[split][i]) for i in range(len(yolo_config[split]))]\n",
    "images_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params:dict):\n",
    "\n",
    "    # Define detector\n",
    "    model = Detector(path_to_weights=args.path_to_weights,\n",
    "                        confidence_threshold=params['confidence_threshold'],\n",
    "                        overlap_ratio=params['overlap_ratio'],\n",
    "                        tilesize=params['tilesize'],\n",
    "                        imgsz=params['imgsz'],\n",
    "                        use_sliding_window=args.use_sliding_window,\n",
    "                        device=args.device,\n",
    "                        is_yolo_obb=args.is_yolo_obb\n",
    "                    )\n",
    "\n",
    "    df_results, df_labels, col_names = get_preds_targets(images_dirs=images_path,\n",
    "                                                        pred_results_dir=args.pred_results_dir,\n",
    "                                                        detector=model,\n",
    "                                                        load_results=False,\n",
    "                                                        save_tag=f\"{params['imgsz']}-{params['tilesize']}-{params['overlap_ratio']}-{params['confidence_threshold']}\"\n",
    "                                                        )\n",
    "\n",
    "    df_results_per_img = compute_detector_performance(df_results,df_labels,col_names)\n",
    "    # df_results_per_img = get_uncertainty(df_results_per_img=df_results_per_img,mode=args.hn_uncertainty_method)\n",
    "\n",
    "    # minizing loss -> maximize map50 and map75\n",
    "    loss = -1.0*df_results_per_img[\"map50\"].mean() - df_results_per_img[\"map75\"].mean() #+ df_results_per_img[\"uncertainty\"].mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "                'confidence_threshold': hp.uniform('x', 0.1, 0.7),\n",
    "                'overlap_ratio': hp.uniform('y', 0, 0.25),\n",
    "                'tilesize': hp.choice(label='tilesize',options=[640, 2*640]),\n",
    "                'imgsz': hp.choice(label='imgsz',options=[640, 2*640, 3*640, 4*640]),\n",
    "            }\n",
    "\n",
    "best = fmin(\n",
    "    fn=objective, # Objective Function to optimize\n",
    "    space=search_space, # Hyperparameter's Search Space\n",
    "    algo=tpe.suggest, # Optimization algorithm (representative TPE)\n",
    "    max_evals=2 # Number of optimization attempts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset label format conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_label_format(loaded_df:pd.DataFrame)->str:\n",
    "    \"\"\"checks label format\n",
    "\n",
    "    Args:\n",
    "        loaded_df (pd.DataFrame): target values\n",
    "\n",
    "    Raises:\n",
    "        NotImplementedError: when the format is not yolo or yolo-obb\n",
    "\n",
    "    Returns:\n",
    "        str: yolo or yolo-obb\n",
    "    \"\"\"\n",
    "\n",
    "    num_features = len(loaded_df.columns)\n",
    "\n",
    "    if num_features == 5:\n",
    "        return \"yolo\"\n",
    "    elif num_features == 9:\n",
    "        return \"yolo-obb\"\n",
    "    else:\n",
    "        raise NotImplementedError(f\"The number of features ({num_features}) in the label file is wrong. Check yolo or yolo-obb format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = r\"D:\\PhD\\Data per camp\\DetectionDataset\\Rep 1\\train\\labels\\DJI_20231002150401_0009_0_48_0_1271_640_1911.txt\"\n",
    "df = pd.read_csv(label_path,sep=' ',header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(df.iloc[:,0].dtype, np.dtypes.IntDType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_label_format(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['id','x1','y1','x2','y2','x3','y3','x4','y4']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "from datalabeling.arguments import Arguments, Dataprepconfigs\n",
    "import os, logging, traceback\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import math\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Dataprepconfigs()\n",
    "\n",
    "args.empty_ratio = 15\n",
    "args.parse_ls_config = True\n",
    "args.label_map =  r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\exported_annotations\\label_mapping.json\" \n",
    "args.min_visibility = 0.7 \n",
    "args.dest_path_labels =  r\"D:\\PhD\\Data per camp\\IdentificationDataset\\val\\labels\"  \n",
    "args.ls_json_dir =  r\"D:\\PhD\\Data per camp\\Exported annotations and labels\\identification-splits\\val\\labelstudio\" \n",
    "args.dest_path_images = r\"D:\\PhD\\Data per camp\\IdentificationDataset\\val\\images\" \n",
    "args.coco_json_dir = r\"D:\\PhD\\Data per camp\\Exported annotations and labels\\identification-splits\\val\\coco-format\"\n",
    "args.height = 800\n",
    "args.width = 800\n",
    "args.data_config_yaml =  r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\dataset_identification.yaml\"\n",
    "args.keep_labels = (\"buffalo\", \"impala\", \"nyala\", \"nyala(m)\", \"roan\", \"sable\")\n",
    "args.discard_labels = None\n",
    "args.save_all = False\n",
    "args.clear_yolo_dir = False\n",
    "args.save_only_empty = False\n",
    "args.is_detector = False\n",
    "\n",
    "args.load_coco_annotations = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datalabeling.dataset import  update_yolo_data_cfg, get_slices, sample_data, load_coco_annotations, load_label_map\n",
    "\n",
    "if args.load_coco_annotations:\n",
    "    map_imgdir_cocopath = load_coco_annotations(dest_dir_coco=args.coco_json_dir)\n",
    "\n",
    "    # load label map\n",
    "if not args.is_detector:\n",
    "    label_map = load_label_map(path=args.label_map,\n",
    "                                label_to_discard=args.discard_labels,\n",
    "                                labels_to_keep=args.keep_labels)\n",
    "    # update_yolo_data_cfg(args.data_config_yaml, label_map=label_map)\n",
    "    name_id_map = {val:key for key,val in label_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "# slice coco annotations and save tiles\n",
    "for img_dir,cocopath in map_imgdir_cocopath.items():\n",
    "    # try:\n",
    "    # slice annotations\n",
    "    coco_dict_slices = get_slices(coco_annotation_file_path=cocopath,\n",
    "                        img_dir=img_dir,\n",
    "                        slice_height=args.height,\n",
    "                        slice_width=args.width,\n",
    "                        overlap_height_ratio=args.overlap_ratio,\n",
    "                        overlap_width_ratio=args.overlap_ratio,\n",
    "                        min_area_ratio=args.min_visibility,\n",
    "                        ignore_negative_samples= (args.empty_ratio<1e-8 and not args.save_all), # equivalent to args.empty_ratio == 0.0\n",
    "                        )\n",
    "    \n",
    "    break\n",
    "    # sample tiles\n",
    "    df_tiles = sample_data(coco_dict_slices=coco_dict_slices,\n",
    "                            empty_ratio=args.empty_ratio,\n",
    "                            out_csv_path= None, #Path(args.dest_path_images).with_name(\"gt.csv\"),\n",
    "                            img_dir=img_dir,\n",
    "                            save_all=args.save_all,\n",
    "                            labels_to_discard=args.discard_labels,\n",
    "                            labels_to_keep=args.keep_labels,\n",
    "                            sample_only_empty=args.save_only_empty\n",
    "                            )\n",
    "    # except Exception as e:\n",
    "    #     traceback.print_exc()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_image(file_name:str):\n",
    "    ext = '.jpg' \n",
    "    file_name = Path(file_name).stem\n",
    "    # print(file_name)\n",
    "    parent_file = '_'.join(file_name.split('_')[:-5])\n",
    "    p = os.path.join(img_dir,parent_file+ext)\n",
    "    if os.path.exists(p):\n",
    "        return p\n",
    "    raise FileNotFoundError(f'Parent file note found for {file_name} in {img_dir} >> {parent_file}')\n",
    "    \n",
    "    \n",
    "# build mapping for labels\n",
    "label_ids = [cat['id'] for cat in coco_dict_slices['categories']]\n",
    "label_name = [cat['name'] for cat in coco_dict_slices['categories']]\n",
    "label_map = dict(zip(label_ids,label_name))\n",
    "\n",
    "# build dataFrame of image slices \n",
    "ids = list()\n",
    "x0s, x1s = list(), list()\n",
    "y0s, y1s = list(), list()\n",
    "file_paths = list()\n",
    "parent_file_paths = list()\n",
    "for metadata in coco_dict_slices['images']:\n",
    "    # img_path = os.path.join(img_dir,metadata['file_name'])\n",
    "    file_paths.append(metadata['file_name'])\n",
    "    file_name = os.path.basename(metadata['file_name'])\n",
    "    x_0,y_0,x_1,y_1 = file_name.split('.')[0].split('_')[-4:]\n",
    "    parent_image = get_parent_image(file_name)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata, img_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.data import YOLODataset\n",
    "from datalabeling.arguments import Arguments\n",
    "from ultralytics.data import build_dataloader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/dataset.py\n",
    "# img_path (str): Path to the folder containing images.\n",
    "# imgsz (int, optional): Image size. Defaults to 640.\n",
    "# cache (bool, optional): Cache images to RAM or disk during training. Defaults to False.\n",
    "# augment (bool, optional): If True, data augmentation is applied. Defaults to True.\n",
    "# hyp (dict, optional): Hyperparameters to apply data augmentation. Defaults to None.\n",
    "# prefix (str, optional): Prefix to print in log messages. Defaults to ''.\n",
    "# rect (bool, optional): If True, rectangular training is used. Defaults to False.\n",
    "# batch_size (int, optional): Size of batches. Defaults to None.\n",
    "# stride (int, optional): Stride. Defaults to 32.\n",
    "# pad (float, optional): Padding. Defaults to 0.0.\n",
    "# single_cls (bool, optional): If True, single class training is used. Defaults to False.\n",
    "# classes (list): List of included classes. Default is None.\n",
    "# fraction (float): Fraction of dataset to utilize. Default is 1.0 (use all data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments()\n",
    "args.data_config_yaml = r\"D:\\datalabeling\\data\\data_config.yaml\"\n",
    "\n",
    "with open(args.data_config_yaml, 'r') as file:\n",
    "    data_config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "kwargs = dict(img_path=os.path.join(data_config['path'],data_config['val'][0]),\n",
    "              data=data_config,\n",
    "              task='detect',\n",
    "              imgsz=800,\n",
    "              cache=None,\n",
    "              augment=False,\n",
    "              hyp=None,\n",
    "              batch_size=8,\n",
    "              single_cls=False,\n",
    "              rect=False,\n",
    "              prefix='debug',\n",
    "              stride=1,\n",
    "              pad=0.,\n",
    "              classes=None,\n",
    "              fraction=1.0\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = YOLODataset(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = build_dataloader(dataset=dataset,batch=8,workers=8,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['im_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['ori_shape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['img'].flip(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['img'].min(),batch['img'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['img'].shape, batch['bboxes'].shape, batch['cls'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataprep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
