{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements, uncomment to run\n",
    "# !pip install geopandas pillow label-studio-converter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Savmap dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gdp\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load data\n",
    "root = \"/Users/sfadel/Desktop/savmap\"\n",
    "polygons = gdp.read_file(os.path.join(root,'savmap_annotations_2014.geojson'))\n",
    "\n",
    "path_to_images = root #os.path.join(root,'images')\n",
    "path_to_labels = os.path.join(root,'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bounding boxs\n",
    "for cat in ['x_min', 'y_min', 'x_max', 'y_max','width','height','x','y']:\n",
    "    polygons[cat] = None\n",
    "for i in range(len(polygons)):\n",
    "    x_min, y_min, x_max, y_max = polygons['geometry'].iloc[i].bounds\n",
    "    image_path = os.path.join(path_to_images,f\"{polygons['IMAGEUUID'].iloc[i]}.JPG\")\n",
    "    try:\n",
    "        width, height = Image.open(image_path).size\n",
    "    except:\n",
    "        continue\n",
    "    polygons['x_min'].iat[i] = int(x_min)\n",
    "    polygons['x_max'].iat[i] = int(x_max)\n",
    "    polygons['y_min'].iat[i] = int(y_min)\n",
    "    polygons['y_max'].iat[i] = int(y_max)\n",
    "    polygons['x'].iat[i] = round(0.5*(x_max+x_min))\n",
    "    polygons['y'].iat[i] = round(0.5*(y_max+y_min))\n",
    "    polygons['width'] = width\n",
    "    polygons['height'] = height\n",
    "\n",
    "polygons['bbox_w'] = polygons['x_max'] - polygons['x_min']\n",
    "polygons['bbox_h'] = polygons['y_max'] - polygons['y_min']\n",
    "polygons['class'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select labels\n",
    "data = polygons.loc[:,['x_min','x_max','y_min','y_max','IMAGEUUID','width','height','class']].copy()\n",
    "data.rename(columns={'IMAGEUUID':'filename',\n",
    "                     'x_max':'xmax',\n",
    "                     'x_min':'xmin',\n",
    "                     'y_min':'ymin',\n",
    "                     'y_max':'ymax'},inplace=True)\n",
    "data['filename'] = data['filename'].apply(lambda x: f\"{x}.JPG\")\n",
    "data = data.dropna()\n",
    "data = data[['filename','class','width', 'height','xmin','ymin','xmax','ymax']]\n",
    "data.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CSV to COCO\n",
    "save_json_path = ... #'cocoformat.json'\n",
    "\n",
    "images = []\n",
    "categories = []\n",
    "annotations = []\n",
    "\n",
    "category = {}\n",
    "category[\"supercategory\"] = 'None'\n",
    "category[\"id\"] = 0\n",
    "category[\"category_name\"] = 'wildlife'\n",
    "categories.append(category)\n",
    "\n",
    "data['fileid'] = data['filename'].astype('category').cat.codes\n",
    "data['categoryid']= pd.Categorical(data['class'],ordered= True).codes\n",
    "# data['categoryid'] = data['categoryid']+1\n",
    "data['annid'] = data.index\n",
    "\n",
    "def image(row):\n",
    "    image = {}\n",
    "    image[\"height\"] = row.height\n",
    "    image[\"width\"] = row.width\n",
    "    image[\"id\"] = row.fileid\n",
    "    image[\"file_name\"] = row.filename\n",
    "    return image\n",
    "\n",
    "# def category(row):\n",
    "#     category = {}\n",
    "#     category[\"supercategory\"] = 'None'\n",
    "#     category[\"id\"] = row.categoryid\n",
    "#     category[\"category_name\"] = row[2]\n",
    "#     return category\n",
    "\n",
    "def annotation(row):\n",
    "    annotation = {}\n",
    "    area = (row.xmax -row.xmin)*(row.ymax - row.ymin)\n",
    "    annotation[\"segmentation\"] = []\n",
    "    annotation[\"iscrowd\"] = 0\n",
    "    annotation[\"area\"] = area\n",
    "    annotation[\"image_id\"] = row.fileid\n",
    "    annotation[\"score\"] = 1.0\n",
    "\n",
    "    annotation[\"bbox\"] = [row.xmin, row.ymin, row.xmax -row.xmin,row.ymax-row.ymin ]\n",
    "\n",
    "    annotation[\"category_id\"] = row.categoryid\n",
    "    annotation[\"id\"] = row.annid\n",
    "    return annotation\n",
    "\n",
    "for row in data.itertuples():\n",
    "    annotations.append(annotation(row))\n",
    "\n",
    "imagedf = data.drop_duplicates(subset=['fileid']).sort_values(by='fileid')\n",
    "for row in imagedf.itertuples():\n",
    "    images.append(image(row))\n",
    "\n",
    "# catdf = data.drop_duplicates(subset=['categoryid']).sort_values(by='categoryid')\n",
    "# for row in catdf.itertuples():\n",
    "#     categories.append(category(row))\n",
    "\n",
    "data_coco = {}\n",
    "data_coco[\"images\"] = images\n",
    "data_coco[\"categories\"] = categories\n",
    "data_coco[\"annotations\"] = annotations\n",
    "# json.dump(data_coco, open(save_json_path, \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth = {annot['file_name']:[] for annot in data_coco['images']}\n",
    "for annot,image_data in zip(data_coco['annotations'],data_coco['images']):\n",
    "    annot.update(image_data)\n",
    "    # pprint.pp(annot)\n",
    "    annot['category_name'] = category['category_name']\n",
    "    groundtruth[annot['file_name']].append(annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide correct alias, \"pt\", \"onnx\"\n",
    "handler = Annotator(mlflow_model_alias='pt',\n",
    "                    mlflow_model_version=\"groundtruth\")\n",
    "directory_preds = handler.build_upload_json(path_img_dir='/Users/sfadel/Desktop/savmap',\n",
    "                                            root='/Users/sfadel',\n",
    "                                            bulk_predictions=groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting savmap dataset\n",
    "Formatting the savmap dataset so it can be imported in label studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gdp\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load data\n",
    "root = r\"D:\\Datasets_WildlifeConservation\\savmap_dataset_v2\"\n",
    "polygons = gdp.read_file(os.path.join(root,'savmap_annotations_2014.geojson'))\n",
    "\n",
    "# create directories\n",
    "path_to_images = os.path.join(root,'images')\n",
    "path_to_labels = os.path.join(root,'labels')\n",
    "Path(path_to_images).mkdir(exist_ok=True,parents=True)\n",
    "Path(path_to_labels).mkdir(exist_ok=True,parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move files\n",
    "# ! mv ../data/savmap_dataset_v2/*.JPG ../data/savmap_dataset_v2/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bounding boxs\n",
    "for cat in ['x_min', 'y_min', 'x_max', 'y_max','width','height','x','y']:\n",
    "    polygons[cat] = None\n",
    "for i in range(len(polygons)):\n",
    "    x_min, y_min, x_max, y_max = polygons['geometry'].iloc[i].bounds\n",
    "    image_path = os.path.join(path_to_images,f\"{polygons['IMAGEUUID'].iloc[i]}.JPG\")\n",
    "    width, height = Image.open(image_path).size\n",
    "    polygons['x_min'].iat[i] = int(x_min)\n",
    "    polygons['x_max'].iat[i] = int(x_max)\n",
    "    polygons['y_min'].iat[i] = int(y_min)\n",
    "    polygons['y_max'].iat[i] = int(y_max)\n",
    "    polygons['x'].iat[i] = round(0.5*(x_max+x_min))\n",
    "    polygons['y'].iat[i] = round(0.5*(y_max+y_min))\n",
    "    polygons['width'] = width\n",
    "    polygons['height'] = height\n",
    "\n",
    "polygons['bbox_w'] = polygons['x_max'] - polygons['x_min']\n",
    "polygons['bbox_h'] = polygons['y_max'] - polygons['y_min']\n",
    "polygons['class'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to yolo format\n",
    "def save_df_as_yolo(df_annotation:gdp.GeoDataFrame,dest_path_labels:str):\n",
    "    \n",
    "    cols = ['class','x','y','bbox_w','bbox_h']\n",
    "    for col in cols:\n",
    "        assert df_annotation[col].isna().sum()<1,'there are NaN values. Check out.'\n",
    "        # df_annotation[col] = df_annotation[col].apply(int)\n",
    "\n",
    "    # normalize values\n",
    "    df_annotation.loc[:,'x'] = df_annotation['x']/df_annotation['width']\n",
    "    df_annotation.loc[:,'y'] = df_annotation['y']/df_annotation['height']\n",
    "    df_annotation.loc[:,'bbox_w'] = df_annotation['bbox_w']/df_annotation['width']\n",
    "    df_annotation.loc[:,'bbox_h'] = df_annotation['bbox_h']/df_annotation['height']\n",
    "    \n",
    "    for image_name,df in tqdm(df_annotation.groupby('IMAGEUUID'),desc='Saving yolo labels'):\n",
    "        txt_file = f'{image_name}.txt'\n",
    "        df[cols].to_csv(os.path.join(dest_path_labels,txt_file),sep=' ',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_as_yolo(df_annotation=polygons,dest_path_labels=path_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutorial here: https://github.com/HumanSignal/label-studio-converter/tree/master\n",
    "# if it does not work, use a terminal\n",
    "# !label-studio-converter import yolo -i \"D:\\PhD\\Data per camp\\Extra training data\\savmap_dataset_v2\\raw_data\" -o \"D:\\PhD\\Data per camp\\Extra training data\\savmap_dataset_v2\\raw_data\\labelstudio.json\" --image-root-url \"/data/local-files/?d=PhD/Data per camp/Extra training data/savmap_dataset_v2/raw_data/images\" --image-ext \".JPG\" --out-type \"predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting General Dataset from Delplanque 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiling test\n",
    "# !python ../../HerdNet/tools/patcher.py \"D:\\PhD\\Data per camp\\Extra training data\\general_dataset\\test\" 640 640 64  -dest \"D:\\PhD\\Data per camp\\Extra training data\\general_dataset\\tiled_data\\test_tiled\\images\" -pattern \"**/*.JPG\" -csv \"D:\\PhD\\Data per camp\\Extra training data\\general_dataset\\groundtruth\\csv\\test_big_size_A_B_E_K_WH_WB.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiling val\n",
    "# !python ../../HerdNet/tools/patcher.py \"D:\\PhD\\Data per camp\\Extra training data\\general_dataset\\val\" 640 640 64  -dest \"D:\\PhD\\Data per camp\\Extra training data\\general_dataset\\val_tiled\" -pattern \"**/*.JPG\" -csv \"D:\\PhD\\Data per camp\\Extra training data\\general_dataset\\groundtruth\\csv\\val_big_size_A_B_E_K_WH_WB.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiling Train\n",
    "# !python ../../HerdNet/tools/patcher.py \"D:\\PhD\\Data per camp\\Extra training data\\general_dataset\\train\" 640 640 64  -dest \"D:\\PhD\\Data per camp\\Extra training data\\general_dataset\\train_tiled\" -pattern \"**/*.JPG\" -csv \"D:\\PhD\\Data per camp\\Extra training data\\general_dataset\\groundtruth\\csv\\train_big_size_A_B_E_K_WH_WB.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to YOLO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to yolo format\n",
    "def save_df_as_yolo(df_annotation:pd.DataFrame,dest_path_labels:str,is_detector:bool=False):\n",
    "    \n",
    "    cols = ['class','x','y','bbox_w','bbox_h']\n",
    "    for col in cols:\n",
    "        assert df_annotation[col].isna().sum()<1,'there are NaN values. Check out.'\n",
    "        # df_annotation[col] = df_annotation[col].apply(int)\n",
    "    \n",
    "    for col in ['x','y','bbox_w','bbox_h']:\n",
    "        df_annotation[col] = df_annotation[col].astype(float)\n",
    "\n",
    "    # normalize values\n",
    "    df_annotation.loc[:,'x'] = df_annotation['x']/df_annotation['width']\n",
    "    df_annotation.loc[:,'y'] = df_annotation['y']/df_annotation['height']\n",
    "    df_annotation.loc[:,'bbox_w'] = df_annotation['bbox_w']/df_annotation['width']\n",
    "    df_annotation.loc[:,'bbox_h'] = df_annotation['bbox_h']/df_annotation['height']\n",
    "\n",
    "    if is_detector:\n",
    "        df_annotation['class'] = 0\n",
    "    \n",
    "    for image_name,df in tqdm(df_annotation.groupby('images'),desc='Saving yolo labels'):\n",
    "        txt_file = f'{Path(image_name).stem}.txt'\n",
    "        df[cols].to_csv(os.path.join(dest_path_labels,txt_file),sep=' ',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "split='train' # 'train' 'test', 'val\n",
    "root = Path(rf\"D:\\PhD\\Data per camp\\Extra training data\\general_dataset\\tiled_data\\{split}_tiled\")\n",
    "path_to_csv = root/\"gt.csv\"\n",
    "path_images = root/'images'\n",
    "path_to_labels = root/'labels'\n",
    "detection_mode=False # save label for wildlife detection only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations = pd.read_csv(path_to_csv)\n",
    "# df_annotations.head(2)\n",
    "\n",
    "# update df_annotations\n",
    "df_annotations['width'] = 0.0\n",
    "df_annotations['height'] = 0.0\n",
    "\n",
    "\n",
    "for name in set(df_annotations.images):\n",
    "    width, height = Image.open(root/f\"images/{name}\").size\n",
    "    df_annotations.loc[df_annotations.images==name,'width'] = float(width)\n",
    "    df_annotations.loc[df_annotations.images==name,'height'] = float(height)\n",
    "\n",
    "df_annotations['x'] = 0.5*(df_annotations['x_min'] + df_annotations['x_max'])\n",
    "df_annotations['y'] = 0.5*(df_annotations['y_min'] + df_annotations['y_max'])\n",
    "df_annotations['bbox_h'] = df_annotations['y_max'] - df_annotations['y_min']\n",
    "df_annotations['bbox_w'] = df_annotations['x_max'] - df_annotations['x_min']\n",
    "\n",
    "df_annotations.rename(columns={'labels':'class'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_annotations.head(2)\n",
    "# df_annotations['class'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_as_yolo(df_annotations,path_to_labels,is_detector=detection_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAID "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trasforming labels for detector training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\datalabeling\\data\\WAID\\labels\"\n",
    "path = Path(path)\n",
    "\n",
    "for p in tqdm(path.glob(\"*/**/*.txt\")):\n",
    "    df = pd.read_csv(p,sep=\" \",header=None)\n",
    "    df.columns = [\"class\",'x','y','w','h']\n",
    "    df['class'] = 0\n",
    "    # un-comment to run\n",
    "    # df.to_csv(p, sep=\" \", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping labels to a reference mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting datasets to yolo<>OBB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_yolo_to_obb(yolo_dataset_dir:str,output_dir:str)->None:\n",
    "\n",
    "    cols = ['id','x1','y1','x2','y2','x3','y3','x4','y4']\n",
    "    names = ['id','x','y','w','h']\n",
    "\n",
    "    # Iterate through labels\n",
    "    for label_path in tqdm(Path(yolo_dataset_dir).glob(\"*.txt\"),desc='yolo->obb'):\n",
    "        df = pd.read_csv(label_path,sep=' ',names=names)\n",
    "\n",
    "        # check bounds\n",
    "        assert df[names[1:]].all().max() <=1., \"max value <= 1\"\n",
    "        assert df[names[1:]].all().min() >= 0., \"min value >=0\"\n",
    "\n",
    "        for col in names[1:]:\n",
    "            df[col] = df[col].astype(float)\n",
    "        df['id'] = df['id'].astype(int)\n",
    "\n",
    "        df['w'] = 0.5*df['w']\n",
    "        df['h'] = 0.5*df['h']\n",
    "        # top left\n",
    "        df['x1'] = df['x'] - df['w']\n",
    "        df['y1'] = df['y'] - df['h']\n",
    "        # top right\n",
    "        df['x2'] = df['x'] + df['w']\n",
    "        df['y2'] = df['y'] - df['h']\n",
    "        # bottom right\n",
    "        df['x3'] = df['x'] + df['w']\n",
    "        df['y3'] = df['y'] + df['h']\n",
    "        # bottom left\n",
    "        df['x4'] = df['x'] - df['w']\n",
    "        df['y4'] = df['y'] + df['h']\n",
    "\n",
    "        # check bounds\n",
    "        assert df[names[1:]].all().max() <=1., \"max value <= 1\"\n",
    "        assert df[names[1:]].all().min() >= 0., \"min value >=0\"\n",
    "\n",
    "        # save file\n",
    "        df[cols].to_csv(Path(output_dir)/label_path.name,\n",
    "                        sep=' ',index=False,header=False)\n",
    "\n",
    "def convert_obb_to_yolo(obb_dataset_dir:str,output_dir:str)->None:\n",
    "\n",
    "    names = ['id','x1','y1','x2','y2','x3','y3','x4','y4']\n",
    "    cols = ['id','x','y','w','h']\n",
    "\n",
    "    # Iterate through labels\n",
    "    for label_path in tqdm(Path(obb_dataset_dir).glob(\"*.txt\"),desc='obb->yolo'):\n",
    "        df = pd.read_csv(label_path,sep=' ',names=names)\n",
    "\n",
    "        # check bounds\n",
    "        assert df[names[1:]].all().max() <=1., \"max value <= 1\"\n",
    "        assert df[names[1:]].all().min() >= 0., \"min value >=0\"\n",
    "\n",
    "        # center\n",
    "        df['x'] = (df['x1'] + df['x2'])/2.\n",
    "        df['y'] = (df['y1'] + df['y4'])/2.\n",
    "        # width\n",
    "        df['w'] = df['x2'] - df['x1']\n",
    "        # height\n",
    "        df['h'] = df['y4'] - df['y1']\n",
    "\n",
    "        # check bounds\n",
    "        assert df[names[1:]].all().max() <=1., \"max value <= 1\"\n",
    "        assert df[names[1:]].all().min() >= 0., \"min value >=0\"\n",
    "\n",
    "        # save file\n",
    "        df[cols].to_csv(Path(output_dir)/label_path.name,sep=' ',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to run\n",
    "# convert_yolo_to_obb(yolo_dataset_dir=r\"C:\\Users\\fadel\\OneDrive\\Bureau\\WILD-AI\\datalabeling\\data\\train_wildai\\labels\",\n",
    "#                     output_dir=r\"C:\\Users\\fadel\\OneDrive\\Bureau\\WILD-AI\\datalabeling\\data\\train_wildai\\labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to run\n",
    "# convert_obb_to_yolo(obb_dataset_dir=r\"C:\\Users\\fadel\\OneDrive\\Bureau\\WILD-AI\\datalabeling\\data\\train_wildai\\labels\",\n",
    "#                     output_dir=r\"C:\\Users\\fadel\\OneDrive\\Bureau\\WILD-AI\\datalabeling\\data\\train_wildai\\labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
