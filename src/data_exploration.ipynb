{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from skimage.io import imread,imsave\n",
    "# import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image tiling for annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meanings of arguments\n",
    "- ```-ratioheight``` : proportion of tile  w.r.t height of image. Example 0.5 means dividing the image in two bands w.r.t height.\n",
    "- ```-ratiowidth``` : proportion of tile w.r.t to width of image. Example 1.0 means the width of the tile is the same as the image.\n",
    "- ```-overlapfactor``` : percentage of overlap. It should be less than 1.\n",
    "- ```-rmheight``` : percentage of height to remove or crop at bottom and top\n",
    "- ```-rmwidth``` : percentage of width to remove or crop on each side of the image\n",
    "- ```-pattern``` : \"**/*.JPG\" will get all .JPG images in directory and subdirectories. On windows it will get both .JPG and .jpg. On unix it will only get .JPG images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New script for tiling data\n",
    "# images_to_tile = r\"D:\\PhD\\Data per camp\\Extra training data\\savmap_dataset_v2\\raw_data\\images\"\n",
    "# destination_directory = r\"D:\\PhD\\Data per camp\\Extra training data\\savmap_dataset_v2\\raw_data\\images-tiled\"\n",
    "!python ../../HerdNet/tools/patcher.py \"D:\\PhD\\Data per camp\\Dry season\\Leopard rock\\Camp 22 + 37-40\\Rep 2\" 0 0 0 -overlapfactor 0.1  -ratiowidth 0.33334 -ratioheight 0.5 -rmheight 0.21 -rmwidth 0.08 -dest \"D:\\PhD\\Data per camp\\Dry season\\Leopard rock\\Camp 22 + 37-40\\Rep 2 - tiled\" -pattern \"**/*.JPG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-annotating data for labelstudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Annotator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# provide correct alias, \"pt\", \"onnx\"\n",
    "handler = Annotator(mlflow_model_alias='cycle1')\n",
    "path_img_dir=r\"D:\\PhD\\Data per camp\\Dry season\\Leopard rock\\Camp 22 37-40\\Rep 2 - tiled\"\n",
    "root=\"D:\\\\\"\n",
    "save_json_path = os.path.join(Path(path_img_dir).parent,\n",
    "                              f\"{Path(path_img_dir).name}_preannotation_label-studio.json\")\n",
    "directory_preds = handler.build_upload_json(path_img_dir=path_img_dir,\n",
    "                                            root=root,\n",
    "                                            save_json_path=save_json_path,\n",
    "                                            pattern=\"**/*.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from preprocessing import Annotator\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# from dotenv import load_dotenv \n",
    "# from label_studio_ml.utils import get_local_path\n",
    "# from label_studio_sdk import Client\n",
    "# from PIL import Image\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def upload_predictions(project_id:int,\n",
    "#                        annotator:Annotator,\n",
    "#                        dotenv_path:str):\n",
    "\n",
    "#     # Load environment variables\n",
    "#     load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "#     # Connect to the Label Studio API and check the connection\n",
    "#     LABEL_STUDIO_URL = os.getenv('LABEL_STUDIO_URL')\n",
    "#     API_KEY = os.getenv(\"LABELSTUDIO-API-KEY\")      \n",
    "#     ls = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
    "\n",
    "#     # Select project\n",
    "#     project = ls.get_project(id=project_id)\n",
    "\n",
    "#     # Upload predictions for each task\n",
    "#     tasks = project.get_tasks()\n",
    "#     for task in tqdm(tasks,desc=\"Uploading predictions\"):\n",
    "#         task_id = task['id']\n",
    "#         img_url = task['data']['image']\n",
    "#         img_path = get_local_path(img_url)\n",
    "#         img = Image.open(img_path)\n",
    "#         prediction = annotator.predict(img)\n",
    "#         img_width, img_height = img.size\n",
    "#         formatted_pred = [annotator.format_prediction(pred,\n",
    "#                                                     img_height=img_height,\n",
    "#                                                     img_width=img_width) for pred in prediction]\n",
    "#         conf_scores = [pred['score'] for pred in prediction]\n",
    "#         max_score = 0.0\n",
    "#         if len(conf_scores)>0:\n",
    "#             max_score = max(conf_scores)\n",
    "#         project.create_prediction(task_id=task_id,\n",
    "#                             score=max_score,\n",
    "#                             result=formatted_pred,\n",
    "#                             model_version=annotator.modelversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload_predictions(project_id=37,\n",
    "#                    annotator=Annotator(mlflow_model_alias='cycle1'),\n",
    "#                    dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi.models.yolov8 import Yolov8DetectionModel\n",
    "from ultralytics import YOLO\n",
    "from sahi.predict import get_sliced_prediction\n",
    "import torch\n",
    "from PIL import Image\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import json\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv \n",
    "from label_studio_ml.utils import get_local_path\n",
    "from label_studio_sdk import Client\n",
    "# from preprocessing import Annotator\n",
    "from copy import deepcopy\n",
    "\n",
    "class Annotator(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                dotenv_path:str='../.env',\n",
    "                path_to_weights:str=None,\n",
    "                mlflow_model_alias:str=\"cycle1\",\n",
    "                mlflow_model_name:str=\"detector\",\n",
    "                mlflow_model_version:str=None,\n",
    "                confidence_threshold:float=0.1):\n",
    "        \n",
    "        ## Load env variables\n",
    "        load_dotenv(dotenv_path=dotenv_path)\n",
    "        LABEL_STUDIO_URL = os.getenv('LABEL_STUDIO_URL')\n",
    "        API_KEY = os.getenv(\"LABELSTUDIO-API-KEY\")      \n",
    "        self.labelstudio_client = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
    "\n",
    "        ## Load model from path\n",
    "        self.tilesize=640\n",
    "        self.overlapratio=0.1\n",
    "        self.sahi_prostprocess='NMS'\n",
    "        self.path_to_weights = path_to_weights\n",
    "        if self.path_to_weights is None:\n",
    "            ## Load  from mlflow\n",
    "            TRACKING_URI=\"http://localhost:5000\"\n",
    "            mlflow.set_tracking_uri(TRACKING_URI)\n",
    "            client = mlflow.MlflowClient()\n",
    "            name = mlflow_model_name\n",
    "            alias = mlflow_model_alias\n",
    "            version = client.get_model_version_by_alias(name=name,alias=alias).version\n",
    "            self.modelversion = f'{name}:{version}'\n",
    "            self.modelURI = f'models:/{name}/{version}'\n",
    "            self.model = mlflow.pyfunc.load_model(self.modelURI)\n",
    "        else:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            print('Device:', device)\n",
    "            self.model = Yolov8DetectionModel(\n",
    "                                                        model=YOLO(path_to_weights,task='detect'),\n",
    "                                                        confidence_threshold=confidence_threshold,\n",
    "                                                        image_size=self.tilesize,\n",
    "                                                        device=device,\n",
    "                                                        )\n",
    "            self.modelversion = Path(path_to_weights).stem\n",
    "            \n",
    "        # LS label config\n",
    "        self.from_name = \"label\"\n",
    "        self.to_name = \"image\"\n",
    "        self.label_type = \"rectanglelabels\"\n",
    "        if mlflow_model_version is not None:\n",
    "            self.modelversion = mlflow_model_version\n",
    "\n",
    "    def predict(self, image:bytearray):\n",
    "\n",
    "        if self.path_to_weights is None:\n",
    "            return self.model.predict(image)\n",
    "        \n",
    "        result = get_sliced_prediction(image,\n",
    "                                        self.model,\n",
    "                                        slice_height=self.tilesize,\n",
    "                                        slice_width=self.tilesize,\n",
    "                                        overlap_height_ratio=self.overlapratio,\n",
    "                                        overlap_width_ratio=self.overlapratio,\n",
    "                                        postprocess_type=self.sahi_prostprocess,\n",
    "                                        )\n",
    "        return result.to_coco_annotations()\n",
    "\n",
    "    def format_prediction(self,pred:dict,img_height:int,img_width:int):\n",
    "        # formatting the prediction to work with Label studio\n",
    "        x, y, width, height = pred['bbox']\n",
    "        label = pred['category_name']\n",
    "        score = pred['score']\n",
    "        if not isinstance(score,float):\n",
    "            score = 0.0\n",
    "        template = {\n",
    "                    \"from_name\": self.from_name,\n",
    "                    \"to_name\": self.to_name,\n",
    "                    \"type\": self.label_type,\n",
    "                    \"original_width\":img_width,\n",
    "                    \"original_height\":img_height,\n",
    "                    \"image_rotation\":0,\n",
    "                    'value': {\n",
    "                        self.label_type: [label,],\n",
    "                        'x': x / img_width * 100,\n",
    "                        'y': y / img_height * 100,\n",
    "                        'width': width / img_width * 100,\n",
    "                        'height': height / img_height * 100,\n",
    "                        'rotation':0\n",
    "                    },\n",
    "                    'score': score\n",
    "        }\n",
    "        return template\n",
    "    \n",
    "    def upload_predictions(self,project_id:int):\n",
    "\n",
    "        # Select project\n",
    "        project = self.labelstudio_client.get_project(id=project_id)\n",
    "\n",
    "        # Upload predictions for each task\n",
    "        tasks = project.get_tasks()\n",
    "        for task in tqdm(tasks,desc=\"Uploading predictions\"):\n",
    "            task_id = task['id']\n",
    "            img_url = task['data']['image']\n",
    "            img_path = get_local_path(img_url)\n",
    "            img = Image.open(img_path)\n",
    "            prediction = self.predict(img)\n",
    "            img_width, img_height = img.size\n",
    "            formatted_pred = [self.format_prediction(pred,\n",
    "                                                    img_height=img_height,\n",
    "                                                    img_width=img_width) for pred in prediction]\n",
    "            conf_scores = [pred['score'] for pred in prediction]\n",
    "            max_score = 0.0\n",
    "            if len(conf_scores)>0:\n",
    "                max_score = max(conf_scores)\n",
    "            project.create_prediction(task_id=task_id,\n",
    "                                score=max_score,\n",
    "                                result=formatted_pred,\n",
    "                                model_version=self.modelversion)\n",
    "        \n",
    "    def build_upload_json(self,project_id:int,path_img_dir:str=None,root:str=None,\n",
    "                          pattern=\"*.JPG\",\n",
    "                          bulk_predictions:list[dict]=None,\n",
    "                          save_json_path:str=None):\n",
    "\n",
    "        directory_preds = list()\n",
    "        project = self.labelstudio_client.get_project(id=project_id)\n",
    "\n",
    "        # Upload predictions for each task\n",
    "        # for image_path in Path(path_img_dir).glob(pattern):\n",
    "        #     d=image_path.relative_to(Path(root)).as_posix()\n",
    "        tasks = project.get_tasks()\n",
    "        for task in tqdm(tasks,desc=\"Uploading predictions\"):\n",
    "            img_url = task['data']['image']\n",
    "            image_path = Path(get_local_path(img_url))\n",
    "            pred = { \n",
    "                        \"data\": {\"image\" : img_url},\n",
    "                        \"predictions\":[],\n",
    "                    }\n",
    "            # get predictions\n",
    "            if bulk_predictions is None:\n",
    "                start = time()\n",
    "                image = Image.open(image_path)\n",
    "                predictions = self.predict(image)\n",
    "                print(f'Prediction time:{time() - start:.3f} seconds.')\n",
    "                # format predictions\n",
    "                img_width, img_height = image.size\n",
    "                formatted_pred = [self.format_prediction(pred,\n",
    "                                                        img_height=img_height,\n",
    "                                                        img_width=img_width) for pred in predictions]\n",
    "            else:\n",
    "                predictions = bulk_predictions[image_path.name]\n",
    "                formatted_pred = [self.format_prediction(pred,\n",
    "                                                        img_height=pred['height'],\n",
    "                                                        img_width=pred['width']) for pred in predictions]\n",
    "            conf_scores = [pred['score'] for pred in predictions]\n",
    "            # store predictions\n",
    "            if len(conf_scores)>0:\n",
    "                pred['predictions'].append({'result':formatted_pred,\n",
    "                                            'model_version':self.modelversion,\n",
    "                                            'score':max(conf_scores),\n",
    "                                            }\n",
    "                                            )\n",
    "            else:\n",
    "                pred['predictions'].append({'result':formatted_pred,\n",
    "                                            'model_version':self.modelversion,\n",
    "                                            'score':0.0\n",
    "                                            }\n",
    "                                            )\n",
    "            # update buffer\n",
    "            directory_preds.append(pred)\n",
    "\n",
    "        if save_json_path is not None:\n",
    "            with open(Path(save_json_path),'w') as file:\n",
    "                json.dump(directory_preds,file,indent=2)\n",
    "\n",
    "        return directory_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide correct alias, \"pt\", \"onnx\"\n",
    "handler = Annotator(mlflow_model_alias='cycle1')\n",
    "# directory_preds = handler.build_upload_json(project_id=37)\n",
    "handler.upload_predictions(project_id=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_img_dir=r\"D:\\PhD\\Data per camp\\Extra training data\\savmap_dataset_v2\\raw_data\\images\"\n",
    "# save_json_path = os.path.join(Path(path_img_dir).parent,\n",
    "#                               f\"{Path(path_img_dir).name}_preannotation_label-studio.json\")\n",
    "# if save_json_path is not None:\n",
    "#             with open(Path(save_json_path),'w') as file:\n",
    "#                 json.dump(directory_preds,file,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(directory_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with Sahi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi.models.yolov8 import Yolov8DetectionModel\n",
    "from ultralytics import YOLO\n",
    "from sahi.predict import get_sliced_prediction\n",
    "import torch\n",
    "from PIL import Image\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                path_to_weights:str,\n",
    "                confidence_threshold:float=0.3):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.detection_model = Yolov8DetectionModel(\n",
    "                                                    # model_path=path_to_weights,\n",
    "                                                    model=YOLO(path_to_weights,task='detect'),\n",
    "                                                    confidence_threshold=confidence_threshold,\n",
    "                                                    image_size=640,\n",
    "                                                    device=device,\n",
    "                                                    )\n",
    "        self.tilesize=640\n",
    "        self.overlapratio=0.1\n",
    "        self.sahi_prostprocess='NMS'\n",
    "        print('Device:', device)\n",
    "        \n",
    "    def predict(self, image:str):\n",
    "        image = Image.open(image)\n",
    "        result = get_sliced_prediction(image, \n",
    "                                        self.detection_model,\n",
    "                                        slice_height=self.tilesize,\n",
    "                                        slice_width=self.tilesize,\n",
    "                                        overlap_height_ratio=self.overlapratio,\n",
    "                                        overlap_width_ratio=self.overlapratio,\n",
    "                                        postprocess_type=self.sahi_prostprocess,\n",
    "                                        ) \n",
    "\n",
    "        return result.to_coco_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Camp 6-8\\Rep 1 - tiled\\DJI_20231003081043_0016_1.JPG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for ext in ['.pt','.onnx']:\n",
    "    path = r\"..\\base_models_weights\\yolov8.kaza\" + ext\n",
    "    model = Detector(path_to_weights=path,confidence_threshold=0.3)\n",
    "    start = time()\n",
    "    model.predict(image=image_path)\n",
    "    times.append((ext,time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = Path(r\"C:\\Users\\fadel\\OneDrive\\Bureau\\e-savior\\SAVMAP_samples\\00a033fefe644429a1e0fcffe88f8b39.JPG\")\n",
    "# directory = img_path.parent/'preprocessed'\n",
    "# directory.mkdir(parents=False,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = imread(str(img_path))\n",
    "# tilesize_h = 1000\n",
    "# tilesize_w = 1000\n",
    "# height, width, channels = data.shape \n",
    "# count = 0\n",
    "# for i,j in tqdm(product(list(range(0,height,tilesize_h)),list(range(0,width,tilesize_w)))):\n",
    "#     tile = data[i:min(i+tilesize_h,height),j:min(j+tilesize_w,width),:]\n",
    "#     count += 1\n",
    "#     filename = img_path.name.split('.')[0] + f'#{i}#{j}' + img_path.suffix\n",
    "#     savepath = directory/filename\n",
    "#     imsave(savepath,tile)\n",
    "#     #assert sum(tile.shape) == tilesize_h+tilesize_w+channels,f\"{tile.shape}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(tile)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# height,width\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO data_config.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from arguments import Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yaml\n",
    "with open(r\"D:\\PhD\\Data per camp\\IdentificationDataset\\data_config.yaml\",'r') as file:\n",
    "    yolo_config = yaml.load(file,Loader=yaml.FullLoader)\n",
    "yolo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load label mapping\n",
    "args = Arguments()\n",
    "with open(r\"D:\\PhD\\Data per camp\\IdentificationDataset\\label_mapping.json\",'r') as file:\n",
    "    label_map = json.load(file)\n",
    "names = [p['name'] for p in label_map if p['name'] not in args.discard_labels ]\n",
    "label_map = dict(zip(range(len(names)),names))\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_config.update({'names':label_map,'nc':len(label_map)})\n",
    "yolo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\PhD\\Data per camp\\IdentificationDataset\\data_config.yaml\",'w') as file:\n",
    "    yaml.dump(yolo_config,file,default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yaml\n",
    "with open(r\"D:\\PhD\\Data per camp\\Extra training data\\WAID\\data_config.yaml\",'r') as file:\n",
    "    yolo_config = yaml.load(file,Loader=yaml.FullLoader)\n",
    "yolo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = yolo_config['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "\n",
    "path_dataset = os.path.join(yolo_config['path'],yolo_config[split][0])\n",
    "path_dataset = path_dataset.replace('images','labels')\n",
    "\n",
    "path_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list()\n",
    "\n",
    "for txtfile in Path(path_dataset).glob(\"*.txt\"):\n",
    "\n",
    "    df = pd.read_csv(txtfile,sep=\" \",names = ['class','x','y','w','h'] )\n",
    "    df['class'] = df['class'].astype(int)    \n",
    "    df['image'] = txtfile.stem\n",
    "    labels.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(labels,axis=0)\n",
    "df['class'] = df['class'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_per_class = dict()\n",
    "for cls in df['class'].unique():\n",
    "    num_imge = df.loc[df['class'] == cls,'image'].unique().shape[0]\n",
    "    images_per_class[cls] = num_imge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Split:\", split)\n",
    "print(images_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Split:',split)\n",
    "print(df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts().plot(kind='bar',figsize=(10,5),logy=True,title=f\"{split} label distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "path = r\"C:/Users/Machine Learning/Desktop/workspace-wildAI/datalabeling/runs/mlflow/382537255263464058/5cc559b1a98d487983b3defbabe95c5f/artifacts/weights/best.pt\"\n",
    "model = YOLO(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize validation settings\n",
    "validation_results = model.val(data=r\"D:\\PhD\\Data per camp\\IdentificationDataset\\data_config.yaml\", imgsz=640, batch=64, conf=0.25, iou=0.5, device=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sahi tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi.slicing import slice_coco\n",
    "from sahi.utils.file import load_json\n",
    "from skimage.io import imread,imsave\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from arguments import Arguments\n",
    "from utils import save_df_as_yolo, sample_data, get_slices, convert_json_annotations_to_coco,COCO_DIR_PATH,JSON_DIR_PATH, save_tiles, ALL_CSV\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dict = load_json(r\"..\\exported_annotations\\coco-format\\result.json\")\n",
    "coco_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coco_dataset(input_dir=COCO_DIR_PATH):\n",
    "\n",
    "    def get_upload_img_dir(coco_annotation:dict):\n",
    "        directory = set([os.path.dirname(metadata['file_name']) for metadata in coco_annotation['images']])\n",
    "        assert len(directory)==1,'There should be one upload directory per annotation project'\n",
    "        return directory.pop() #list(directory)[0]\n",
    "\n",
    "    upload_img_dirs,coco_paths = list(),list()\n",
    "    for path in Path(input_dir).glob('*.json'):\n",
    "        annot = load_json(path)\n",
    "        upload_img_dirs.append(get_upload_img_dir(coco_annotation=annot))\n",
    "        coco_paths.append(path)\n",
    "    \n",
    "    return dict(zip(upload_img_dirs,coco_paths))\n",
    "\n",
    "load_coco_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_yolo_dataset(args:Arguments,ls_json_dir:str=JSON_DIR_PATH,clear_out_dir:bool=False,load_coco_existing:bool=False,ls_coco_dir:str=COCO_DIR_PATH):\n",
    "\n",
    "    #clear directories\n",
    "    if clear_out_dir:\n",
    "        for p in [args.dest_path_images,args.dest_path_labels,COCO_DIR_PATH]:\n",
    "            shutil.rmtree(p)\n",
    "            Path(p).mkdir(parents=True,exist_ok=True)\n",
    "    if load_coco_existing:\n",
    "        map_imgdir_cocopath = load_coco_dataset(ls_coco_dir)\n",
    "    else:\n",
    "        # convert  json to coco\n",
    "        map_imgdir_cocopath = convert_json_annotations_to_coco(input_dir=ls_json_dir)\n",
    "\n",
    "    # slice coco annotations and save tiles\n",
    "    for img_dir,cocopath in map_imgdir_cocopath.items():\n",
    "        # slice annotations\n",
    "        coco_dict_slices = get_slices(coco_annotation_file_path=cocopath,\n",
    "                            img_dir=img_dir,\n",
    "                            slice_height=args.height,\n",
    "                            slice_width=args.width,\n",
    "                            overlap_height_ratio=args.overlap_ratio,\n",
    "                            overlap_width_ratio=args.overlap_ratio,\n",
    "                            min_area_ratio=args.min_visibility\n",
    "                            )\n",
    "        # sample tiles\n",
    "        df_tiles = sample_data(coco_dict_slices=coco_dict_slices,\n",
    "                                empty_ratio=args.empty_ratio,\n",
    "                                out_csv_path=None,\n",
    "                                img_dir=img_dir,\n",
    "                                labels_to_discard=args.discard_labels\n",
    "                                )\n",
    "        return df_tiles\n",
    "        # save tiles\n",
    "        # save_tiles(df_tiles=df_tiles,\n",
    "        #            out_img_dir=args.dest_path_images,\n",
    "        #            clear_out_img_dir=False)\n",
    "        # # save labels in yolo format\n",
    "        # save_df_as_yolo(df_annotation=df_tiles[~df_tiles['x'].isna()].copy(),\n",
    "        #                 slice_height=args.height,\n",
    "        #                 slice_width=args.width,\n",
    "        #                 dest_path_labels=args.dest_path_labels)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments()\n",
    "args.empty_ratio = 1\n",
    "df_tiles = build_yolo_dataset(args=args,clear_out_dir=False,load_coco_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_tiles) - 1537*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tiles.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tiles['images'].iloc[:10].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated = df_tiles.duplicated(['x0','x1','y0','y1','images'])\n",
    "df_tiles[~duplicated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tiles.duplicated(['images']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_annot = list()\n",
    "for image_name,df in df_tiles.groupby('images'):\n",
    "    # print(len(df),image_name)\n",
    "    num_annot.append(len(df))\n",
    "    # print(image_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(num_annot),max(num_annot),min(num_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2205+869"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(num_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(num_annot,bins=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
