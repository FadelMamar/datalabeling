# ML Microservices Monorepo Implementation Roadmap

This document outlines a 6-week plan (16 hours/week) to consolidate existing services and build new components in a single GitHub repository.
> https://chatgpt.com/c/68120a64-a1d4-8001-a6a3-a7dcc81de7bb
> https://chat.deepseek.com/a/chat/s/869b5735-78fb-4d25-9d0b-372d469ead66
> https://chatgpt.com/c/68114057-e398-8001-89f3-86cf4e61d097

## ðŸ“ Monorepo Structure
```
plain
ml_microservices/
â”œâ”€â”€ .github/                   
â”‚   â””â”€â”€ workflows/               # CI / workflows
|
|
â”œâ”€â”€ gateway/                    # Entry point for frontend and orchestration
|
|
â”œâ”€â”€ shared/                      # Shared components between services
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ schemas.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â””â”€â”€ setup.py                 # Installable as a local package
â”‚
|
â”œâ”€â”€ inference_service/           # Handles model predictions
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ endpoints.py
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ predictor.py
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”‚   â””â”€â”€ core/
â”‚   â”‚       â”œâ”€â”€ data_loader.py
â”‚   â”‚       â””â”€â”€ preprocessing.py
â”‚   â””â”€â”€ Dockerfile
â”‚   â””â”€â”€ Logs â†’ Prometheus
â”‚   â””â”€â”€ Sends inputs/stats â†’ EvidentlyAI
|
|
yolo_baal_backend/
â”œâ”€â”€ main.py               # FastAPI app
â”œâ”€â”€ config.py             # Config management
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ yolo_baal.py      # Model wrapper with YOLOv8 + Baal
â”‚   â””â”€â”€ trainer.py        # Training loop
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ loader.py         # Dataset I/O (e.g., from Label Studio export)
â”‚   â””â”€â”€ transform.py      # Image transforms
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py         # Structured logging
â”‚   â””â”€â”€ ls_interface.py   # Label Studio formatting helpers
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
|
|
â”œâ”€â”€ labeling_service/          
â”‚   â””â”€â”€ Sends retraining trigger â†’ Training Service
|
|
â”œâ”€â”€ dashboard/                 # â† Streamlit;
â”‚
|
â”œâ”€â”€ training_service/            # Handles training and retraining
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ endpoints.py
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ trainer.py
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”‚   â””â”€â”€ core/
â”‚   â”‚       â”œâ”€â”€ mlflow_wrapper.py
â”‚   â”‚       â””â”€â”€ annotation_converter.py
â”‚   â””â”€â”€ Dockerfile
â”‚   â””â”€â”€ Logs metrics â†’ Prometheus
â”‚   â””â”€â”€ Sends data snapshots â†’ EvidentlyAI|
|
|
calibration_service/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ endpoints.py           # POST /calibrate
â”‚   â”œâ”€â”€ services/
|   â”‚   â”œâ”€â”€ calibrator.py          # Coordinates multiple strategies
|   â”‚   â”œâ”€â”€ fp_classifier.py       # Trains & evaluates FP filter
|   â”‚   â””â”€â”€ evaluator.py           # Calculates precision/recall before/after filtering
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py            # Pydantic request/response models
â”‚   â””â”€â”€ main.py                   # FastAPI app
|   â”œâ”€â”€ Dockerfile
|
|
â”œâ”€â”€ data_service/               # Preprocessing, annotation formatting
|
|
â”œâ”€â”€ mlflow_service/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â””â”€â”€ mlflow_server.sh     # Entrypoint to launch tracking server
â”‚   â”œâ”€â”€ mlruns/                  # Local volume (if not using MinIO backend)
â”‚   â””â”€â”€ Dockerfile
|
|
â”œâ”€â”€ storage/                   # MinIO compose
â”‚   â””â”€â”€ Used as MLflow artifact store
â”‚   â””â”€â”€ docker-compose config or manifest
|
|
â”œâ”€â”€ database/                  # Weaviate compose
â”‚   â””â”€â”€ docker-compose config or manifest
|
|
â”œâ”€â”€ monitoring/                       # Prometheus, Grafana, Evidently
â”‚   â”œâ”€â”€ prometheus/                   # Metrics collection
â”‚   â”œâ”€â”€ grafana/                      # Visualization
â”‚   â””â”€â”€ evidently_service/            # Model/data drift monitoring
â”‚       â””â”€â”€ REST or shared volume reports
â”‚
â””â”€â”€ docker-compose.yml         # Orchestrates all services
â””â”€â”€ README.md
```

**Internal Communication Plan**
| From                   | To                     | Method                         | Purpose                            |
|------------------------|------------------------|----------------------------------|-------------------------------------|
| **Dashboard**          | Gateway                | HTTP (REST)                     | Frontend requests                   |
| **Gateway**            | Other Services         | REST or gRPC                    | API routing                         |
| **Labeling Backend**   | Label Studio API       | REST (Label Studio SDK)         | Pull new annotations                |
| **Labeling Backend**   | Training Service       | REST or Message Queue (e.g., Celery, Kafka) | Trigger retraining    |
| **Inference Service**  | Database               | Direct DB/API                   | Log/query predictions               |
| **Training Service**   | MinIO                  | S3 API (boto3, MinIO SDK)       | Save/load datasets and models       |
| **Training Service**   | Database               | Direct DB/API                   | Store vectors, metadata             |
| **Data Service**       | MinIO, Label Studio    | S3 + REST                       | Upload raw/annotated data           |
| **All services**       | Shared config/models   | Mounted or installed dependency | Common data structures              |



---

# ðŸ—“ 6-Week Roadmap (16 h/week)

### Week 1 â€“ Monorepo & Dev-Ops Setup

**Goals:** Consolidate repos, scaffold services, set up CI

- Consolidate existing `inference_service`, `labeling_service`, and `dashboard` into one repo
- Create top-level `docker-compose.yml` to bring up:
  - Inference, Labeling, Dashboard
  - MinIO, Weaviate
- Scaffold directories for new services: `training_service`, `calibration_service`, `mlflow_service`, `monitoring`
  - Add empty `main.py` and Dockerfile in each
- Setup GitHub Actions to lint, format, and build all Docker images

> **Milestone:** Single repo builds all services and infra containers.

### Week 2 â€“ Training Service & MLflow Integration

**Goals:** Implement training logic, integrate MLflow

- Develop `training_service`:
  - YOLOv8 training wrapper in `trainer.py`
  - REST endpoint to start training jobs
- Stand up `mlflow_service`:
  - Dockerized MLflow server pointing at MinIO for artifact store
  - Configure `MLFLOW_S3_ENDPOINT_URL`, `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`
- Connect training â†’ MLflow:
  - Use `mlflow.set_tracking_uri("http://mlflow:5000")`
  - Log runs and register models

> **Milestone:** Train a model, view it in MLflow UI, and store artifacts in MinIO.

### Week 3 â€“ Calibration Service & FP Classifier

**Goals:** Build calibration logic and integrate with MLflow

- Create `calibration_service`:
  - API endpoint `/calibrate`
  - Implement strategies: threshold/NMS tuning, FP classifier training in `fp_classifier.py`
  - Log calibrated model versions in MLflow
- Update `inference_service` to load â€œProductionâ€ or â€œCalibratedâ€ model from MLflow

> **Milestone:** Calibrate a registered model, register the calibrated version, and serve it.

### Week 4 â€“ Database & Storage Enhancements

**Goals:** Wire up Weaviate and secure MinIO access

- Instrument `inference_service` to push embeddings and prediction metadata into Weaviate
- Ensure `training_service` writes metadata and model vectors to Weaviate or another structured store
- Centralize MinIO credentials and buckets in shared config
- Add health-check endpoints for MinIO and Weaviate

> **Milestone:** Full data flow: inference â†” Weaviate, training artifacts in MinIO.

### Week 5 â€“ Monitoring Stack

**Goals:** Implement infrastructure and model monitoring

- Deploy Prometheus & Grafana in `monitoring/`:
  - Instrument FastAPI apps with `prometheus_fastapi_instrumentator`
  - Create Grafana dashboards for throughput, latency, and error rates
- Stand up EvidentlyAI:
  - Stream inference data into Evidently for drift/quality reports
  - Expose HTML/JSON reports via shared volume or REST endpoint
- Surface Grafana/Evidently links in the Streamlit dashboard

> **Milestone:** Collect infra metrics and model/data drift stats with visualization.

### Week 6 â€“ Active-Learning Loop & Final Polish

**Goals:** Close the feedback loop and document

- Extend labeling backend to orchestrate active learning:
  1. Pull unlabeled samples from storage or Weaviate
  2. Trigger new labeling round in Label Studio
  3. On completion, invoke training â†’ calibration workflows
- Perform an end-to-end test:
  ```plain
  Ingestion â†’ Labeling â†’ Training â†’ Calibration â†’ Inference â†’ Monitoring
  ```
- Write developer documentation, `README.md`, and a `Makefile` or CLI:
  - `make up` / `make down`
  - `make train` / `make calibrate`

> **Milestone:** A self-contained monorepo enabling full ML pipeline with observability.
````
graph TD
    A[Initial Dataset] --> B[Train YOLO]
    B --> C[Predict with Uncertainty]
    C --> D[Select Uncertain Samples]
    D --> E[Label in Label Studio]
    E --> F[Retrain Model]
    F --> C
````
